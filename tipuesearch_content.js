var tipuesearch = {"pages":[{"title":"Tutorials","text":"Be careful not to choke on your aspirations. Introduction Open any AWS certification job description , an excerpt is posted below. Excellent working knowledge of key AWS services such as EC2, ECS, Lambda, S3, ELB, CloudFront, RDS Aurora, API Gateway, etc. Expertise in creating cloud migration strategies; defining delivery architecture , creating the migration plans and designing the orchestration plans . Experience in defining new architectures and ability to drive project from architecture standpoint Experience with Leveraging appropriate AWS services to execute migration plan Experience writing Infrastructure-as-Code (IaC) , using tools like CloudFormation . Experience with CI/CD systems. AWS certifications preferred. If you started your career at the dawn of this new century, this list of requirements can be overwhelming. This is just a sample, if you go on searching, we will have different other technologies involved. This is not the age of Master of one . You have to become Jack of all trades . A Certification is just the beginning of the journey and not the destination. If you have to target for next 5 years, I believe multi cloud is the future, as every cloud provider has something unique to provide for the customer. When we combine multi cloud into the above job description, we cannot imagine what it will look. The journey of a thousand miles begins with one step. Lao Tzu You should start today, and we all should target one cloud certification first. A simple search on LinkedIn reveals we have 31,806 AWS jobs, 22,470 Azure jobs, and a very minimal 6,578 GCP jobs. We can target AWS or Azure as the first certification choice. Now we have selected the cloud provider for certification, the next step is to choose from where to study. The internet is filled with options. Udemy Neal Davis Stephane Maarek DolfinED Tutorials Dojo A Cloud Guru LinuxAcademy which is now acquired by A Cloud Guru. CloudAcademy Even here you can be overwhelmed with option. Just pick any one from the list. Finally, and the most important problem. How do you recollect everything on the exam day and beyond? There are lots of techniques which can help, especially if you are mid career, with kids, family and job you have limited amount of time for study. Your only approach can be the approach which saves time per day. You have to increase your efficiency because now you cannot sit like in your younger days and complete syllabus after syllabus in hours. The 3 main techniques which help increase efficiency are. Spaced repetition. Active recall Richard Feynman's study technique. Please read about the first 2 on Wikipedia, if you want me to write about it comment below. Finally the important part is the Richard Feynman's study technique. This Blog is essentially my effort of teaching others the concept of cloud, AWS so that it sticks in my brain also. When you study for yourself, you tend to ignore a lot of concepts. When you think of teaching other or helping others, it gives a purpose for learning, and that helps in the extra effort required to understand a vary tough or boring topic. I am writing about my journey as it happens to that it can help you tomorrow to avoid any pitfalls which I have faced. When you Blog retrospectively you tend to forget very small details which could have created an impact on the long run. Here I am updating all the learning as it happens. I am starting with AWS Developer Associate certification (DVA-C01), which will be followed by AWS Architect Associate. Let me know in the comments below, How do you like the progression of Blogs. AWS Certification | AWS Certified Developer - Associate ( DVA-C01 ) AWS IAM AWS Global Infrastructure & shared responsibility model AWS is very vast in its present form, understanding it would be very challenging. We will begin this journey at the epicenter of AWS, the Shared responsibility model and its global infrastructure. AWS IAM Introduction. In this Blog let us understand the basics of IAM, Identity and access Management. This is service which is used to control access to all the resources in AWS. The key component of IAM are User, Groups, Policy, Roles, API keys. AWS IAM Policy. When we want to give access to various resources to AWS, how do we do it, how do we define the various ways a user can get access. These are parameters are defined in a JSON document called IAM Policy. We will learn about this in this blog. AWS IAM Users. We have learned about IAM policy and its basics. We will apply those to a specific user here. We will also learn the various ways IAM users can get access to the AWS resources and also how can an IAM user rotate password. AWS IAM Roles When to create an IAM role and when to use an IAM User is always a confusing topic to master. You will learn through this piece the answer to this dilemma. You will learn when to create an IAM Roles, how to use the IAM Role, Uses of IAM roles. After learning this you will never be scratching your head using IAM Roles. AWS STS AWS STS or Security Token Service, provides temporary access credentials to access any AWS resource. This temporary access can be requested by the other AWS account, or a federated user in case of hybrid cloud environment who can be authenticated using SAML 2.0, Web identity provider. AWS STS works very closely with IAM Roles. AWS IAM API Keys Cloud technology is a tool for the developers, creating sophisticated softwares. How does developer access the cloud? Does AWS Console is the only way to access the cloud? Developers love code, CLIs and SDKs, AWS provides access to its cloud resources using AWS IAM API Keys. AWS KMS Blog When you are storing your data on a physical server, which is not owned by you. The concern of data security is paramount. AWS KMS or key management service helps in this regards to safeguard data by encrypting it. You can use the keys provided by you or by AWS to encrypt the data. AWS KMS makes sure that the data and the keys are not stored together, restricting the extent of data theft. AWS Inspector Blog As part of the Shared responsibility of cloud between users and AWS, as a user we have to manage the application security and VPC security. This may be difficult to do for a lot of independent developers. AWS has a managed service called AWS inspector. Lets study about it. AWS Cognito and Cognito Sync Blog Create a highly secure web application, by offloading user management, Social sign-in, login along with data sync across devices onto AWS Cognito. This is an intense AWS Cognito tutorial, which will explain about user pool, and identity pool. How to use the user pool with identity pool. We will even write a Python code, to implement the basic AWS Cognito API, using Boto3 SDK. AWS EC2 AWS EC2 - A Step by Step Guide to launch AWS EC2 You might of heard a lot of cloud computing, The only thing that might bother you, how does it all work. The AWS Cloud is dependent on the AWS EC2 (Elastic Compute Cloud), its atomic unit for servers. You can learn to launch an EC2 instance, in 7 easy to follow steps, which will take less than 5 minutes to complete. AWS EC2 - A Step by Step Guide to launch AWS EC2 AMI (Amazon machine image) has further enhanced the options.. Many instances can use these AMIs to launch identical copies. AMIs are the blueprint. AMIs have a definite life cycle of creating, register, copy, launch and de-register. If you are not creating your own AMI, then you have only last three life cycle states. Azure Certification GCP Certification","tags":"cloud","url":"https://www.archerimagine.com/pages/aws-certification.html","loc":"https://www.archerimagine.com/pages/aws-certification.html"},{"title":"The complete beginner's guide to AWS AMIs","text":"Introduction Have you ever pondered on the question Why AMI (Amazon Machine Image) is not called Operating System? If you revisit our previous article, where the first AWS EC2 instance in free tier was created, see Step by Step guide to create an EC2 instance, . The first selection was for AMI. In this article you will find out, Why Amazon Machine Image (AMI) is not called operating system? This article is everything, you need to understand AWS AMIs. You will also get answer's to these questions Why are they referred to as AMI? What does the AMI cost? This article will enlighten you with the AWS AMIs inner working. It also explains that you pay for the AMI, when you get bill for your EC2 instance. This will be a beginner friendly resource. For an experienced person, it will provide some details which you might have ignored. Let's say Hello World to the Amazon AMIs. What is AMI? You should understand by now, AMI stands for Amazon Machine Image. What is an Amazon Machine Image, the answer depends on the Storage supported. An AMI includes these things depending on the Storage supported Amazon Elastic Block Storage It provides the snapshot. This snapshot will include the operating system, or any other application required. Instance Store Backed It is a template, providing Operating system Application server Application. Amazon Elastic Block Storage and Instance Store, they both provide the same details. You can extract the details of an AWS AMI, by the use of an API describe-images . To use this API, you might need the AMI Id. In the previous post to create Free Tier AWS EC2 instance, see Step by Step guide to create an EC2 instance, you used the Ubuntu AMI. It has a unique identifier call AMI ID, as highlighted in the image below. You will use the AWS CLI, to execute this command aws ec2 describe-images --image-ids ami-0d758c1134823146a The output is { \"Images\" : [ { \"Architecture\" : \"x86_64\" , \"CreationDate\" : \"2021-02-24T18:24:50.000Z\" , \"ImageId\" : \"ami-0d758c1134823146a\" , \"ImageLocation\" : \"099720109477/ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-20210223\" , \"ImageType\" : \"machine\" , \"Public\" : true , \"OwnerId\" : \"099720109477\" , \"PlatformDetails\" : \"Linux/UNIX\" , \"UsageOperation\" : \"RunInstances\" , \"State\" : \"available\" , \"BlockDeviceMappings\" : [ { \"DeviceName\" : \"/dev/sda1\" , \"Ebs\" : { \"DeleteOnTermination\" : true , \"SnapshotId\" : \"snap-072d11ffd95664698\" , \"VolumeSize\" : 8 , \"VolumeType\" : \"gp2\" , \"Encrypted\" : false } }, { \"DeviceName\" : \"/dev/sdb\" , \"VirtualName\" : \"ephemeral0\" }, { \"DeviceName\" : \"/dev/sdc\" , \"VirtualName\" : \"ephemeral1\" } ], \"Description\" : \"Canonical, Ubuntu, 20.04 LTS, amd64 focal image build on 2021-02-23\" , \"EnaSupport\" : true , \"Hypervisor\" : \"xen\" , \"Name\" : \"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-20210223\" , \"RootDeviceName\" : \"/dev/sda1\" , \"RootDeviceType\" : \"ebs\" , \"SriovNetSupport\" : \"simple\" , \"VirtualizationType\" : \"hvm\" } ] } This tells me all about the AMI, which family the AMI belongs, What is the Virtualization type. It also tells what is the root device type. The above information is also present in the below screen. As mentioned above, each AMIs has 3 information in itself, you will try to find these 3 information first. Is the above AMI an EBS backed or Instance Store. \"RootDeviceType\": \"ebs\", What is the operating system? \"PlatformDetails\": \"Linux/UNIX\", \"Description\": \"Canonical, Ubuntu, 20.04 LTS, amd64 focal image build on 2021-02-23\", It will be created from this snapshot id \"SnapshotId\": \"snap-072d11ffd95664698\", There is no additional application server or any other application installed. The above AMI is a bare bone Ubuntu Image. If you want, you can install your own Web Server Apache2, or any other application and then create an AMI from it. The next natural progression should be to explore what is the use of AWS AMIs? What is the use of AMIs? If you observe the image below, you can understand 2 basic uses of the AMIs, Can you please guess the 2 use you can think of? Before I can give you the answer to the above question, you should comprehend the above Image. The first step is to create an EBS Snapshot, Once you have the EBS Snapshot, you should register the AMI. These two steps are not required if you are choosing an existing AMI. In Addition, the above image shows us the 3 use or life cycle of the AMI Launch launches an AWS EC2 instance from AMI Copy You can copy an existing AMI, for future use. De-register You can de-register the AMI if not used. If you have guessed any two use from the above image, please pat your back. Once you are clear with the life cycle / use of an AWS AMIs, you should proceed towards seeing the types of AMIs. Region, Operating System, Architecture, and Storage can define the AMI's classification. Let's us discuss them. Types of AMI Region (see Regions and Zones) The region restricts the availability of a particular AMI. If you check Ohio (us-east-2), the number of public AMIs are 69,431. If you check for N.Virginia (us-east-1), the number of public AMIs are 140,650. The above numbers can change in the future. The disparity between region for AMIs will still be present. The AMI are region specific. You can create EC2 instance with the AMI present in that region. If you want any other AMI from a different region you may have to copy those AMIs to your region. Operating System & Architecture You have already seen, AMIs are available for both Linux and Windows. There are different variants for both the operating system. You also have both 32-bit and 64-bit architecture support in these. Launch permissions The AMI owner can specify the availability, by providing the launch permission. There are three types of launch permissions public - Anyone on AWS can launch using this AMI explicit - One account can grant explicit permission to another account. implicit - Owner of the AMI has implicit permission to launch. You will learn about public, explicit and implicit AMIs once you reach a state of creating AMIs. Till that time being, you can read this article on the same, see Share an AMI with specific AWS accounts. Storage for the root device The storage of the root device, creates two distinct classifications of AMI Types. Backed by AWS EBS The root device would be an AWS Elastic Block Store Backed by AWS Instance Store. The root device would be an instance store volume, created from a template. Property AWS EBS AWS Instance Store Boot time less than 1 minute less than 5 min Size Limit 16 TiB 10 GB Root device volume EBS Instance Store Data Persistence Root Volume data is deleted, Non root EBS is persisted Data is persists only till life of the instance Modification Instance type, Kernel, Ram Disk, User data can be changed in stopped state It is fixed. Charges Charged for instance use, EBS volume usage and Storing of AMI as EBS Snapshot Instance usage and AMI storage on S3 AMI Creation Single command requires installation and use of AMI tool Stopped State Can be in stopped state Cannot be in stopped state. You might have this question in mind, Why should you use Instance Store, when EBS is better in all aspects? Instance Store is the option when you need low latency. Read and write are faster because host has the volume mounted. Check out the result published here . Instance store is ephemeral. This makes them a perfect candidate for temporary data which changes very often. Linux AMI virtualization types You might be thinking, What is Virtualization after all? Virtualization is a type of abstraction. It allows many machines created from a single computer. It translates the virtual machine into the underlying hardware. You might have used a Virtual Box to run Linux on your Windows PC, that is also a type of Virtualization. Virtualization is at the heart for all AWS or any Cloud Provider. You will be running many different operating system on the same piece of hardware. You might be thinking, How can you run different operating system on the same piece of hardware? The answer is a technology called HyperVisor or Virtual Machine Monitor (VMM). It allows to host different virtual machine. The different virtualization techniques used are Full Software Virtualization Hardware-assisted software virtualization or Hardware Virtual Machine (HVM) Paravirtualization or paravitualized Machine (PV) Hardware assisted software virtualization with Paravirtual drivers (PVHVM) Component or resource virtualization AWS supports two types of AMIs based on the virtualization techniques. Paravirtual (PV) Hardware virtual machine (HVM) What are these Paravirtualization or Hardware Virtual Machine (HVM)? Come further the rabbit hole. Hardware virtual machine (HVM) CPU Chips with built in virtualization can support Hardware virtual machine (HVM). The Hardware virtual machine is a type of full software virtualization. It is dependent on the hardware capability. Hardware Virtual Machine (HVM) is the future. Paravirtualization In this virtualization techniques, the guest OS uses the facilities provided by the host OS. PV and HVM AMIs Have you seen any Paravirtual (PV) or Hardware Virtual Machine (HVM) AMIs in AWS. Check the images below HVM:- Paravirtual You can see that Paravirtual is a thing of the past. It is only available in community AMIs and not present in the Quick Start AMIs. You have only HVM's as an option in Quick Start AMIs. HVM Vs PV Property HVM PV Description HVM AMIs are presented with a fully Virtualized set of hardware and boot by executing the master boot record of the root block device of the image. PV AMIs boot with a special boot loader called PV-GRUB, which starts the boot cycle and then chain loads the kernel specified in the menu. Hardware extension Yes, can take advantage No, cannot take any advantage Instance Type All current generation Only certain generation, like C1, C3, HS1, M1, M3, M2 and T1 Region All Region Asia Pacific (Tokyo), Asia Pacific (Singapore), Asia Pacific (Sydney), Europe (Frankfurt), Europe (Ireland), South America (SÃ£o Paulo), US East (N. Virginia), US West (N. California), and US West (Oregon) How to find Virtualization type of the AMI is set to hvm virtualization type of the AMI is set to paravirtual, You have understood the difference between the virtualization techniques. Its time to move to understand why would you pay for an AMI? Shared and Paid AMIs You can separate the AMIs based on the payment option. As a Developers, you can share AMIs to the community, which you have created. Other developer's can pick these AMIs for modification. The Community AMIs are falling in this category. Use of these shared AMIs in production environment without audit is a security risk. The Paid AMIs are available from AWS Marketplace. It provides high quality licensed software configured in the AMIs. These are generally charged based on the hourly rate given by the owner. You might be thinking, normal EC2 billing I can understand. How does a component of that EC2 charges money? Come with me to find.. Billing You have executed the describe-images command in What is AMI? section. In the JSON output you got this two details. { \"PlatformDetails\" : \"Linux/UNIX\" , \"UsageOperation\" : \"RunInstances\" , } These two parameters define the billing of the AMI, and also compatibility. When launching a spot instance, always check if the spot instance supports the particular PlatformDetails . In case of Reserved Instance, you should check if the operating system platform lits the AMI PlatformDetails . The UsageOperation tells the lineitem/Operation in the actual billing of the AWS EC2 instance. Conclusion Cloud Computing is possible today because of advancement in virtualization techniques. Hardware virtual machine (HVM) has a special role in this. AMI (Amazon machine image) has further enhanced the options.. Many instances can use these AMIs to launch identical copies. AMIs are the blueprint. AMIs have a definite life cycle of creating, register, copy, launch and de-register. If you are not creating your own AMI, then you have only last three life cycle states. AMIs are different based on the region, you are launching. It is different based on the architecture and operating system. It differs in the way different people have access to launch. It is differentiated based on the root device type. AWS supports only HVM and PV virtualization techniques. Between these, AWS is recommending the HVM and in future you may not even have a PV AMI. The billing of an EC2 instance includes AMI cost. PlatformDetails & UsageOperation field of the AMI contributes toward this cost. Hope you are clear with the concept of AMI. If you want to launch an EC2 instance. Checkout, the free tier EC2 instance launch article. see Step by Step guide to create an EC2 instance . Reference AWS AMI Virtualization Types: HVM vs PV (Paravirtual VS Hardware VM) AWS AMI- Amazon Machine Image. What are Amazon Machine Images (AMI)? Cost of storing AMI EC2 EBS-SSD vs instance-store performance on an EBS-optimized m3.2xlarge Deep Dive on New Amazon EC2 Instances and Virtualization Technologies Overview of Virtualization Technologies","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-ami-tutorial.html","loc":"https://www.archerimagine.com/articles/aws/aws-ami-tutorial.html"},{"title":"The Insider's Guide to AWS EC2","text":"Introduction This is the shortest, easiest, and a beginner friendly step by step guide to launch and run a free tier AWS EC2 instance. I am going to teach you everything, launch a free tier EC2 instance, connect to the EC2 instance using SSH or RDP. This post will teach you just enough to be dangerous. If you ever wanted, just be able to launch a server (EC2) in the cloud and use that server, without worrying about the inner details. Even worried that it may cost you a fortune. This is the post you were waiting for. In seven days, God Created the world. You need only seven steps to create an AWS EC2 instance in cloud. Let's say Hello World to the cloud. What is AWS EC2 ? EC2 stands for Elastic Compute Cloud, AWS's virtual server in the cloud. In order to create an AWS EC2 instance, you should understand What is AWS EC2? Why to use EC2? What is the benefit over your existing on premise server? Cloud computing is all about collection of servers running on someone else premise. In case of AWS it is Amazon's premise. AWS EC2 is that single server in the school of servers on the cloud. AWS EC2 is one of the ingredients in the cloud recipe. AWS EC2 is that virtual server running in the cloud. It is also referred to as an instance sometimes. You can select the amount of memory, storage and compute power each of your AWS EC2 should have. You can even commission and decommission AWS EC2 on demand. This is was makes it a preferred choice compared to the on premise server. Due to this facility, cloud can always manage the traffic coming to them. When the load is high you can have more EC2 instance, and less instance when the load is low. Cloud computing is all about paying what you use. You never pay for anything which you are not using. How do you create this AWS EC2 instance? Lets jump right into it. How to create EC2 instance in AWS ? You have learned about AWS IAM before this, if not please read it. You have become comfortable with AWS Management console. You have still not seen anything happening in the cloud. The purpose of this Blog, is to get your own private server running in the cloud. You will learn about all the inner details of the AWS EC2 instance, in future series of Blog. In short you have still not said Hello, World! to cloud. You will learn it today. Login into AWS Management console and follow along. You are entering The Matrix. EC2 DashBoard In the AWS Management console, search for the service called EC2. It should launch an EC2 Dashboard just like this one. You have to select the Launch Instance . There are two ways to launch instances, Launch Instance Launch Instance with a template. This is a basic configuration which can be saved as a template and be used to launch multiple instance with the same configuration. You will learn more about this in future, when you learn Auto Scaling Group. You will have this screen next. You can clearly see that creating an EC2 instance is a 7 step process. These steps are. Choose AMI Choose Instance Type Configure Instance Add Storage Add Tags Confirm Security Group. Review You will have to proceed with these 7 steps to create your AWS EC2 Instance. Step 1: Choose an Amazon Machine Image (AMI) An AMI or Amazon Machine Image, is a template which provide these information Operating System Application Server Application An AMI is just like kids Stencil, which they use draw same diagram multiple times. Hope you remember these... There are multiple vendor for AMIs, but you will not focus on them here. This is a battle for another day. Please note, all the AMI listed at Quick Start are not free, please select the check box Free tier only , to list only 16 AMIs out of 40 AMIs. See the below image for reference. Broadly the AMI's are divided based on Operating System, Linux and Windows, and Linux is subdivided primarily based on its flavor. You should go with Ubuntu Server 20.04 LTS (HVM), SSD Volume Type , Select 64-bit (x86) version and move to the next step to Choose the instance type. Checkout this article to know in depth about Amazon Machine Image. Step 2: Choose an Instance Type This step is analogous to choosing the hardware for your server. You have hardware which is optimized for database operation, some are optimized for Machine learning and AI application, some web application. You will understand the details on this in the very short future. This screen looks like this. The instance type tells us in detail about the family of hardware, the number of CPU cores it has, what type of instance storage it supports, Network performance, etc.You will learn about this, but as of now focus on the free tier instance type. You have a t2.micro , which is eligible for free tier. You get 750 hrs of usage each month for the first year of joining. Please select this instance and proceed to the next step of, Configure Instance. Step 3: Configure Instance Details This is the step which will require an article on its own. Just look at the configuration you can do on this screen. You can configure all the below option. Number of instances you want to launch Do you need spot instance, a type of instance which is very cheap and can be used for non production work Do you want to use the default VPC, which will have an Internet gateway, making it easier for the instance to connect to the Internet. Do you want it to launch in the default subnet of into your own subnet, also do you want to Auto assign public IP. Do not worry if you do not understand anything above 2 points, will make all of this information very easy to digest in a future Blog. What placement group, capacity reservation to do, any join directory, or any specific IAM role to use. How does the machine shutdown, hibernation needs to be enabled, any detailed Cloud watch monitoring to be done, please note this is a paid option so do not enable details cloud watch monitoring. What type of tenancy will you choose, dedicated instance, dedicated host or shared instance. Do you want your instance to burst from baseline performance in case of high load. Do you want to mount an EFS file system on the creation time? In certain advanced details, you can select to enable the Metadata information access, version, response hop limit etc., There is also an option to create a bootstrapping script. For the matter of simplicity, do not change any option in this screen and just proceed to the next step to Add Storage. Step 4: Add Storage Every computer, be it server or desktop needs a hard disk. The hard disk can be magnetic or SSD. AWS provides all the options for hard drive. Depending on the type of instance chosen in the step two the option of Hard drive may change. The screen will look like this. If you see the option carefully, you can observe that there is also option to encrypt the data. Even you can attach additional storage (EBS) later. Select the default option and move to the next step, Add Tags. Step 5: Add Tags AWS gives you an option to add tags for everything. This will help in finding a particular instance and execute some script on those instances. The tags are case-sensitive. The Add Tags screen looks like this. Add a Name and value as Webserver tag. Proceed to next step a very important step to Configure the security group. Step 6: Configure Security Group Security is important in AWS, and as you learned in the Shared Responsibility model . The security of the instance is the user's responsibility. The screen looks like this You have to understand few things about security group. Security group is basically a firewall to the instance. The default is to deny all traffic. The port which you open is the only traffic allowed. You may be aware, there are two needs to communicate with your AWS EC2 instance. SSH to the AWS EC2 instance You should open port 22 for this. You can open it from anywhere in the world, denoted by 0.0.0.0/0 Source, in the above screen shot. You can even restrict the Source to be from just our public IP. The AWS EC2 instance, should behave as a Web Server You can just open port 80 for this. The default set in the above screen is allowing SSH to this particular AWS EC2 instance from anywhere in the world. This is good for our first AWS EC2 instance launch. Select next and go to the last and final step of every AWS operation - Review . Step 7: Review Instance Launch Here you are in the EndGame. This screen looks like this. This gives all the six steps you have done till now. This is the place you can review it. If you still want to change anything you can change it now. Press Launch if you have no changes to be done. It provides with a pop-up. This Key Pair is necessary for SSH-ing to the Linux AWS EC2, and you can use this Key-Pair to generate RDP (Remote desktop) credentials in case of Windows AWS EC2. Save the eye pair (PEM file) in your local machine. You are now ready to launch into Cloud, click Launch instance . Once the AWS EC2 is created, you will be presented with this screen. This means the AWS EC2 is launching Congratulation on launching your first AWS EC2 instance. Welcome to the Cloud. What next? Login to AWS EC2 using ssh You have launched your first AWS EC2 instance. What Next.... What do you do with it? How do you run the application on this server? You have to SSH to the AWS EC2 instance. To SSH into the AWS you need some information. You should go to EC2 Dashboard > Instance, the screen will look like this. If you see in the above screen, the AWS EC2 is still in Initializing phase. Please click the Connect button to get the connect information, which would look like this. The above screen tells you all the information you need to connect. To elaborate on the step, it is a two step process Reduce the permission on the PEM key. Connect to the AWS EC2 using the SSH command. Reduce permission of PEM File You should have the PEM file downloaded before , saved locally. Navigate to the directory where the PEM file is present. Execute this command to reduce the permission chmod 400 <key_file>.pem The chmod command above will only provide the read permission to only owner. Groups and Others have no permission on the files. Once you have reduced the permission move to the next step. SSH connect to AWS EC2 This is the big step, where you actually connect to AWS EC2. The general structure of the command is, ssh -i \"key_file_name.pem\" ubuntu@ec2-12-66-176-143.<region>.compute.amazonaws.com Once you have the SSH executed your terminal should look like this. You can execute some command on the SSH terminal, like for Ubuntu the most popular commands to execute is sudo apt update && sudo apt upgrade -y This is will update the software on the server. Please think how you can launch an Apache2 Web Server in this AWS EC2 instance. Terminate of launched AWS EC2 You have launched the AWS EC2 instance, in the free tier, it does not mean that you have infinite free usage. You should terminate the launched AWS EC2 instance. This will prevent you from unwanted cost. Go to EC2 Dashboard > Instance, and click on Action, it will look like this. Select the Instance State > Terminate. This will ask for confirmation like this, Once selecting Yes, Terminate, it will start the termination process, and once done you should a screen like this. With this, you have safely launched, executed and terminated an AWS EC2 instance. Congratulation on the big step. Conclusion When this article started, you may not have known what is an AWS EC2 instance, How to make it work. You may have never thought it could be so easy to create one. You may still do not understand a lot of options which were selected in the above process, but you will understand each and every detail in the coming series of article. Launching a new free-tier EC2 instance is a 7 Step process, and you can login to the instance by SSH in case of Linux OS and RDP for Windows. Now you have your own piece in the cloud, do not forget to terminate the instance else, this piece of cloud will put a hole in your pocket. Reference Image by Comfreak from Pixabay Photo by Anna Shvets from Pexels Technology photo created by onlyyouqj - www.freepik.com Photo by Wil Stewart on Unsplash","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-ec2-tutorial.html","loc":"https://www.archerimagine.com/articles/aws/aws-ec2-tutorial.html"},{"title":"How to start using AWS Cognito","text":"Introduction Does AWS provide any service which will offload my sign-up, login, user management responsibility? What if I told you, AWS has a service which does all of the above, and also provide a hosted web UI which can used by you. It even provides data sync across devices. Everything is perfectly secure. I will help you understand everything you need to know about this service. As a developer if you ever wished to focus only on the functionality or business logic of the application you are developing, and leaving the worries or sign-up, login, user management, data sync across devices in a safe and secure manner. Paying only based on the number of users per month. The AWS has answered your wish, and I will guide you through this. As per AWS You can focus on creating great app experiences instead of worrying about building, securing, and scaling a solution to handle user management, authentication, and sync across platforms and devices. Let's jump right into the enticing world of AWS Cognito. What Is Amazon Cognito? The official definition from AWS:- Amazon Cognito provides authentication, authorization, and user management for your web and mobile apps. The most important concept in the above definition is authentication & authorization . This is provided using two components in AWS Cognito. AWS Cognito User Pool AWS Cognito Identity Pool Originally AWS Cognito was used for mobile developers, who could use AWS Cognito for its authentication & authorization capabilities along with the user management. AWS Lambda and ServerLess architecture have given a new dimension to use AWS Cognito. These developers can now offload user management of their application to an AWS Managed service. AWS Cognito provides such developer with fully managed, scalable an cost-effective sign-up/sign-in service. Before you jump into learning about User Pool and Identity Pool , you should have a fair understanding of the terms authentication & authorization . You may also need to understand federation . Basics of Identity and Access Management (IAM) There is a great article by Okta , which explains about IAM. Since you are here, I will summarize it. Authentication This is the first step in the security process of identity and access management. Authentication is the act of validating that users are whom they claim to be. The most common ways to authenticate user are: User Name and password combination OTPs Biometrics SSO (Social Sign On) Authentication tells the application Who am I? . Authorization Authorization in a system security is the process of giving the user permission to access a specific resource or function. Once a person is Authenticated, you have to provide him with relevant access. Even a Guest user, can be provided with minimum access. You can divide your users, into these four categories. Admin of the application Authenticated User Premium user (Paid) Guest user An Administrator can have a different view of the application than a normal authenticated user. Even an authenticated user can be free user or a premium user, depending on the type, the view of your application may be different. What type of experience you want to provide your user, decides the level of access. Lets understand this concept used by an analogy in most of the company. Most companies in pre-covid times used to give the RFID access card to its employee. Authentication means the process by which someone receives the RFID Access card. Once you receive your RFID access card, depending on the authorization of the employee, he may or may not have access to different parts of the office buildings. Hope you are clear on these questions now What is Authentication? --> This answers the question \"Who am I?\" What is Authorization? --> This answers the question \"What I can use?\" There is a third variable in this equation, which is called Federation. Lets understand What is federation? Federation The word federation , means a united, trusted relationship between two or more entities. To understand federation properly, you have to understand few other concepts. Identity Federation It is a system of trust between two parties, to authenticate users and also convey the information required for giving authorization. Identity provider The party in identity federation, which stores the user information, responsible for user authentication. Service Provider The party in identity federation, which provides the service based on authentication and authorization provided by Identity provider. Open Standards Identity federation is possible because of these open standards OIDC (OpenID Connect) SAML (Security assertion markup language) 2.0 OAuth 2.0 When you book movie ticket online, you are authenticated by an online entity, who takes your money and gives you the ticket, when you go to the actual theater you are granted entry based on the ticket, you purchased online. In this case the online ticket vendor is the Identity provider , the theater is the service provider , and the bi-party arrangement is the Identity Federation . Once the basic identity and access management is clear to you, let's move on to understand User Pool and identity pool . User Pool AWS Cognito User Pool , is a way to provide Authentication to user of an Application. It is represented as a user directory in Amazon Cognito. The authentication mechanism provided by AWS Cognito User Pools is:- Social Identity Providers SAML Identity Providers AWS Cognito User Pools, also provide authentication, or act as an identity provider. In Federation , as explained, the Identity provider, stores the user information. When AWS Cognito User Pools are used as the identity provider, the user directory of AWS Cognito stores the user login details, else its store in the identity providers storage. The user directory is accessible by an SDK. This can be used by applications to access user profile. AWS Cognito User pool provide:- Sign up and sign in service A built-in customizable web-ui for user to register. Social sign-in with social identity provider. User directory management and user profile. MFA. Once a user is authenticated, the application receives a JWT (JavaScript Web token). The next step of authorizing uses this JWT. Configure User Pool When you select AWS Cognito in AWS console you get this screen, asking to choose . Since you will configure the User Pool, lets choose the User Pool Option. To Make life easier you can select the Review Defaults , and it could provide you with a good basic user pool. You can also choose to configure each of these ten settings. Customizing all the settings for user pool creation, would be beyond the scope of this Blog. Let's take these two approaches. Create a user pool with the default option. Add an App to enable the hosted WebUI. Create a User Pool (Default) Step 1 : Select the \" Create a user pool \". Step 2 : It provides us with this screen. Step 3 : Provide a name for Pool, and press the Review Defaults . Step 4 : On pressing the Review Defaults , you get this Review screen. The review pages, tells us these important information. Pool Name Email is a required attributes. There is a password policies. How the message's for AWS Cognito needs to be communicated. MFA is enabled or not. Tags are created or not App Clients are registered or not. Which are Triggers to configure. like pre sign-up, pre-authentication If you carefully watch the Review page and the steps to create a user pool, they match. Step 5 : Press the Create Pool , button and your User Pool is created. Congrats on creating the default user pool. Now you should check out the Hosted UI provided by the AWS Cognito for sign-up and login. Add an App to Enable the Hosted Web UI AWS Cognito even goes a step ahead into offloading your user management work. It provides a user sign-in, login page as a hosted web. Let's see, how can you configure this. You will use the default user pool created before. Once you have created a User Pool, you can edit a lot of attributes provided here. To Use the hosted WebUi, you will focus on the App Integration property of the user pool. Step 1: Select App Integration from Setting of User Pool. To get the Web Hosted UI, you have to use this configuration, if you have your own domain, provide your custom domain, else use the AWS domain. Step 2: Add Domain. On choosing the Add Domain option, you get this screen. Enter the domain, you wish, and keep a note of this, you will require it later. Step 3 : Add App Client under General settings Select the App Client under General Setting, so you can enter the app client attributes. The screen will look like this. You should select the Add an app client option. The screen will look like this. You should provide the name of the client, and de-select the option Generate client secret . This option can be used when you have a server side component to generate the client secret. Once the app client is created. We move to the Step 4. Step 4 App Client Settings: Select the App Client Setting, under App Integration. You will get a screen like this. If you check the App Client details are already present, In the above screen, you have to select Cognito User Pool as the enabled identity providers. Since you are testing, provide \" http://localhost \" as the callback URLs, this is for validation. Choose Implicit Grant , in Allowed OAuth Flows. Select All the allowed OAuth Scope, you want. Save the option. Step 5 : Launch the WebHosted UI. At the bottom of the previous screen, there is an option for Launch Hosted UI . Use this option. You should get a sign-up page like this. Here is your simple web hosted a for Login and sign up. Though everything may not work. Just refer this as a guideline. At this point you can use IAM roles for your application and this authentication to make your application function. But providing different level of authorization will still be the application's responsibility. If you want to hand over this part also then move to Identity Pool. Identity Pool AWS Cognito Identity pool does both Authentication and authorization, but in a different way. The AWS Cognito Identity pool uses Federated identity for authenticating users. Different identity federation can be provided by Social Identity Provider SAML Identity Provider OpenID Connect Provider Amazon Cognito User Pool. Kindly note, the AWS Identity pool is the service provider in the identity federation paradigm. It uses the identity providers to authenticate the user. Once these identity providers do their magic (authentication), they inform the Identity pool by issuing a type of token. On receiving the token, identity pool will authorize users to a different level of access. Identity pool will provide the user with different level of access by using IAM Roles. Identity pool uses AWS STS , service to grant the users credentials to access AWS resources. Configure Identity Pool Configuring Identity Pool is a 2 Step Process. Select Manage identity Pool from the screen when you created the user pool . You will be greeted with this screen. Step 1 : Create Identity Pool In this step you have to configure 3 things apart from providing a name to the identity pool. Unauthenticated Identities AWS Cognitio provides support for Guest user. It generates a unique ID for each guest. In the future if they register, the complete session is saved into the user directory. Authentication flow settings We can select a basic or an enhanced authentication flow. Authentication providers We have All the Social Identity providers along with OpenID and SAML. We will use Cognito, which needs us to provide user pool id and App Client ID , which was created during the user pool. Once you fill the required details, let's proceed to the most important step in AWS Cognito, providing Permissions. Step 2 : Set Permissions This is the screen, as you can guess, can provide two types of IAM Role to both Authenticated and Unauthenticated user. The IAM Role will be created here. Here is the IAM policy statement for Authenticated Users in AWS Cognito Identity pool. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"mobileanalytics:PutEvents\" , \"cognito-sync:*\" , \"cognito-identity:*\" ], \"Resource\" : [ \"*\" ] } ] } Here is the IAM policy statement for UnAuthenticated Users in AWS Cognito Identity pool. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"mobileanalytics:PutEvents\" , \"cognito-sync:*\" ], \"Resource\" : [ \"*\" ] } ] } Once you are ready, just select Allow . You will have your AWS Cognito Identity Pool created, just use this option for integrating with the SDK you want to use. Getting Started with Amazon Cognito Python Code for AWS Cognito You might be thinking, how does it all come together. You have a user pool and an identity pool. You also created the Web Hosted UI. You might be thinking how do I use it together. If you like video, to learn, visit the AWS Cognito Python tutorials by Paris Nakita Kejser . This is the only AWS Cognito's in Python video tutorial. We will just pick two important flow from the above tutorials, as some changes need to be done in the code mentioned in the video. Sign-Up using AWS Cognito, Python SDK Boto3 import os import boto3 from dotenv import load_dotenv , find_dotenv load_dotenv ( find_dotenv ()) # read the .env-sample, to load the environment variable. dotenv_path = os . path . join ( os . path . dirname ( __file__ ), \".env-sample\" ) load_dotenv ( dotenv_path ) username = \"abc.xyz@gmail.com\" password = \"#Abc1234\" client = boto3 . client ( \"cognito-idp\" , region_name = \"<region-name>\" ) print ( os . getenv ( \"COGNITO_USER_CLIENT_ID\" )) # The below code, will do the sign-up response = client . sign_up ( ClientId = os . getenv ( \"COGNITO_USER_CLIENT_ID\" ), Username = username , Password = password , UserAttributes = [{ \"Name\" : \"email\" , \"Value\" : username }], ) There are certain prerequisites for this code to work. Create a file called .env-sample , in the current directory where you have the above code. In this file you should provide the macro COGNITO_USER_CLIENT_ID , with the client ID from General Settings > App Client > App client id . The above will be picked using the dotenv module. When you execute the above code, you will get this back as a response, { \"UserConfirmed\" : false , \"CodeDeliveryDetails\" :{ \"Destination\" : \"a***@g***.com\" , \"DeliveryMedium\" : \"EMAIL\" , \"AttributeName\" : \"email\" }, \"UserSub\" : \"123456-d094-44e0-942d-789012134\" , \"ResponseMetadata\" :{ \"RequestId\" : \"123-1842-4027-345-789abc09234\" , \"HTTPStatusCode\" : 200 , \"HTTPHeaders\" :{ \"date\" : \"Mon, 19 Apr 2021 05:11:44 GMT\" , \"content-type\" : \"application/x-amz-json-1.1\" , \"content-length\" : \"175\" , \"connection\" : \"keep-alive\" , \"x-amzn-requestid\" : \"123-1842-4027-345-789abc09234\" }, \"RetryAttempts\" : 0 } } If you check the General Setting > User and groups , the user will be unconfirmed, you will see this, and get a verification code in your email. Confirmation Code using AWS Cognito, Python SDK Boto3 Now you are an unconfirmed user, you have got the confirmation code in the mail. Let's find a way to make you a confirm user. Here is the code to do that. import os import boto3 from dotenv import load_dotenv , find_dotenv load_dotenv ( find_dotenv ()) dotenv_path = os . path . join ( os . path . dirname ( __file__ ), \".env-sample\" ) load_dotenv ( dotenv_path ) username = \"abc.xyz@gmail.com\" client = boto3 . client ( \"cognito-idp\" , region_name = \"<region-id>\" ) print ( os . getenv ( \"COGNITO_USER_CLIENT_ID\" )) confirm_code = \"112418\" # Below API sends the confirmation code. response = client . confirm_sign_up ( ClientId = os . getenv ( \"COGNITO_USER_CLIENT_ID\" ), Username = username , ConfirmationCode = confirm_code , ) print ( response ) Post this the response is this. { \"ResponseMetadata\" :{ \"RequestId\" : \"3412d123-c175-4571-91a5-12349c14a9bd\" , \"HTTPStatusCode\" : 200 , \"HTTPHeaders\" :{ \"date\" : \"Mon, 19 Apr 2021 05:22:10 GMT\" , \"content-type\" : \"application/x-amz-json-1.1\" , \"content-length\" : \"2\" , \"connection\" : \"keep-alive\" , \"x-amzn-requestid\" : \"3412d123-c175-4571-91a5-12349c14a9bd\" }, \"RetryAttempts\" : 0 } } If you again go and check in General Setting > User and groups , the user should be confirmed now. You have successfully created your first app user. Login and Getting User details using AWS Cognito You have now successfully created a new user, and also confirmed the user. The next logical step will be to Login and get some user details from AWS Cognito. This you can achieve in this manner. import os import boto3 from dotenv import load_dotenv , find_dotenv load_dotenv ( find_dotenv ()) dotenv_path = os . path . join ( os . path . dirname ( __file__ ), \".env-sample\" ) load_dotenv ( dotenv_path ) username = \"abc.xyz@gmail.com\" password = \"#Abc1234\" client = boto3 . client ( \"cognito-idp\" , region_name = \"ap-south-1\" ) print ( os . getenv ( \"COGNITO_USER_CLIENT_ID\" )) # Initiating the Authentication, response = client . initiate_auth ( ClientId = os . getenv ( \"COGNITO_USER_CLIENT_ID\" ), AuthFlow = \"USER_PASSWORD_AUTH\" , AuthParameters = { \"USERNAME\" : username , \"PASSWORD\" : password }, ) # From the JSON response you are accessing the AccessToken print ( response ) # Getting the user details. access_token = response [ \"AuthenticationResult\" ][ \"AccessToken\" ] response = client . get_user ( AccessToken = access_token ) print ( response ) Please note sometime you may get this error botocore.errorfactory.InvalidParameterException: An error occurred ( InvalidParameterException ) when calling the InitiateAuth operation: USER_PASSWORD_AUTH flow not enabled for this client If you get this error, please check in General Settings > App Client > Auth Flow Configuration You should have this option selected ALLOW_USER_PASSWORD_AUTH selected, or just for testing enable all the option like this. You will get a JSON as a response of initiate_auth , you have to just pick the AccessToken from it and pass it to get_user . Once that is done, you will get this as a response. { \"Username\" : \"abc.xyz@gmail.com\" , \"UserAttributes\" :[ { \"Name\" : \"sub\" , \"Value\" : \"1234eb31-d094-44e0-942d-50a1234a66b\" }, { \"Name\" : \"email_verified\" , \"Value\" : \"true\" }, { \"Name\" : \"email\" , \"Value\" : \"abc.xyz@gmail.com\" } ], \"ResponseMetadata\" :{ \"RequestId\" : \"xxxxxxx-1231-4f1c-b881-dcf10c54e576\" , \"HTTPStatusCode\" : 200 , \"HTTPHeaders\" :{ \"date\" : \"Mon, 19 Apr 2021 08:26:10 GMT\" , \"content-type\" : \"application/x-amz-json-1.1\" , \"content-length\" : \"213\" , \"connection\" : \"keep-alive\" , \"x-amzn-requestid\" : \"xxxxxxx-1231-4f1c-b881-dcf10c54e576\" }, \"RetryAttempts\" : 0 } } This is enough to understand how AWS Cognito works, you can even follow the video for Forgot Password flow also. AWS Cognito Sync The official documentation of AWS Cognito Sync is. Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. Few use cases of the AWS Cognito Syncs are. Synchronize the user profile data across the mobile devices and the web, without a back-end. The application can cache data locally if there is no connectivity, and once you get connected the data are synced. The AWS Identity pool is required to use the AWS Cognito Sync. Conclusion AWS Cognito is a service to use if you want to offload complete user management tasks to AWS. This will cost a little, but you can continue to focus on the good things of your application and ignore the mundane activity of user management. You can rest assured that AWS will take responsibility of upgrading the service for latest security patches, and you are not exposed to such security flaws. AWS Cognito User pools and Identity pools are the two brothers of AWS Cognito, shouldering the responsibility of authentication and authorization. Using IAM roles, you can provide very fine grained access to users. Integration using SAML, OIDC also help in using 3rd party vendors as your Identity Providers. Even the Guest users can be assigned a unique ID which can later be saved to a user profile, if he registers. Though you may never use the AWS Cognito provided Web Hosted UI, but it makes a great point in AWS service, how thought out their services are. The AWS Boto3 SDK can use the AWS Cognito APIs to provide the complete functionality of User Management in your application with the most minimal amount of code. This would have given you a fair understanding of AWS Cognito Service. Let me know if you tried the Python Code to login the user. Reference Introduction to Amazon Cognito - User Authentication and Mobile Data Service on AWS Authentication for Your Applications: Getting Started with Amazon Cognito - AWS Online Tech Talks AWS Cognito | Amazon Cognito | AWS Tutorial for Beginners | AWS Training | Edureka Fine-grained Access Control with Amazon Cognito Identity Pools User Authentication For Web And iOS Apps With AWS Cognito (Part 1) What's the difference between Amazon Cognito user pools and identity pools? Understanding AWS Cognito User and Identity Pools for Serverless Apps IAM 101 Series: Federation and Federated SSO What is the difference between Federated Login and Single Sign On? What Is Amazon Cognito? Identity federation in AWS Deep Dive on User Sign-up and Sign-in with Amazon Cognito Learning AWS Cognito with Python AWS Cognito Authentication USER_PASSWORD_AUTH flow not enabled for this client Python Boto3 for AWS Cognito What is the use of python-dotenv? Authentication and Authorization Flows Info graphics Authentication Vs Authorization Authentication Authorization Validate the user provide access to resources Passwords, biometrics, OTP used for authentication. Policy and rules are used to grant access. First Step in IAM Follows authentication, (exception - Guest user) OIDC 2.0 OAuth 2.0 ID Token are used Access tokens are used User has some control User have no control AWS Cognito User Pool Vs Identity Pool User Pool Identity Pool Authentication Authorization User Management IAM Roles for Access Sign-Up & Login Fine Grain Access Provide WebHosted UI Complete back end access User Pool is charged Identity pool is free No Guest user Guest User Using AWS Cognito User Pool With Identity Pool When you have both AWS Cognito user pool and identity pool, they can function together with each other to provide access to user for AWS resources. These can be done in three steps. The Application uses the AWS Cognito User Pool, to authenticate, and gets tokens in return. The application exchanges this tokes with the AWS Cognito Identity Pool and received AWS credentials in returns. Use the AWS Credentials, and use to access the AWS resources. When you are using AWS Cognito User Pool With Identity Pool, the flow is explained above. The application authenticates and get token from AWS Cognito User Pool as a JWT Token. This JWT Token is then passed on to AWS Cognito Identity Pool, which returns an IAM Roles for the user. Once the IAM role is assigned, the user can access any resources on AWS.","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-cognito-tutorials.html","loc":"https://www.archerimagine.com/articles/aws/aws-cognito-tutorials.html"},{"title":"The AWS Inspector Blog of your dreams","text":"Introduction You have learned about the AWS Shared responsibility model, Have you stopped and thought, how do you check if your VPC is accessible from outside network? Does my application compromises the EC2 in any regards? You should remember that network accessibility and software security is the user's responsibility in AWS Shared responsibility model. Being a beginner using the AWS, this responsibility may seem daunting. No worries, when AWS gives you a challenge, it also shows you the path. You will learn how can we use an AWS Service to check the security vulnerability of your software. You will also learn how this service helps in checking the network accessibility of the VPC. Let's dive into the inspected world of AWS Inspector. What is AWS Inspector? AWS inspector is a service provided by AWS, which helps you in two ways Finding security vulnerabilities in your software. Checking the network accessibility of the VPCs. AWS inspector gives findings for the checks done, on which you can act on. You can use the findings and corrects the weakness in your application or the network. AWS Inspector Agent You should be thinking by now, okay, network assessment can be done via some external tools or service given sufficient permission via the IAM Roles. How does AWS inspector do a security vulnerability test on an EC2 instance? Is some application is running beyond your knowledge in your own EC2 instance. The answer to this is, When you enable the AWS Inspector to do a security vulnerability test on your application running on the EC2 instance, it asks permission to install an AWS inspector agent on the EC2 instance. The AWS inspector agent does software telemetry for application and the OS running on the EC2 instance. It provides various information about EC2 instance and the application running on it. Installation of this AWS inspector agent is optional . If installed, AWS inspector agent monitors Behavior of the EC2 instance. checks the network file system process activity collects a lot of behavior and configuration data Benefits of AWS Inspector You have learned that AWS inspector does network assessments and security vulnerability checks on the EC2 instance, is this the only use of the AWS inspector? No, you do get the other benefits of using the AWS inspector including Automation You can integrate the security vulnerability, and network assessments automatically in your CI/CD pipeline . This gives your findings if any security or network related check are broken in the upgrade and could be corrected. Application security The application security checks also can be automated, providing you with valuable information. AWS inspector vulnerability scanning when automated, helps in finding issues which can lead to hacking of your application. Caution while using AWS Inspector. You shouldn't relax knowing that AWS inspector does both network assessments and security vulnerability test. This should not give you a false sense of security that you will find all types of vulnerability by just running the AWS inspector. AWS inspector helps in finding some of the security issues with your EC2 instance and application running on it. AWS Inspector does not find issues in real time by log analysis like an AWS GuardDuty, or AWS Trusted Advisor, which even provides optimization techniques for your architecture. The application running on the EC2 instance, and it's instance configuration itself poses complexity, which AWS inspector may not be configured for. We as a user have a responsibility, which we should fulfill by running some complementary test like the AWS GuardDuty or AWS Trusted Advisor. AWS inspector is part of the security and network monitoring not the heart of it. AWS Inspector Pricing Pricing is an important parameters while choosing a particular AWS Service, especially when we have 3rd party tools also competing Tenable Nessus. . Qualys Cloud Platform The AWS inspector pricing is based on these two dimensions. Number of EC2 instance included in each assessment. Number of rules invoked in each run. Host assessment Common vulnerability and exposures ( CVE ) CVE is a mission which identify, define and catalogs publicly disclosed CyberSecurity vulnerabilities. Center for Internet security ( CIS ) benchmarks CIS provides more than 100 configuration guidelines across 25+ vendor product families to safeguard systems against today's evolving cyber threats. Security best practices Runtime behavior analysis. Network assessment For more detailed pricing report, visit the AWS official pricing page. The pricing on Free Tier is very easy, read along to find out. Free tier You do not have to bother about all the above complexities, accounts which have never run AWS inspector once, are eligible for 250 agent assessments with host rules package 250 instance assessments with the network reachability. In the first 90 days. Other assessments will be billed at the normal price. AWS Inspector service limits AWS Inspector has a predefined service limits for different resource you can use. AWS inspector provides these four major categories for resources. Number of instance running assessment. Number of assessment running. Number of various assessment template in every assessment. Number of assessment targets Getting started with Amazon Inspector Let's dive into configuring AWS inspector, and how it can be used. There are certain prerequisite for starting AWS inspector configuration. Prerequisite for AWS Inspector You much have at least one EC2 instance running . What will AWS inspector check if there is no EC2 instance running? For Host assessment, you may need to install the AWS inspector agent on the EC2 instance. Lets first see how to configure AWS Inspector. You can see that AWS inspector uses a service-linked role , to describe the EC2 instance and network configuration. You can see there are two types of AWS inspector setup we can do. Network Assessment (Inspector Agent is not required.) Host Assessment (Inspector Agent is required.) The default option is the most easiest configuration to trigger AWS Inspector. Let us understand Network Assessment and Host Assessment . Network Assessment The checks performed by AWS inspector without the installation of an agents are Network configuration analysis to checks which ports are reachable from outside of the VPC. If you have the AWS inspector agents installed , it can provide you with additional information like, The process whose ports are reachable from outside of the VPC. Host Assessment Host assessment requires the installation of the AWS inspector agent, so once it is installed we get this information Common vulnerability and exposures (CVE) The host is checked towards the know CyberSecurity vulnerabilities. Center for Internet security (CIS) benchmarks Security best practices Once you click on Run Once , the confirmation screen is displayed like this. When we complete the test, we will receive a findings from the test. Before looking into findings, lets see there is an Advanced setup. AWS Inspector | Advanced Setup As shown in this screen, advance setup is a three step process. Define an assessment target Define an assessment template Review. Advance Setup | Define an assessment target You might have guessed by now, an assessment, target is the AWS resources on which you can run the AWS inspector. As of now it is restricted by the operating system and region. Network reachability test can be run on any EC2 instance without using the AWS inspector agent. For running the assessment with an AWS inspector agent let us first check the supported Linux based operating systems . 64-bit x86 instance Amazon Linux 2 Ubuntu Debian Red Hat Enterprise Linux CentOs ARM instance Amazon Linux 2 Red Hat Enterprise Linux Ubuntu The supported Windows operating systems are Windows Server 2019 Base Windows Server 2016 Base Windows Server 2012 R2 Windows Server 2012 Windows Server 2008 R2 The supported AWS regions are US East (Ohio) us-east-2 US East (N. Virginia) us-east-1 US West (N. California) us-west-1 US West (Oregon) us-west-2 Asia Pacific (Mumbai) ap-south-1 Asia Pacific (Seoul) ap-northeast-2 Asia Pacific (Sydney) ap-southeast-2 Asia Pacific (Tokyo) ap-northeast-1 Europe (Frankfurt) eu-central-1 Europe (Ireland) eu-west-1 Europe (London) eu-west-2 Europe (Stockholm) eu-north-1 AWS GovCloud (US-East) gov-us-east-1 AWS GovCloud (US-West) gov-us-east-2 The first task of defining the assessment, target is to give it a name . Then you have an option to run it on all EC2 instances in your account, or you can run it based on certain tags on the EC2 instance. Generally, we can run these assessments only on the production tagged system. The AWS inspector agent is pre-installed on Amazon Linux AMIs. If you want to install on other AMIs manually you may have to uses AWS System Manager service. Best option is to use the install the agent automatically. Advanced Setup | Define an assessment template As shown below, The first task is to Name the assessment templates. Once done, we have to define the rules packages to use. By default, there are 4 rules packages to select. Common Vulnerabilities and Exposures-1.1 CIS Operating System Security Configuration Benchmarks-1.0 Network Reachability-1.1 Security Best Practices-1.0 Once you select which of these 4 rules you want to run, next selection to be done is the duration of the test. In addition, you also also schedule the assessment to be recurring. The findings of these assessments can also be fed to an SNS topic. Advanced Setup | Review As you might have seen many a times, the final step is the Review step. Post all this you can just run the inspector assessment. AWS Inspector Findings Once you run any AWS inspector assessment, the result is called Findings . These are the potential security issues that AWS inspector has found during its assessment. Findings is not generated while the assessment is on going, it is only generated after the assessment is completed. Individual findings from AWS Inspector cannot be deleted. You have to delete the completed assessment run. Conclusion AWS works in Shared responsibility model. In this model, as a user of AWS services you have certain responsibility. Two of them primarily are. Check the network reachability of the VPCs. Check the application's security running on the VPCs. It would have been very difficult for you to accomplish this on your own. Just like for many other services, AWS has provided a managed service for this called the AWS Inspector . AWS Inspector is a managed service which helps in finding security vulnerabilities in the application running on your EC2 instance and also checks if the instance's VPC is reachable from outside. If we install an AWS inspector agent using the AWS system manager service, then we get additional telemetry about the application running on the EC2 instance. You should not get a false sense of security thinking that AWS inspector will find all types of security vulnerabilities, It finds some, but it still does not find all the different possible vulnerabilities, it will be in your best interest to find some other alternatives to test complete security vulnerabilities. AWS inspector has a limitation today on the type of resources it can evaluate, like it can evaluate the security vulnerabilities of an application on an EC2 instance running selective Linux and windows operating system. Though you can use its network reachability test for any type of hardware. AWS inspector provides 250 agent and instance, assessment for the free tier, and then normal pricing. Before running AWS inspector you should at least have 1 instance running and the agent should be installed. You can use the most basic configuration to trigger the assessment which does both types of checks. The report or findings of the test, inform you about the vulnerabilities in different form. AWS inspector is a great tool for doing the self evaluation of the application running on the EC2 instance, and also check the network reachability. Info graphics AWS Inspector Vs AWS GuardDuty AWS Inspector AWS GuardDuty Finds if known threat exists. Finds threat from different log source. Static analysis from configuration and settings. Dynamic analysis from multiple log source. Scheduled timings. Continuous monitoring EC2 and VPC is monitored Multiple services are monitored.. even S3 Available in 14 region Available in 24 region Free tier use of 90 days Free tier use of 30 days Pricing based on number of assessment. Pricing based on volume of logs analysed. AWS Inspector Vs AWS Trusted Advisor AWS Inspector AWS Trusted Advisor Agent-based Agent-less No impact on performance Improves performance by checking service limit Free tier Premium support EC2 configuration AWS account & administrations No cost recommendations Recommendations to optimize cost Scheduled Real time guidance No impact on performance Improves performance by checking service limit Reference How do I set up Amazon Inspector to run security assessments on my Amazon EC2 instances? AWS re:Invent 2015 | (SEC324) New! Introducing Amazon Inspector. AWS Hands on Lab - Amazon Inspector. AWS Tutorial - Amazon Inspector - Overview. What Is Telemetry? How Telemetry Works, Benefits of Telemetry, Challenges, Tutorial, and More. Amazon Inspector supported operating systems and Regions Amazon Inspector, Security Vulnerability Assessment Service Now Generally Available. What is Amazon Inspector? Getting started with Amazon Inspector. Amazon Inspector assessment templates and assessment runs 6 AWS Services for Cloud Security Detection. Amazon Inspector for beginners AWS trusted adviser vs Inspector AWS Trusted Advisor vs. AWS Config vs. AWS Inspector Difference between AWS Inspector and AWS Trusted Advisor Let me know if you run the AWS inspector assessment on your EC2 instance, and what are its findings ð.","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-inspector-tutorials.html","loc":"https://www.archerimagine.com/articles/aws/aws-inspector-tutorials.html"},{"title":"The Dummies Guide to AWS KMS.","text":"Introduction This is the article you have been waiting for to gain knowledge on security, encryption, cipher text. All these topics puts a scare in a lot of us. You have always wished if somehow these topics just don't cross your road. Just like the black cat. I am going to explain you these topics with interesting visuals, charts and info graphics. This will help you to hold a conversation with your colleague when such scary topics are being discussed. In Short, if you want to score few marks of these topics in AWS Certification exams. If you want to have a conversation with your team mates next time, when these topics come up. This is the article you have been waiting for. AWS provides a managed service which does a lot of heavy lifting for us. This service is like Rubeus Hagrid, the guide to novice Harry porter, though the woods of The Forbidden Forest. This service provides a mechanism to encrypt and decrypt data, 2 of the most important task in encryption. It also helps us with envelope encryption. It can also generate the data keys for you. This service is everything you wished for when you wanted to make your application secure and your data encrypted. This service also automatically integrates with lots of other AWS service, making it easier to encrypt the data. Lets jump right in.. to the mystical world of AWS KMS. What is encryption? Before you get into the journey to understanding the AWS provided service. You should understand what is encryption in its simplest form. As mentioned by Wikipedia, Encryption is Encryption is the process of encoding information. This process converts the original representation of the information, known as plaintext , into an alternative form known as ciphertext . You can consider it like this, there is a black box, into which we pass a plain text and get an encrypted text, which no one can understand. This helps in keeping the data secure, if we delete the plain text now, no one can decode the encrypted text back. The process to get back the original plaintext differs on the basis of the encryption used. There are two types of encryption used. Symmetric Encryption Asymmetric Encryption The name derives from the fact that if they are using the same key to encrypt or decrypt the data. Symmetric Encryption In symmetric encryption, we use the same key to encrypt and decrypt the data. Consider your home lock, it opens and closes with the same key. This is same as symmetric encryption. Asymmetric Encryption In Asymmetric Encryption, we have to encrypt the data with a public key which is known to everyone, but the decrypt happens with a specific private key known only the authorized person. In this case, both the keys are different. Consider the bank locker, to open the locker, you need your key, and the bankers key. You cannot open with just one key. Asymmetric encryption is like the bank locker. AWS KMS Definition As per AWS, a KMS is AWS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control customer master keys (CMKs), the encryption keys used to encrypt your data. This is the most apt definition for AWS KMS you will ever find. Trust me, I have searched. AWS KMS creates and controls the customer master keys (CMKs) only. For you to understand this is the master key which you have, and it can open any lock for you. This is the skeleton key for you. If you notice carefully in the definition, it does not mention about the storing of keys, it just mentioned create and control. As you go further in this article you will understand why is that the case. You might be getting an inkling, why is AWS KMS important after reading about the definition, encryption and its type. You can store a lot of data on AWS, using various services, namely AWS S3, RDS, EBS, etc. To keep the enterprise data on public platforms in a plain text form is not advisable. So let's jump into the world of AWS KMS, by learning about some key concepts. Customer Master Keys (CMKs) Customer Master Keys (CMKs) are the core of AWS KMS. This is a logical representation of a master key. It is assumed that a CMKs have these metadata. Key ID Creation Date Description Key State Key Materials You cannot see all these metadata upfront. CMKs are used to encrypt or decrypt up to 4KB of data. As there are different types of encryption, we have corresponding types of CMKs. Symmetric CMKs A 256 byte key, which is used for both encryption and decryption. Asymmetric CMKs represent the RSA Key Pair. The Symmetric CMKs keys, and the private key of Asymmetric CMKs never leave the AWS KMS unencrypted. The CMKs provide APIs for various programming languages, like Boto3 for Python, which can use the AWS KMS Api's to get the task done, in place of the actual physical key materials which is not visible in AWS KMS. As a user we only have to option to delete the CMKs but not modify any of the key metadata. AWS KMS supports 3 different type of CMKs. Customer Managed CMKs AWS Managed CMKs AWS Owned CMKs AWS services manage to use the above 3 types of CMKs in different manner, some use only the Customer Managed CMKs, some use only AWS Managed CMKs or AWS Owned CMKs, and some gives the flexibility of all 3. Customer Managed CMKs These are the CMKs which are created, owned and managed by you. These are under your full control. You can find the customer managed CMKs in the AWS Console. Just remember that Customer Managed CMKs has a monthly fee even in the free tier. A word of advice, after learning about customer managed CMKs, please delete the keys, not disable it. AWS Managed CMKs These are the CMKs which are automatically used by some AWS services when we decide to encrypt the data. These are completely in control of AWS, and you as a user have no control on these CMKs. AWS Managed CMKs are free to use in free tier. You may have to pay for excess usage. Data Keys The Keys used to encrypt data, is called a Data Keys. CMKs are used to generate, encrypt and decrypt the data keys. Please note, we are using CMKs to generate, encrypt and decrypt the data keys and not the data itself. AWS KMS does not take the responsibility of storing, managing or tracking your data keys. Create a data key When you use the AWS KMS APIs, for encryption, the generation of data keys happens automatically. You can explicitly create a data key by calling the API, GenerateDataKey . The API returns 2 things. Plain Text copy of the data key. The data Key encrypted with the CMKs. Encrypt a data key AWS KMS does not have a mechanism for you to encrypt the data key, You can use the OpenSSL library to encrypt the data with Data key, or use the AWS Encryption SDKs . The plain text data key generated above, can be used to encrypt the data using the OpenSSL Library or the encryption SDKs. The encrypted data can now be stored safely, but delete the plain text data key as soon as possible. Decrypt a data key Now you have stored the encrypted data, at some point you want to decrypt the data, You can use the decrypt API, which will decrypt the data key, and returns the plain text data key. This plain text data key can now be used to decrypt the data. Envelope encryption By definition, Envelope Encryption is, Envelope Encryption is a practice where the data is encrypted with a key (data key), and then the key (data key) in turn is again encrypted. To understand the above statement more clearly, you can take the analogy of when you leave your house for a long time, you lock your important document and valuable in a safety lock. You then keep the keys to this safety lockers inside another room or lock. Finally, you lock the front of your door and leave. In above case the data (your valuable) & your key (safety lockers) are in the same place (your home), but you have the master key to the house (CMKs). This helps you leaving your house in peace. In AWS KMS, Customer Master Keys (CMKs) are the master keys, which never leave AWS KMS unencrypted. You can create a chain of such encryption cycle, and the reason why envelope encryption works is for this reason. Encrypting Data is a slow process, but encrypting just the data key is a fast process. Encryption of data keys, gives an added layer of security for you, and it helps you storing the encrypted data keys along with the data. Different encryption algorithms can be used to encrypt multiple keys in each layer of the envelop. Create A CMKs on AWS You are now well versed with the major concepts in AWS, now lets dive into how to create a CMKs and its usage. One thing you should note that a lot of old blogs mentions the AWS KMS as an option under AWS IAM, it is not the case now. AWS KMS is an independent service. The first step in AWS KMS is to create a CMKs. Creation of CMKs is a five step process. Configure Key âï¸ Add Label ð·ï¸ Define key administrative ð¶ Define Key usage permissions ð¨âð» Review ð Lets get you a AWS KMS CMKs. Create CMKs | Step 01 | Configure Key âï¸ Here is the screen for your reference. You learned about Symmetric encryption & Asymmetric encryption, in the topic above What is encryption? . You have chosen one of those options for your own CMKs, lets keep is simple and choose Symmetric encryption, as you will use the same key to encrypt and decrypt data. The default Advanced Option is selected as KMS, you can keep the same. The other 2 option are for giving your own Keys to encrypt the data or take it from CloudHSM. Create CMKs | Step 02 | Add Label ð·ï¸ Let me take you through this option one at a time. You have to give an Alias first, this is the name you will use to reference this key in your code, or for any other service. Give a good name and a description of the Key. You cannot go to the next step without giving the Alias name. The next option is to give a Tag name, you can provide a key/value pair which you can easily reference in code or in service to identify the key in addition to the name you provided. Create CMKs | Step 03 | Define key administrative ð¶ Once you are done with naming your keys, you have to find out off all the IAM user's or Roles who can be the Admin of these keys. AWS KMS CMKs administrators are those people who can administer the keys, but they cannot use the key for cryptographic operation, which make it important for the next step. Select the Administrator User or role, who will not be using it for cryptographic operation, and lets go to the next step. Create CMKs | Step 04 | Define Key usage permissions ð¨âð» You have the administrators of your AWS KMS CMKs, now lets get one of the IAM role or user be selected as the user of the key. These selections enable this user with two extra policies. Policy to use the CMK directly. Use the CMKs with the AWS Services. Create CMKs | Step 05 | Review ð The final step for you in most of the AWS configuration is Review. Check all the details, and see the key policy document. What ever we discussed in the last two steps will now make sense to you. You analyze the key policy mentioned below, and understand what are the special permission given to the administrators and the user. { \"Id\" : \"key-consolepolicy-3\" , \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"Enable IAM User Permissions\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::123456789012:root\" }, \"Action\" : \"kms:*\" , \"Resource\" : \"*\" }, { \"Sid\" : \"Allow access for Key Administrators\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::123456789012:user/XYZ\" }, \"Action\" : [ \"kms:Create*\" , \"kms:Describe*\" , \"kms:Enable*\" , \"kms:List*\" , \"kms:Put*\" , \"kms:Update*\" , \"kms:Revoke*\" , \"kms:Disable*\" , \"kms:Get*\" , \"kms:Delete*\" , \"kms:TagResource\" , \"kms:UntagResource\" , \"kms:ScheduleKeyDeletion\" , \"kms:CancelKeyDeletion\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"Allow use of the key\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::123456789012:user/ABC\" }, \"Action\" : [ \"kms:Encrypt\" , \"kms:Decrypt\" , \"kms:ReEncrypt*\" , \"kms:GenerateDataKey*\" , \"kms:DescribeKey\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"Allow attachment of persistent resources\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::123456789012:user/ABC\" }, \"Action\" : [ \"kms:CreateGrant\" , \"kms:ListGrants\" , \"kms:RevokeGrant\" ], \"Resource\" : \"*\" , \"Condition\" : { \"Bool\" : { \"kms:GrantIsForAWSResource\" : \"true\" } } } ] } administrators The administrators is XYZ in the above policy, check carefully what you see. These administrators are given the Actions to most of the AWS KMS administrators an allow effect. This administrators do not have any encryption, decryption or generate key action associated with it. user. The user is ABC in the above policy, The user as you see is granted the Action of encrypt, decrypt, generate and other key related uses with an allow effect. The user also has an addition allow effective on the AWS KMS Grants, which is mostly used by AWS Services when encrypting data at rest. This makes you the owner of a CMK. Please keep this in mind, the AWS KMS CMKs are charged, so if you are finished learning delete the key immediately, and I would like to remind you, do not disable the key, delete the key . AWS KMS API When you have your CMKs configured, you obviously want to use it. You should wait a bit before using it. Let me explain to you a few of the AWS KMS APIs. There are many APIs in AWS KMS, the full list of AWS KMS api's are mentioned here. We will not go through all of them, but the three most important once, which you can use in a fun manner in the next section. GenerateDataKey - Returns a plain text and a cipher text version of a data key. Encrypt - Encrypt the plain text using a CMKs Decrypt - Decrypt the cipher text which was encrypted with the Encrypt API. The name is self describing but still let me explain you the little fundamental about the APIs. GenerateDataKey When you are using an SDKs, you generally do not need to use the GenerateDataKey , because you can use the Key Id generated above. This API generates a symmetric data key for you, which you can use for client side encryption. It generates two keys for you. Plain text copy of the data key encrypted copy of the data key with the CMKs. Once the client side encryption of data is done and stored, the plain text copy of the data key can be deleted and the encrypted copy of the data key can be kept along with the encrypted data. GenerateDataKey always returns a unique data key on each call. There are different variations of the same API, which you can consider to use. GenerateDataKeyWithoutPlaintext - Which will only generate the encrypted copy of the data key. GenerateDataKeyPair - Generates the Asymmetric data key. Encrypt Once you have the Key id generated , you can use the encrypt API to encrypt your data. This API just encrypts the plain text data into ciphertext using the CMKs we generated. This API has two primary use cases handled. Encrypt small amount of data, like a database password. It can be used to move encrypted data from one Region to another. Decrypt The name itself tells you, it will decrypt the data we have encrypted till now. We do not need to pass the Key Id in the API, if we are symmetric encryption, which is our case anyway. AWS KMS API in action with Boto3 Finally the code which uses the Boto3 SDK to do all the good things for you. import boto3 kms = boto3 . client ( 'kms' ) key_id = 'alias/test' # this should be present in the KMS database_password = 'Lorem-ipsum-dolor-sit-amet.' result = kms . encrypt ( KeyId = key_id , Plaintext = database_password ) # result will now have these fields # ChiphertextBlob - encrypted data encrypted_password = result [ 'ChiphertextBlob' ] decrypt_result = kms . decrypt ( ChiphertextBlob = encrypted_password ) # will have the password decrypted decrypt_result [ 'Plaintext' ] key_id = 'alias/test' this is the key id you created the label. , you have to add alias/ before the actual label. result = kms.encrypt(KeyId=key_id, Plaintext=database_password) You will call the encrypt() API, passing the key id, and the text which has to be encrypted. This API is only used to encrypt small amount of data. encrypted_password = result['ChiphertextBlob'] You can retrieve the encrypted password which is mapped with a key value of ChiphertextBlob in the result. decrypt_result = kms.decrypt(ChiphertextBlob=encrypted_password) You can decrypt, the encrypted_password using the decrypt api. You were using symmetric encryption, this is the reason you are not passing the key id. decrypt_result['Plaintext'] You can retrieve the plain text password back from the key value of Plaintext The above is a very simplistic example of the use of Boto3 for encrypt & decrypt APIs. You can explore further once you have completed the AWS certification. For now the understanding of these three important APIs is enough. Conclusion You might have always wondered about cloud computing, How does the data in the cloud is secured? Though there are multiple service are involved in AWS to provide that data security. You came to know about one of the building blocks in that data security infrastructure. You understood that encryption is the way of encoding information with a key so that you can transmit data securely. You also found that you can use the same or a different key for encrypting and decrypting. AWS KMS is a service which provides a way to create Customer master Keys (CMKs), which can be made by you, or AWS or owned by AWS. This key is the master keys, which you can use the data key or small plain text data. When encrypting large chunks of data, we cannot use the CMKs, but we need something called data keys, which can be generated using the CMKs we created. You can use these data keys to encrypt the data, and deleted the plain text data key. You can store the encrypted data keys with the data. AWS KMS provide different APIs to for you to generate the data key. AWS KMS does not give the option to encrypt and decrypt the data, You have to use the OpenSSL library, or the AWS Encryption SDKs. AWS KMS does not own the responsibility of storing the data key, you are the owner to store it. In encryption you also understood envelope encryption, where you can use multiple keys to encrypt the data. You can use the combination of symmetric and asymmetric encryption to encrypt the data. You followed the 5 step process to create an AWS KMS CMKs, You also found out about the three AWS KMS API, GenerateDatakey , Encrypt and Decrypt . You also used the AWS Boto3 SDK for Python to create a sample understanding of the process to encrypt and decrypt data. Overall you have now a fair understanding of the AWS KMS. You can further enhance the reading of AWS KMS, by following the various articles provided in reference. Reference Photo by Ales Nesetril on Unsplash AWS KMSâ - Key Management Service AWS KMS - Encrypt & Decrypt DEMO | KMS pricing | KMS Key Rotation (Part 2) What is AWS Key Management Service? Encryption What types of encryption are there? AWS Key Management Service concepts Using grants What is the purpose of kms:GenerateDataKey in AWS? Let me know if you used the AWS Boto3 SDK to encrypt and decrypt data. How was your experience, if you could not encrypt or decrypt the data, please give the error in the comment and I will try to help you.","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-kms.html","loc":"https://www.archerimagine.com/articles/aws/aws-kms.html"},{"title":"Who really uses AWS IAM API keys?","text":"Introduction Developer community uses the cloud technologies the most. In your conquest to learn about AWS, you have been focusing on configuring thing using the AWS console. As a developer, you may not find the use of AWS console efficient to do the tasks. Sounds like Neo's trapped in the matrix. It's you, but you didn't find Morphaeus. You have no knowledge of the existence of the RED pill to show you the truth. Soon you can access AWS Cloud with tools like CLIs, SDKs or HTTP APIs. These are the tools you completely understand. Even if you don't, sit tight we will make it easy to flow along. You will first create a user with programmatic access, then progress to configure the developer's machine to connect to the cloud. After this, you will get information from the cloud using programmatic access. Stay seated and enjoy this journey of programmatic access to the promised Earth of the Cloud. IAM API Keys You have already found out that there are three ways to connect to AWS . The AWS console is the most basic way to access. AWS CLIs and SDKs provide a much better way to access, as this can be controlled programmatically. To revise we will list does the various ways to access AWS. AWS Console AWS SDKs AWS CLIs Windows PowerShell You can use any of the above methods to access AWS, all of them use the AWS APIs in the back-end. The way of access to AWS does not change the features of AWS. You might think, AWS Console is easy to use, there is a web interface, you type in the username and password, which allows access to AWS. How will you use the AWS using these programmatic ways. IAM API Keys to the rescue You need to have access to IAM API keys to enable programmatic access to AWS. This API Keys is tied up with an IAM user, so you have to create an IAM user and enable the IAM API Keys. So let's dive into the world of creating and using the IAM API Keys. How to create IAM API Keys? The first step is to build an IAM user. You have already learned the creation of IAM User here . IAM User creation is a five step process. We have to follow all the steps with a minor change in Step 1 . You can see below, we have to just select the option, Programmatic Access . You can also enable this user to have an AWS Console access by enabling the option AWS Management console access . You have to follow the remaining steps from the IAM user creation article . In the step five, Success , you have to take some specific action. You might see in step five, your screen will look little different than the last time you created the user. We have two new field Access Key ID Secret access key You will get these two fields only when you enable the IAM user with programmatic access. Please download the security credentials in CSV format, and keep it safe. Previous time you may have seen this screen. In this the user did not have the programmatic access. Now you have a user with IAM API keys enabled. Properties of IAM API Keys You should keep in mind few important points about IAM API Keys. The above user creation step is the only time you will see both Access Key ID and Secret access key together. Access Key ID is only visible in IAM User's security credentials. It is advised to key the security credentials downloaded in CSV format. Once lost, you cannot recreate the Secret access key corresponding to the Access Key ID . You have to deactivate the old Access Key ID and create a new pair of Access Key ID & Secret access key . You might have already figured this out, since the IAM API keys are tied to an IAM User, we cannot have it associated with IAM Roles . The above combination should never be stored in an AWS EC2 instance, you should use IAM Roles for this. How to use the IAM API Keys? You can use these IAM API Keys in two major ways. AWS SDKs AWS CLIs We will go through both these steps. Using IAM API Keys with AWS CLI. We can install the Python based AWS CLI with a simple command if you have pip package manager. pip install awscli To use the AWS CLI you need to configure the AWS environment by running this command. aws configure If you get four questions as an output than it proves that the AWS CLI is installed. The above command will ask four questions which you can provide the details from the above Access Key ID & Secret access key you created for the new IAM user. AWS Access Key ID [ None ] : AKIAIOSFODNN7EXAMPLE AWS Secret Access Key [ None ] : wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY Default region name [ None ] : us-west-2 Default output format [ None ] : json Let me try to explain you the details about these 4 options. AWS Access Key ID [None] : fill the value you got for Access Key ID AWS Secret Access Key [None]: fill the value you got for the Secret access key . Default region name [None]: You should provide the region on which the AWS CLIs or SDKs should execute. Default output format [None]: JSON is generally the preferred option. The above execution creates 2 file in ~/.aws folder. -rw------- 1 user staff 116B Dec 24 23 :43 credentials -rw------- 1 user staff 44B Dec 24 23 :43 config The credentials file contains this, you gave the Access Key ID & the Secret access key above. [ default ] aws_access_key_id = AKIAIOSFODNN7EXAMPLE aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY The config file has this. [ default ] region = us-west-2 output = json With this you can execute AWS CLI specific commands. Here is a reference of such commands . Once you have the CLI installed and configured, using the SDK is much easier. Using IAM API Keys with AWS SDKs. AWS support SDKs in many languages. C++ Go Java JavaScript .NET Node.js PHP Python Ruby You will use the Python SDK for this example. The Python3 AWS SDK is called Boto3 You should complete the AWS CLI installation before proceeding. Boto3 installation | QuickStart Python has a very good package manager called pip , and we can install boto3 with a simple command pip install boto3 Post this you need to add a few details to some configuration files if you have still not installed the AWS CLI. Create the credentials and config file in the aws directory as shown above. Here is a sample code, which just list all the buckets on AWS S3. Even if you do not understand AWS S3 no worries. The below code will not give any errors if you do not have any S3 buckets. No error in the below code signifies that the AWS SDKs and CLIs are working in conjunction with each other. import boto3 s3 = boto3 . resource ( \"s3\" ) for bucket in s3 . buckets . all (): print ( bucket . name ) Conclusion AWS Console if a great way to use AWS, but it becomes difficult to use only AWS Console for all the tasks. There are times when you may need to do the job more than once. Manually performing these repeated tasks is very difficult and subject to errors. AWS offers two additional access channels, AWS CLIs and SDK. These accesses require a user enabled with programmatic access. The same user can also have the console access with programmatic access. After you have created the user, you should note that you receive an Access Key ID and a Secret Access Key. These pairs of key are only available during user creation. This is the reason we should download this information in the CSV format, as this information is required multiple time for access. You now have all the raw materials to connect to AWS programmatically. The next step is to download the Python AWS CLI package and configure it. Once configured, you will see a folder in the home directory ~/.aws . This folder now has all the sufficient information to connect to AWS. The CLI is a powerful tool to use, if you still want the SDK, you can download the BOTO3 Python SDKs and use the sample code to access all the bucket in the S3 if you have created. If the above code executes without any error you should assume that the SDKs is installed. You can now use any of the methods to access AWS. Let me know your experience of accessing the AWS using CLIs or SDKS, was it easy or hard. Info Graphics Reference Photo by Ales Nesetril on Unsplash AWS SDKs Browse by Programming Language Boto3 | Python SDK AWS CLI Command Reference","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-iam-api-keys.html","loc":"https://www.archerimagine.com/articles/aws/aws-iam-api-keys.html"},{"title":"Doing AWS STS the right way.","text":"Introduction You have seen in the previous topic on IAM Roles , some users and resource can assume a role, moreover an IAM Roles are like a hat which anyone can wear and gets its power. One important part of this should bother you, how does AWS authenticate such users, if the user is a genuine or not. AWS STS of Security token service plays an important part in enabling IAM Roles . When you are using a cross account resource or any federated users, you can also use AWS STS to provide temporary user credentials. AWS STS though can be used to support mobile application using AWS resources, but it is advised to use AWS cognitio, which will be discussed in the future. You will learn what is AWS STS, what are its benefits, when to use it. You will also learn to use a specific Action/API called assumerole to get access to an AWS resource for an AWS cross account. AWS STS AWS STS (Security token service) as the name suggest, provides a security token for accessing a AWS resources. You may think AWS STS as the provider of temporary access. AWS STS has these specific properties when assigning temporary access It can range from few minutes to a few hours. Once the AWS STS provided temporary token expires, it cannot be reused at any point. You can invoke AWS STS only through AWS SDKs or AWS CLIs. Benefits of AWS STS AWS STS solves a very specific problem for you, when you want someone to temporarily access your AWS resource without having concerns of revoking the permission. AWS STS provides a way to You should not embedded long term AWS security credentials into an application. You should not create extra IAM identities, using IAM roles with AWS STS is enough to satisfy the temporary access requirement. You do not have to worry about deactivating the AWS STS credentials, 36 hours is the maximum you can set the AWS STS expiry time depending on the API invoked. When to use AWS STS You have now understood what is AWS STS, also what are the benefits of AWS STS. You might also have guessed the use cases for using AWS STS. Here if a breakdown for this. In Hybrid Cloud setup, where you have to give access to the non AWS account holder. These methods are generally used for giving access to 3rd party SAML 2.0 Identity federation. Web Identity Federation. (Facebook, Github, etc.) Cross Account roles, when you have to give your developer account a temporary access to your production account. IAM roles for AWS services. AWS STS Actions You should learn about these five common AWS STS Actions. AssumeRole : This is used for getting cross account access. AssumeRoleWithWebIdentity : This is using any 3rd party web IDP like Google or Facebook. AssumeRoleWithSAML : This is for hybrid cloud, where you have an entity with SAML 2.0 GetFederationToken : This is used by the AWS root account or any IAM user. GetSessionToken : This is used by the AWS root account or any IAM user. Here is a comparison for you on the above APIs. AWS STS | AssumeRole Action You have the basic understanding of the different Action provided by AWS STS. Let's now try to use AssumeRole API to understand how this works. Here is what you are going to try, or what we call a problem definition. You will have a user with no permission on the AWS Account. Now create an IAM Role, with AmazonS3FullAccess permission. Once you have the Role, edit the trust relationship to give ARN of the user which does not have any permission. Now using AWS Boto3 SDK you will make the user connect to AWS. A pictorial representation of the step to help you understand. AWS STS | Create Role You have to follow all the steps mentioned in the article, IAM Roles . Once change would be this time we should select the Another AWS account option. You may need to give the 12 digit account number. The above steps are the same we will use for cross account access. The steps will not change. AWS STS | Change the trust relationship When you create the role like it is mentioned in the previous step, by default it will always point to the account root user, you have to change it to the ARN of the user you want to do a AssumeRole . Here is how you can do it. Select the role. Click on Trust Relationship options, and select the Edit trust relationship button. This will open a JSON Editor and edit the JSON for this particular user shown below, esp. the Principal, AWS option. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::123456789012:user/Test\" }, \"Action\" : \"sts:AssumeRole\" , \"Condition\" : {} } ] } You might be thinking, if we have to edit this option every time we have to assign to a new user, then how is this scalable? The answer is, most of the time we will add a particular group with the IAM role attached and the required user is added or removed from the group to control the access. AWS STS | BOTO3 code to AssumeRole Now you have to write using the BOTO3 SDK provide for Python, the sample code to AssumeRole . Here is the sample code. import boto3 import pprint from boto3.session import Session # Below is the ARN of the role. arn = \"arn:aws:iam::123456789012:role/account-s3-full-access\" session_name = \"example-role\" client = boto3 . client ( \"sts\" ) account_id = client . get_caller_identity ()[ \"Account\" ] print ( account_id ) # Assume role takes the roles ARN and a sample session name response = client . assume_role ( RoleArn = arn , RoleSessionName = session_name ) pprint . pprint ( response ) # Create an S3 resource that can access the account with the temporary credentials. temp_credentials = response [ \"Credentials\" ] # Access the S3 as a resource passing the temporary credentials received from STS. s3_resource = boto3 . resource ( \"s3\" , aws_access_key_id = temp_credentials [ \"AccessKeyId\" ], aws_secret_access_key = temp_credentials [ \"SecretAccessKey\" ], aws_session_token = temp_credentials [ \"SessionToken\" ], ) print ( f \"Listing buckets for the assumed role's account:\" ) for bucket in s3_resource . buckets . all (): print ( bucket . name ) Once you run the above code, you should be getting the list of S3 buckets in your account. Though the user did not have access initially. AWS STS | AssumeRole | Return Parameters You should be thinking what is returned by STS, here is the JSON response returned by calling assume_role BOTO3 API. { \"AssumedRoleUser\" :{ \"Arn\" : \"arn:aws:sts::123456789012:assumed-role/acc-s3-full-access/example-role\" , \"AssumedRoleId\" : \"AROAUXRIFYXT7BG3ENQGE:example-role\" }, \"Credentials\" :{ \"AccessKeyId\" : \"ASIAUXRIFYXTUVPIQWFL\" , \"Expiration\" : datetime.datetime( 2021 , 3,20,9,24,27, \"tzinfo=tzutc())\" , \"SecretAccessKey\" : \"BsXYZCGNuemA8wevm6CnYVfZtNgdGaoOCJ4VwXnf\" , \"SessionToken\" : \"FwoGZXIvYXdzEEoaDFAVerghnmasN971Z76yKwAfJgq3tccU72Gj6Xl28zJwJIUS/UEEMtwYmxUDsplTKg0if/keQ9z1BdoPFdLsmDtUiWDnfvIkICUbCeVk+DKI4c9LtdIAXmhpssg4IAMncYFsmh+ylOdbbcud134TOkDkCtuZMkfKuUbIMG3lTq10k93DsiUFAoH5pqyLAa9IyqHUbKUxwwde0UAcUU1lNFMO/sTZI8kAIQNM4cpGMxdyPsYZaX5M1IGWqr2gPNLqLtKLvi1oIGMi33r+lP9GWX5W+Ich1MHUAfUfhgqIjXHjpmDQY5S0e/WOTBwrPLoorgXQlHMak=\" }, \"ResponseMetadata\" :{ \"HTTPHeaders\" :{ \"content-length\" : \"1057\" , \"content-type\" : \"text/xml\" , \"date\" : \"Sat, 20 Mar 2021 08:24:27 GMT\" , \"x-amzn-requestid\" : \"72f38158-1d90-4619-a431-5a2fcf460a31\" }, \"HTTPStatusCode\" : 200 , \"RequestId\" : \"72f38158-1d90-4619-a431-5a2fcf460a31\" , \"RetryAttempts\" : 0 } } You should really be concerned about the Credentials parameters which is returned. It basically provides 4 information. AccessKeyId : The access key id, which is always required for programmatic access. Expiration : Generally it is 15 min, but can vary depending on the type of API being called. SecretAccessKey : The secret access key, which is also generated only once. SessionToken : As the name suggests, a unique way of identifying the session. Conclusion You might have now understood that IAM Roles and AWS STS have a symbiotic relationship. AWS STS is required when you need to provide these range of access. The cross account use case, ex. the developer account my need a temporary access to the production system. The Hybrid cloud use case, ex. On premise user, authenticated using SAML may need access to AWS resources. In Hybrid cloud use case and cross account use case, ex. Authenticating the user using the web identity providers. Sometimes IAM Services may need permission to another service, ex EC2 wants to write to a S3 bucket. The above access is provided to you using the AWS STS because. AWS STS provides short term credentials, which lives from a few minutes to some hours. We should not be bothered to revoke the access as you cannot reuse the expired access. AWS STS can be provided using the AWS SDKs or CLIs. The real benefits of AWS STS are, No need to embed long term credentials to the application. No need to create multiple identities for each access request. No need to revoke the access, as it expires automatically. The various actions provided by AWS STS are AssumeRole AssumeRoleWithWebIdentity AssumeRoleWithSAML GetFederationToken GetSessionToken and we discussed AssumeRole Action uses the cross account example, where you created an IAM role, edited the trust relationship and then the user assuming the role, by using the Boto3 SDKs. By doing this the IAM user from another account could access AWS resources for a short period of time. The AssumeRole returns these parameters when a call is made it AccessKeyId Expiration SecretAccessKey SessionToken You might recognize it returns all the parameters created for IAM user having programmatic access, i.e. AccessKeyId and SecretAccessKey. In addition, we get the SessionToken and an expiration time. Please provide you feedback if you have any other use case for AWS STS. Reference Photo by Ales Nesetril on Unsplash How to Use a Single IAM User to Easily Access All Your Accounts by Using the AWS CLI Introduction to AWS Security Token Service(STS) Switching to an IAM role (AWS API) Providing access to an IAM user in another AWS account that you own Requesting temporary security credentials Switching to a role (console) Boto3 Python Gist","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-sts.html","loc":"https://www.archerimagine.com/articles/aws/aws-sts.html"},{"title":"A foolproof guide to AWS IAM Roles","text":"Introduction The FaceLess Men in GoT (Games of Thrones), hope you remember this character, or Raven/Mystique in X-Men. They both share a common power, what would that be? The power to change identity as they wish to accomplish the task at hand. This is what IAM Roles do in a very broad sense. You can also think of IAM roles as the Invisibility Cloak in Harry Potter, who ever acquires it becomes invisible. IAM Roles are just like a hat, which anyone within AWS can wear, and get the powers presented by the hat, and loses the powers as soon as the hat is removed. The hat does not discriminate between real users or hardware like EC2, anyone can wear this hat. You will also see how IAM Roles can be used, how it is created and attached. You will also see how IAM user is a different concept than IAM Role. IAM Roles An IAM role is an identity that you can create having very specific permissions. The only difference being you are not associating this policy to any particular user or services at the time of creation. This IAM role can be assumed by anyone who needs it, be it a user or an AWS resource. You might be thinking, which problem does IAM role solve which a normal IAM user or a new IAM policy cannot solve. So lets understand why we need the use of IAM Role. IAM Roles for user When you are creating a user in your own account, you cannot give them all the permission as explained in IAM Introduction , the principle of least privileges. You cannot have one or multiple users will super user permissions, you cannot have Batman in every city, only Gotham. In addition, we can also have non AWS accounts, like cross account want to access your AWS services, or the federated user, who may also need access to the AWS services in case of Hybrid Cloud. You have to consider many other use-case along with the above two, for understanding we cannot keep on creating IAM Users to address such diverse range of permission request. IAM Roles for AWS Services You might have already read about IAM Policy , AWS resources cannot have IAM policy attach directly. There is another good policy, AWS credentials should not be stored on EC2 instances. If you consider the above two guidelines, it will not be possible for any AWS services to communicate with other AWS services / resources to complete the work. IAM Roles to the rescue When you consider both the limitation of IAM User and AWS services, you can think we need some type of magic hat, which gives the sufficient power to both User or Service to execute the task it is elevated for. This is where magic happens with IAM Roles. You can use IAM Roles for:- One AWS service have to access another AWS service. Example: Application running on EC2 may need to access S3. When you have a Hybrid Cloud implementation User from On Premise may want to access AWS Cloud infrastructure. EC2 can assume a role only at the time of creation, but once it has a role attached, you can modify it using APIs, CLIs or console at any time. You can attach only 1 role to EC2 at any given time. Benefits of IAM Role You may need people from Hybrid cloud implementation to access the AWS account, you may need IAM Roles to be attached once these users are authorized to use SAML or Active directory. You may have production and development account in AWS, and your development team may be required to be elevated to access a production account to fix a bug. This can be done using IAM Roles. You have an application running on an EC2 instance, It needs access to S3, this can be achieved using IAM Role. How to create an AWS IAM Role AWS IAM Role creation is a three step process. Step 1 | Select the trusted entity The first step guides you choose which services or identities can assume the role, your options are AWS Services The EC2 instance, lambda etc which needs access to other services. Another AWS account This is a cross account access, like a developer taking a production access etc. Web Identity Using OIDC to validate user, like the mobile phone apps. SAML 2.0 Federation Using office resources as authentication parameters, like using service Active Directory. You can select AWS Services for this experimentation, You can attach this to an EC2 instance, so that it can access the S3. Step 2 | Attach permission policy You can now select policy required to be attached, this step is same as the explained in IAM Policy . You have basically chosen from one of these 2 type Custom policy, created using the visual editor or JSON editor. AWS managed Policy. You can select the AmazonS3ReadOnlyAccess for attaching to EC2 instance as agreed in Step 1. Step 3 | Review You have to provide these details to complete the IAM Role Creation process. Role Name An explicit name, which can be used to attached to the EC2 instance. Role description A description about the role. Trusted Entities This is the service selected from Step 1 . Policies The policies attached, in this case AmazonS3ReadOnlyAccess How does the IAM roles looks after creation? You can see above, this is how an IAM Role looks like after creation. The policy document looks exactly same as an IAM Policy . This you can also assume as a baton in a relay race, the service or the user can run with the permission as soon as it gets the policy in the form of an IAM Role. IAM Roles Vs IAM Users You should see this video tutorial on YouTube. This video explains much better, also the flow chart shown above should also clear your doubts You can see the logic is elementary, If you have a non living things, like EC2 etc., it gets an IAM Roles. No question asked. If it is a living thing, like a person, we have to ask two questions. If the permission is temporary then he gets an IAM Roles, else he is an IAM User. Hope this clears your mind about the difference between two confusing terms. IAM Roles Vs IAM Policy IAM Roles and IAM Policy both have a JSON document identifying the rule. The only difference is the mention of Trusted Entities . Conclusion You might now be relieved by understanding the concept of IAM roles, as discussed we use IAM Roles for mainly three purposes. One AWS service using another service, like EC2 instance, wants to read from S3. In the hybrid environment, a non AWS user might need access to AWS resources temporarily. Cross account access, where a developer may need access to production account. You should also think of an IAM Roles as a hat, which a person or services wears and it magically gets the permission and when it drops the hat, it comes back to original state. You also should think about EC2 instance, it should not store the IAM User credentials, they should be using the IAM Roles instead. The IAM Roles can be attached to an EC2 instance, during creation, and can be changed afterwards, but not attached after creation. You can attach only one IAM role to an EC2 instance and not multiple. You learned that IAM Role creation is a 3 step process. Select the trusted entity Attach permission policy Review The IAM Role has been just like IAM Policy document, a JSON document having statements. You can be confused with IAM Role and IAM User, we can simplify it, saying, if it is a resource we use an IAM Roles. If a physical user needs temporary access, it uses IAM Roles, otherwise it is an IAM User. You can comment and let me know if the IAM user is different than IAM Role. Reference Photo by Ales Nesetril on Unsplash Photo by Laura Thonne on Unsplash Photo by Ali Kokab on Unsplash Photo by Almos Bechtold on Unsplash AWS IAM Overview - It's Surprisingly Simple - Users vs Roles","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-iam-roles.html","loc":"https://www.archerimagine.com/articles/aws/aws-iam-roles.html"},{"title":"Global Infrastructure comparison of AWS, GCP and Azure. Updated 2020","text":"Introduction AWS, Azure, GCP are the three largest cloud infrastructure providers. These three are already a big name within the technological space. These enterprises have a greater understanding of the technological space. You may be wondering what is the difference between these companies that offer cloud infrastructure?, what really sets these player apart? Can they not simply replicate in order to provide similar infrastructure and services? Let's us compare these Cloud Infrastructure provider with respect to just the infrastructure point of view, we will compare these on five broad terms. Region Availability Zone Countries Served. Connecting to On Premise CDNs The data used for these comparisons are collected from the individual cloud provider's documentation. AWS and GCP had their data presented in straightforward fashion, Azure was little indirect, but tried consolidating the data for you in the best possible manner. Region All the major Cloud Providers have their data centers, which host their core services across the planet to help you get the best reach. The region is a big differentiator for a lot of business to choose a Cloud Provider. The above picture depicts that Azure is a clear winner with almost double the number of regions from both its competitors. This decision to make Azure a clear winner is little in the Grey area. For example, if you have to run a dedicated cloud server out of China, you only have AWS and Azure as your option other than the local vendors. GCP won't be an option for you in China. Similarly for cloud operation in South Africa again, you are out of luck with Google. Azure has the highest density of the region among cloud providers, this helps in failover switching in case of a complete region going down, this is explained very beautifully for you in this blog . Availability zone Once you are through the region, the number of actual physical data centers available in a region, called as Availability Zone, is your next decision making point. The above graphs, clearly depicts that AWS and GCP are head to head and Azure is lagging behind. This will be the truth if you consider the above graph individually. When you see this data point along with the region data points, you can argue that all the cloud vendors are pretty tied up in these regards. Azure did announce its beta of AZs only in 2017, with that in mind it still has to catch-up, but till this point you will agree that the decision to opt for a particular cloud vendor based on geography is still not decisive. Countries Served Counties served individually by the cloud vendors may not be a such great differentiator for the service, but if you reside or your customer resides in any such location then you are out of luck. The above graph, clearly shows AWS as the winner, but even the others will be catching up. AWS will not have any more countries left to expand and these other vendors can easily catch-up if their intent to do so. Direct Connect Vs ExpressRoute Vs Dedicated Interconnect Direct Connect or Azure ExpressRoute or GCP's dedicated interconnect, is a way for you to connect your on-premise data center to AWS. This helps in reducing the cost to you and also avoids the open Internet for your private data. The above graphs shows Google as the winner, but as discussed there is not much difference with the other vendors. The above graphs also only considers the Direct Connect run by the vendors individually and not the partner data centers. When you consider the partners, then maybe the horizon may change. EdgeLocation Vs EdgeZone The CDNs, for making the content available to the user as close to his point of contact is a very important decision to make while choosing a cloud vendor. AWS is the winner again and that too with a big difference. This graph again considers only the vendor owned CDNs, the number may vary greatly if you include the partner's location. Conclusion The above depiction of five key infrastructure variable may not pursue you to a particular cloud vendor. Region and AZs can be a deciding factor, depending on where you want to run your data centers, but also you need to think that all regions are not created equals. As described in this blog , there is a hardware difference between region. When you are deciding between vendors, check these parameters before deciding Geographic limitation for operations. Hardware type as per the region. Countries Served is not a very important parameter to differentiate between the cloud vendors, as in the near future these may not be very important factor. Direct Connect and CDNs also cannot make an important factor if you consider the partner networks. This way you can be sure that just infrastructure cannot be the differentiator while choosing a Cloud vendors, until it is not because of geographic location of the server. There are other factors which come to play when you are deciding on a particular cloud provider, like the services provided, the pricing, ease of deployment, support provided and also expertise availability in your region. You have to take the holistic view while deciding on a particular cloud provider, even nowadays where the difference between cloud providers' services also matches, you can also opt for a multi cloud solution, choosing what is best from a particular cloud provider. Info graphics Reference Availability Regions and Zones for AWS, Azure & GCP Google Cloud Global Infrastructure [AZ-104] Region, Availability Zone, Availability Sets and Fault Domain,Update Domain In Microsoft Azure Azure Regions Interactive Map IaaS Resilience, which cloud platform is better? Azure or AWS? Global Infrastructure Google Cloud locations Introducing Azure Availability Zones for resiliency and high availability When we have discussion on the region, AZs, countries, direct connect and edge location above, what will you think is important factor when you are deciding on the cloud vendors, comment below and let me know?","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-global-Infrastructure.html","loc":"https://www.archerimagine.com/articles/aws/aws-global-Infrastructure.html"},{"title":"AWS Made Easy | IAM Users","text":"IAM Users IAM users are an important AWS resources. This is the identity given to a physical user or application when he logs into AWS. AWS identifies an IAM user with these identifiers. Friendly user name: We provide an identifiable user name. ARN: An amazon resource name (ARN), Access ID: A unique identifier which is returned when working with SDKs or CLIs. IAM Users | User Permission The default IAM User permission is non-explicit deny for all AWS services. To provide the user access we can be provided explicit permission in one of these ways. Adding a user to a group. Copying permission from another user. Attaching a policy directly. IAM Users | Groups In an organization, it makes sense to group each department into a group and the assign users to these groups. IAM Users | ARN ARN (Amazon resource name), is specified in this format. arn:aws:iam:<region>:<account_id>:user/admin the IAM user ARN is like this arn:aws:iam::<account_id>:user/admin The <region> is missing in a IAM user ARN, as IAM is a global service. The user/admin defines the actual resource. IAM Users | Create a new user. IAM user creation is a 5 step process. IAM Users | Creation | Step 1 | Add user The first step in the IAM User creation process is: Add user. We have to fill a few of the fields. Lets understand these fields. Set user details We should provide the user name for the IAM user, mostly a friendly name. We can add multiple user in this step. Select AWS access type: There are two ways for the user to access AWS resources Programmatic Access : This is to generate the access key ID and secret access key for AWS CLIs, SDKs and other development tools. AWS management console access : Creates a user who can log into the AWS console using a password. We can mix and match the programmatic access and also management console access. IAM Users | Creation | Step 2 | Set Permissions The next step in user creation is to set permissions. There are 3 ways to add permissions to a user. Add user to a group. Add the new user to a group based on the job function. Copy permission from existing users. If we have existing user, we can copy policy for that user. Attach existing policies directly. We can singularly attach a multiple policy to a user directly, both AWS managed and customer managed. The most efficient way to add permission to a group of user is to add the user to a group. IAM Users | Creation | Step 3 | Add Tags Tags are an important component in all service creation. We can assign a key/value pair to manage user efficiently. IAM Users | Creation | Step 4 | Review The penultimate steps of user creation is to review the configuration we have provided. As show in the screen shot above, it displays all the configuration selected till now based on which the user will be created. IAM Users | Creation | Step 5 | Success In the last step of user creation it has given these options. Download csv This CSV gives these details User Name Password Access Key ID Secret access key These two are provided when we have the programmatic access enabled. Console login link This is the only time we get the Secret access key along with the access key ID, there is no way to recreate the same secret access key again. We also got the some of the above information displayed accordingly the way the user was created. IAM User Properties Lets see a few of the properties displayed for the user in the AWS IAM dashboard. The above screen shot shows that the IAM user summary / property can be grouped into Permissions Groups Tags Security Credentials Access Advisor From the above group, the most important to consider is Security credentials. Permissions The above screen shot clearly depicts, this property displays the permission present to the user. In the above case, the user gets 2 IAM policy Attached directly. The IAMUserChangePassword is the policy applied directly to the user. Attached from group. The Administrator access is attached via the Group. Groups & Tags This property depicts the groups and the tags associated with the user. Access Advisor There is always a need as an administrator to review user behavior in AWS like the number of services used, the frequency in which it is used. In the Access Advisor tab, IAM provides this information about the user, and helps in identifying the unused permissions. Security Credentials The major section of security credentials are Sign-in credentials Provides with link to console sign-in link Console password: is it enabled or not Assigned MFA device: Is the device assigned or not Signing certificate: We can upload any specific X.509 certificate for use. Access Keys The keys to make programmatic calls to AWS by CLI, SDKs, API calls. We can even create an access key from here. SSH Keys for AWS code commit SSH public key to access AWS code commit. HTTPS Git credentials for AWS code commit User name and password to access AWS code commit Credentials for Amazon keyspaces. User name and password to access Amazon Keyspaces. Conclusion The AWS IAM user is anybody who will use the login credentials or the programmatic access to AWS. Each IAM user can easily be identified using a friendly name, or a unique ARN or an Access ID. The IAM user can get permission by getting itself attached to a group, or having the policy attached directly or copying from an existing user. Providing permission by adding them to a group is the best IAM policy. AWS IAM user creation is a 5 step process, but the important steps are only 2, the first one has given a user name to the user, along with the type of access it needs. This is followed by, adding permission to the user, which can be done by attaching it to a group or copying from an existing user or by attaching the policy directly. The AWS IAM user has specific properties like permission which it has and from where it got the permission, like attached directly or received it from a group. There is a specific property called Access Advisor, which an administrator can use to review if the user is regularly using the permissions or not and depending on that can review to keep the permission or remove it. AWS IAM users also have a property called security credentials, which tells if the user has a password or not and also if MFA is enabled or not, we can also see the access ID but not the key, and we can add the ssh keys for code commit. Each action done on AWS is done by some type of user. Reference Photo by Ales Nesetril on Unsplash AWS Docs | What is IAM?","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-iam-users.html","loc":"https://www.archerimagine.com/articles/aws/aws-iam-users.html"},{"title":"AWS Made Easy | IAM Policy","text":"IAM Policy A document which provides the details of the permission granted to access any AWS resources is called an IAM Policy. The default policy applied to all AWS users is non explicit deny . The IAM Policy takes effect as soon it is attached to a user or group, there is no delay in its application. The User & Groups can have more than 1 policy attached at any given time. Roles are utilized because we cannot attach policy directly to AWS resources. IAM Policy Document How does an IAM Policy look like. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"*\" , \"Resource\" : \"*\" } ] } The above is an example of an IAM Policy If we observe closely, these are the various important points to note about the IAM Policy. Statement Each IAM policy is composed of statements . Policy can have one or more statements. In the above example, there is only 1 statement, composed of Effect It tells if the impact is allowed or deny. It takes these 2 values. In the above example, it is Allow for all. Action What type of action is allowed or denied. We can drill it down to single API also, which we will see later. In the above example, it is allowing all actions. Resource Which resource are being accessed using the policy. Like in case of S3, it can mention the resource is a S3 bucket, not the full S3. In the above example, all the resources are being granted access. Explicit Allow & Explicit deny When granting access explicitly we have 2 categories of IAM Policy Explicit Allow Explicit Deny When a user has both explicit allow and explicit deny, then the explicit deny always take precedence. In all cases just remember deny is the best way. Example of explicit allow { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"*\" , \"Resource\" : \"*\" } ] } Example of explicit deny { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Deny\" , \"Action\" : \"*\" , \"Resource\" : \"*\" } ] } IAM Policy Templates We can have an IAM policy in two ways. Pre-Built Policy Custom Policy Pre-Built Policy These are the policies provided by AWS to all the users. We can just pick and use which ever we like. These policies are identified with an Amazon logo just next to them. Most commonly used pre-build polices are Administrator Access Power User Access Read-Only Access Administrator Access Administrator Access: Full Access to all AWS resources. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"*\" , \"Resource\" : \"*\" } ] } Power User Access Admin Access, except it does not allow user-group management. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"NotAction\" : [ \"iam:*\" , \"organizations:*\" , \"account:*\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:CreateServiceLinkedRole\" , \"iam:DeleteServiceLinkedRole\" , \"iam:ListRoles\" , \"organizations:DescribeOrganization\" , \"account:ListRegions\" ], \"Resource\" : \"*\" } ] } Read-Only Access Only View AWS resources. This policy is very big, please search for it in AWS. - The AWS managed policies cannot be edited, they are read only. Custom Policy Some time the requirements of a policy cannot be fulfilled from an AWS managed policy. We have to use a custom policy in those cases. We can import the pre-built policy into our own, and then modify them. Custom IAM Policy Generation In addition to AWS provided policy, user can also create custom policy. We have 2 methods to create custom policy. Visual Editor JSON Editor Visual Editor The are 4 selections, we have to do for creating a policy, as shown below. Service Choose a service like, IAM, S3, EC2 on which the policy is applicable. Actions Based on the service selected, we can choose the Actions, which can be associated with it. Primarily the actions are List Read Tagging Write Certain service have certain extra Actions associated with itself. Resources We can restrict the Actions allowed on Services selected to specific resources. We may not have to provide the full access. The access can be restricted to even a particular ARN. Request Conditions The policy can also specify if the MFA is required for the access Only a particular public IP should be We also restrict using the time or the day and various other parameters. All the conditions if selected is AND ed We can also take a easy way out, by \"Import Managed Policy\" and then modifying it to our need. After selecting the above information we are only left with providing Tags Review JSON Editor We can use JSON editor if we understand the JSON syntax, and the format in which AWS wants the KEY/Value pairs. We can also import an existing policy and then edit the JSON. Conclusion To summarize, IAM policy is a document representing the permission encapsulated inside for a USER or a Role. The default policy on IAM is non explicit deny, but though IAM we can give either an explicit deny or an explicit allow. The explicit deny always takes precedence over any explicit allow. IAM policy are categorized into either managed by AWS, or user created. There are many AWS managed policy and the most useful once are Administrator policy, Power user policy and Read Only Policy. The IAM policy is a JSON statement, and having these 3 statement at the minimum Effect Action Resource IAM policy can be created by using the custom visual editor or the IAM JSON editor. Generally for new users the visual editor is beneficial once the user becomes comfortable, we can change to JSON editor. Multiple policy can be applied to Groups and Users. Roles are used for AWS resources. Reference Photo by Ales Nesetril on Unsplash Creating IAM policies (console)","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-iam-policy.html","loc":"https://www.archerimagine.com/articles/aws/aws-iam-policy.html"},{"title":"AWS Made Easy | IAM Introduction","text":"IAM Introduction IAM provides the interface to join the great world of AWS. Its major goal is to provide both human and computer, who are the consumers of AWS services a way to access them. IAM Components IAM service have five(5) components:- User - The user of AWS resource, can be a person or a machine Groups - The above user can be grouped together Policy - It defines the permission of the IAM identity. Roles - A roles are just a policy, but not associated with the Users directly. API Keys - The keys used to programmatically accessing the AWS console. IAM property AWS works on two very important principles. Principle of least privilege. Any user, group must be granted minimum permission to complete the activity. Non explicit deny rule. We are new user have no explicit allow rule for a resource, AWS assumes it to be access denied. Only an explicit allow can override a non explicit deny rule. Admin Vs root User root user This is the very first user which is created when we sign-up for AWS using our emailID. This user has god like access and is not under the control of IAM. This login must never be used for daily activities within AWS. These 5 steps should be done as soon as we login to AWS root account. Delete the root access keys. Enable MFA Create user Create groups and assign user to groups Apply an IAM password policy Admin user We can create a copy of the root user with complete admin access. This is the Admin user. This is user comes under the purview of IAM and should be used as a daily driver in AWS. Conclusion When we login to the AWS console for the first time we create the root user, this is the god like user which has all the access and have no control via the IAM policies. This user is not supposed to be used for day to day activity, in place we should be using another user called admin which has all the policies similar to root user but is controlled via IAM. There are a few important steps to perform as soon as we login to the AWS console for the first time, which is to delete the programmatic access key for the root user followed by creation of users and groups. The policy to be attached to groups and user can be part of groups. There also should be a password policy specified for the users. AWS works on 2 important principle, Principle of least privilege Non explicit deny rule This means that, all the user should be provided the bare minimum of the access required to get the job done, and if not specified, by default the user does not have access to any resources on AWS. Reference Photo by Ales Nesetril on Unsplash","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-iam-introduction.html","loc":"https://www.archerimagine.com/articles/aws/aws-iam-introduction.html"},{"title":"AWS - Shared Responsibility & Global Infrastructure.","text":"AWS - Global Infrastructure AWS is a cloud infrastructure provider, it's a pioneer in this field. Cloud and AWS have become synonyms, it would be difficult for a developer today to be unaware of AWS or cloud in general. This blog is an effort of mine to present this vast and ever expanding concept into very easy and fun method. Learning new concepts in their 40's is very difficult, would try to make it as easy as possible for anyone want to embark on this journey. AWS/Cloud has become an indispensable part of the Software industry due to a few of these important properties. Global Deployment Millisecond Latency Commission and decommission at will Pay as you use. Even if core AWS services are not available in certain location, it still uses different non AWS services to enable developer to relax and focus on developing great software. User Type There are 2 types of AWS users. root user The user which is created when we first become an AWS consumer by signing in. This user has God like permission, and is not governed by IAM rules. It is not safe to use this user for day to day activity. Administrator user This is the user which is created though the IAM. This controls the management of other user. This is the user which should be used for daily activity. It has all the access just like root user. Best Practices Always enable MFA for all users. Have an explicit password policy. i.e. expiry of password. Connection Type There are 3 ways to connect to AWS AWS Console The default mode, which we all use. This is the website for AWS, which can do pretty much all the stuff possible. AWS SDKs These are developers to make use of all the AWS resources while developing the application. Major programming languages are supported. AWS CLIs This use command line interface to control the AWS infrastructure. Service Location AWS hosts its service in two types of location AWS Edge Location These does not host any specific AWS service. These hosts only CDN or CloudFront. AWS region These are the real AWS data centers, which host all AWS services. Each region is further divided into an Availability Zone (AZs). AZs are created to provide fault tolerance and high availability. Most services are region specific, which means all the AZs will have that service. AWS - Shared Responsibility Model AWS provides us with the bare metal infrastructure for running our application on their server. This does not imply that outside our application code, we can rely only on AWS. AWS and us as a user work in a shared responsibility model, wherein, we have a certain responsibility and AWS has certain, and AWS is willing to shoulder certain responsibility with increased cost. AWS's responsibility is broadly related to the physical server's like, physical security, personnel security, discarding the storage devices, DDoS protection, and hyper visor isolation. User's responsibility is mostly towards their application like, proper security groups, IAM, MFA, OS patches and VPCs. Compute Services The major compute services provided by AWS are. Elastic Compute Cloud (EC2) Elastic Container service (ECS) AWS Lambda Elastic BeanStalk In the above 4, the user has the maximum control of configuration on EC2 as its bare metal server, with some services running. Lambda and Beanstalk provide the minimum configuration user and most of them are managed by AWS. Storage Service Just like computer the major storage service provided by AWS are * RDS - SQL database. * DynamoDB - No SQL database * RedShift - Data Warehousing * ElastiCache - In memory Cache. Summary AWS is a global infrastructure, it has to serve both its developer and consumer with great efficiency and low latency. This is the reason the AWS has its core services spread across the globe called the AWS region. It also has its edge location spread across the globe to serve the consumer. AWS region is subdivided into availability zones (AZs), which helps to provide high reliability and are connected to each other via a very low latency fiber optic. When an AWS service is mentioned to be region specific, it implies that it is available in all the AZs. There are two types of an Administrator user of AWS service. The first is the root user which has infinite God like permission and is not controlled via the IAM. There is also a admin user which we create from IAM. The admin user have all the permission like the root user, its is just that it was created by IAM. The user admin or root both should have MFA enabled and also have a password policy associated with it. The AWS services can be accessed using 3 methods. AWS Console AWS SDKs AWS CLIs All of the above will use the AWS APIs to connect with the core services. AWS Console is what every start using first and once becoming comfortable they rely on only SDKs or the CLIs. AWS has a shared responsibility with the users, where AWS manages most of the physical security and the server management like DDoS protection and HyperVisor isolation, the user is entrusted with the responsibility of managing the software securities like IAM, MFA, OS Patches, Security groups etc. What is the difference between the AWS Region and AWS Edge Location? When explaining the AWS Global Infrastructure these 3 Keywords are often used. Region Availability Zone (AZs) Edge Location Each of the above are key pieces of the AWS Infrastructure, but to understand the fundamental difference we have seen these with a different perspective. The above are divided based on User of the content present on AWS (End User) User of the content provider to AWS (Developer) Region and AZs as already explained are Developer focused, as a result, they are very few and have to adhere to the highest standard of scrutiny. Edge Location on the other hand, are User focused, Like CDN, Global Accelerator etc., their job is to make the content reach to and fro from the user faster, reduce the latency. As a result are far too many and spread across the world where it can serve its user better. Reference Photo by Eberhard Grossgasteiger on Unsplash Global Infrastructure Shared Responsibility Model Info-graphics","tags":"aws","url":"https://www.archerimagine.com/articles/aws/general-aws.html","loc":"https://www.archerimagine.com/articles/aws/general-aws.html"},{"title":"What is the cost of len() function in python?","text":"","tags":"Python","url":"https://www.archerimagine.com/articles/python/cost-of-len-function-in-python.html","loc":"https://www.archerimagine.com/articles/python/cost-of-len-function-in-python.html"},{"title":"100Days of Code Log File 4th Attempt","text":"Hello World!, You are about the witness the beginning of an epic second coming of the 100-Day coding journey, A story that great sages will pass down from generation to generation. This quest will feature a potpourri of unfiltered joy, unrivaled pain, and unexpected epiphanies. Some moments, I will be the smartest man alive. Others moments, I will be a stupid idiot. But each day, I will be a valiant warrior, fighting to develop and perfect the skills necessary to evolve into a true beast with these keys. I have failed in my previous attempt for the challenge, which you can find here . There are learning from the previous failure, here are the modification which was done to the challenge according to my handicap. Ladies and gentleman, I present to you, #100DaysofCode with @ animeshkbhadra Day 02 | Thursday 07 May 2020 Days Progress Completed the CSS Trasform lecture from Udemy . It was pretty amazing to learn about various type of transforms Thoughts These are the transform's learned. rotate : rotates the object. skew : skews the object. scale : zoom by a factor. We can also combine transition with transforms to give a nice animating effect. translate3d takes a mandatory z-index as a paramter. Reference Day 01 | Wednesday 06 May 2020 Days Progress I am learning HTMl/CSS from the Udemy . I started this course, some days back, from now, will post regularly on behalf of #100DaysOfCode. Today I have completed the CSS Transition. Thoughts Today I learned about:- The CSS transition property. Link to Tweet Reference QuoteFancy | Image Source 100DaysOfCode Official Website Udemy","tags":"#100DaysOfCode, python","url":"https://www.archerimagine.com/articles/100daysofcode-python/100Days-of-code-log-file_V_4_0.html","loc":"https://www.archerimagine.com/articles/100daysofcode-python/100Days-of-code-log-file_V_4_0.html"},{"title":"How to use Sublime Text 2 as a MarkDown Editor.","text":"Sublime Text Sublime Text is a great editor for code and text. There is nothing I can add to the above fact. Markdown is a great format to write the ReadMe's in GitHub, and a lot of other social media also allows MarkDown Format. Sublime Text 2 have basic code snippets triggered for MarkDown Editing. This itself is a huge plus for using Sublime Text for MarkDown Editing though the support is limited to these 6 Snippets, as a user we can always enhance these. The basic assumption I am making that you have the Package Control Plug-in installed. Sublime Text 2 with the help of these 2 Plug-in makes Markdown Editing work like a charm. * MarkdownEditing * InsertDate Plug-ins MarkDown Editing The basic support from Sublime Text 2 is enough to use it as a MarkDown Editor but the MarkdownEditing provides great support in terms of. Paring of Asterisks, underscores, brackets Creation of Numbered list and Un-numbered list, with tab support of Indents. Great key bindings. Good theme with decent syntax highlighting, though I will propose read the blog in the Reference section. These basic settings in helps a lot in MarkDown editing. Save it in Markdown (Standard).sublime-settings file. { # Sets the Color Theme. \"color_scheme\": \"Packages/MarkdownEditing/MarkdownEditor-Dark.tmTheme\", # Sometime md is not recognized as a MarkDown Extensions. \"extensions\": [ \"md\" ], # Enable Spell Check. \"spell_check\": true, # If you want the text to start in the left hand corner, default is Centered. \"draw_centered\": false, \"wrap_width\": 180 } Kindly visit the documentation in the GitHub page for more configuration and settings. Insert Date When writing a blog post using MarkDown we need to enter Date in the present Blog. This plug-in InsertDate is a great way to insert localized time into any documents based of few KeyStrokes. This plug-in is vastly configurable, so we can customize completely to our needs. Change this configuration in the file insert_date.sublime-settings { # Mention your local Time zone \"tz_in\": \"Asia/Kolkata\", # Sunday July 28,2019 Prints in this format \"format\": \"%A %B%e,%Y\" } strftime reference and sandbox , use this to format the date according to your need. The default Key combination to trigger this is [Ctrl + f5] [Ctrl + f5] . This works on all platform except Mac, as it triggers the VoiceOver App. Kindly see this issue for resolution. Conclusion In Conclusion, I will only try to mention that both, * MarkdownEditing * InsertDate are great Plug-ins to write MarkDown files, Lets use these to make a better ReadMe in Github. Reference How to Set Up Sublime Text for a Vastly Better Markdown Writing Experience MarkdownEditing InsertDate Default keybindings don't work on OS X Yosemite #28","tags":"SublimeText","url":"https://www.archerimagine.com/articles/sublimetext/how-to-use-sublime-text-2-as-markdown-editor.html","loc":"https://www.archerimagine.com/articles/sublimetext/how-to-use-sublime-text-2-as-markdown-editor.html"},{"title":"100Days of Code Log File 3rd Attempt","text":"Hello World!, You are about the witness the beginning of an epic third coming of the 100-Day coding journey, A story that great sages will pass down from generation to generation. This quest will feature a potpourri of unfiltered joy, unrivaled pain, and unexpected epiphanies. Some moments, I will be the smartest man alive. Others moments, I will be a stupid idiot. But each day, I will be a valiant warrior, fighting to develop and perfect the skills necessary to evolve into a true beast with these keys. I have failed in my previous attempt for the challenge, which you can find here . There are learning from the previous failure, here are the modification which was done to the challenge according to my handicap. Selected the resource in advance, Learn Python Track from Team TreeHouse FrontEndMasters | Python Fundamentals | Nina Zakharenko Complete first 21 Days first. Ladies and gentleman, I present to you, #100DaysofCode with @ animeshkbhadra Day 45 | Thursday August 8,2019 Days Progress Always group the custom exceptions into a common module called exception in application. The custom exception class name should always end with error or exception, signifying the purpose. Thoughts My Github Url Link to tweet Day 44 | Wednesday August 7,2019 Days Progress Error Specificity The order of catching Exception should be from specific to general. The code should never be catching generic Exception . Custom Exceptions We can raise our custom exception by inheriting from base class Exception . The base Exception __init__() takes a custom message. raise is the keyword to raise the custom exception. Thoughts My Github Url Link to tweet Day 43 | Monday August 5,2019 Days Progress Exception is a great way to make the application more robust. try/except are the tools to catch exceptions. BaseException is the base class of all exception which should never be caught in our code. Exception is the exception class which needs to be caught. except takes a tuple of exception. Thoughts My Github Url Link to tweet Day 42 | Thursday August 1,2019 Days Progress Inheritance is the best way to share property and responsibility across the code. Car has 4 wheels, but Bike has only 2 wheels, but most other parts are similar. It creates code re-usability. Divides the code from more generic to specific. Multiple inheritance is possible, but mostly restricted to Mixins. Thoughts My Github Url Link to tweet Day 41 | Wednesday July 31,2019 Days Progress str() : On our raw object, is we use str() it gives a completely non-relevant information. We can change this by overriding the __str__() method in our class. repr() : Its informs a way of creation of the object. We can override __repr__ method in our class for this information. Thoughts My Github Url Link to tweet Day 40 | Monday July 29,2019 Days Progress isinstance() : returns True if an object is an instance of a class. issubclass() : returns True is an object is a subclass of a class. bool is a subclass and instance of int object is the parent class of all object. any() : returns True , if any value in the collection is True. all() : returns True , if all the value in the collection is True. bool : bool being a subclass of int , we have some weird combination, which we can try. True + True : 2 {0,1, True, False} : returns {0, 1} Thoughts My Github Url Link to tweet Day 39 | Sunday July 28,2019 Days Progress Class methods are unique methods, which operates on class variables. @classmethod : a special decorator to create a class method. Class methods take cls as argument and not self . The instance object can access the class methods, since it is aware of their existence. Thoughts My Github Url Link to tweet Day 38 | Saturday July 27,2019 Days Progress The journey into object oriented programming with python. Python has no protection of its class variables for modification unlike Java. __init__(self) It is special function which python calls under the hood when initializing a object. It takes self by default. Thoughts My Github Url Link to tweet Day 37 | Friday July 26,2019 Days Progress OOPs concept. Everything is an object in Python. Class is template, blue print of a object. Instance is a specific creation of a class. We can have both Class Variables Accessed by class. Instance Variable Accessed by instance of a class. self is a special name given to a instance in python. Thoughts My Github Url Link to tweet Day 36 | Thursday July 25,2019 Days Progress .items() on a dict() returns a list of tuples. A list of tuples can also be converted back to a dictionary. zip() function combines two lists into a list of tuples. for loop can be used to iterate over a zip function. Using zip() on asymmetrical list will create a list of tuples with the least no of elements common in the list. dict(zip(list1, list2)) : converts a list of tuple back to a dictionary. Thoughts My Github Url Link to tweet Day 35 | Wednesday July 24,2019 Days Progress Enhanced the learning of list slice. Simple index gives an element of the list. The slice has the following format. [1:3] : the index will start from 1, ends at 3 - 1 = 2 No of elements will be 3 - 1 = 2 Slice also supports negative index. -1 : give the last element -len(list) : gives the first element Slice also has 3rd index, which is step . [::2] : skips 2 elements. [::-1] : short cut to reverse a list. Thoughts My Github Url Link to tweet Day 34 | Tuesday July 23,2019 Days Progress Today is the day, when I am more confused than aware. There is no tuple comprehension in python, but there are Generator expressions. Generator expressions are created with this syntax. (num * num for num in range(11)) Ideally we expect it to be a tuple comprehension, but it is a generator object. Generator expressions are a memory efficient way of creating a big list. We cannot index on the generator object, gives a TypeError . next() and for/each can be used to iterate over a generator. Thoughts My Github Url References. Generator Unpacking Why is there no tuple comprehension in Python? Link to tweet Day 33 | Monday July 22,2019 Days Progress The set and dict comprehensions are very similar in syntax. set comprehensions. {num * num for num in range(11)} dict comprehensions. {num: num * num for num in range(11)} Both set and dict comprehensions are not ordered. Thoughts My Github Url Link to tweet Day 32 | Friday July 19,2019 Days Progress Built-in function which works on the list sum() : returns the sum of a list min() : returns the smallest element in the list. max() : returns the biggest element in the list. sorted() : returns a sorted list. reverse=True as a default argument will reverse the list. Thoughts My Github Url Link to tweet Day 31 | Thursday July 18,2019 Days Progress Continued with List comprehensions. We can use conditionals in list comprehension to filter values. [num**2 for num in numbers if num %2 == 0] The conditional comes at the end. This conditional works as a filter. We have 3 ways to use List comprehensions. Map - All values are used Filter - Some values are used Map/Filter - Some values filtered and then modified. Thoughts My Github Url Link to tweet Day 30 | Tuesday July16,2019 Days Progress There are various type conversion which we can use. split() : convert string to a list. join() : convert a list to string. \",\".join(my_list) tuple unpacking can also be used when using .split() Learned about list comprehensions. [num**2 for num in numbers] : list of square of numbers. We right the square brackets first Followed by the for loop. The variable used in the for loop is available to the left of loop, num Apply mapping on the variable left of for loop. We can combine with tuple and f strings to get more valuable information. Thoughts My Github Url Link to tweet Day 29 | Saturday July 13,2019 Days Progress Worked on the Github Api Project using these concepts. requests exceptions APIs Query Parameters. Thoughts My Github Url Link to tweet Day 28 | Friday July 12,2019 Days Progress learned mostly about requests library. 4 HTTP methods GET POST PUT DELETE HTTP code 1XX : Information 2XX : Success 3XX : Redirection 4XX : Client error 5XX : Server error Thoughts My Github Url Link to tweet Day 27 | Thursday July 11,2019 Days Progress __name__ is a nice way to put code when we want to execute as a script. The above code is not invoked when called as a library. try/except is a great way to catch error's in code, and give alternate execution path. Thoughts My Github Url Link to tweet Day 26 | Wednesday July 10,2019 Days Progress Mostly learned about python file's and debugging techniques. Thoughts My Github Url Link to tweet Day 25 | Tuesday July 9,2019 Days Progress while loop is used to iterate over numbers in place of sequence. Also when we are not sure about the no of iteration break : It stops the execution of loop, and jumps to end of loop. If present in nested loop, it breaks the loop it is present in. continue : It stops the loop execution and jump to start of the loop. return : It also return the loop control to outside of the loop. Thoughts My Github Url Link to tweet Day 24 | Friday July 5,2019 Days Progress Today's learning was mostly on the Control flow. We have different variant of if condition. if if-else if-elif-else We can also check truthiness. Thoughts My Github Url Link to tweet Day 23 | Thursday July 4,2019 Days Progress Continuing the process of learning loop, understood about looping with dict There are mainly 3 looping with dict .key() - this is default, so do not have to specify in for loop. .items() - returns a tuple of key, value pair. .values() - returns only the values in dict . .enumerate(.items()) - gives, index and a tuple of key value. Thoughts My Github Url Link to tweet Day 22 | Wednesday July 3,2019 Days Progress Mostly improved the understanding of loops with list and range function. for loops create a temporary variable which is in scope outside of for loop. range() is a good function for looping, it has 3 variant. range(5) : default, creates the number from 0 to 4 range(1,5) : start and end index, creates the number from 1 to 4, 5 non inclusive. range(1,5,2) : the 3rd argument is steps , will step those many numbers. It does not take any Keyword arguments. Thoughts My Github Url Link to tweet Day 21 | Monday July 1,2019 Days Progress I have reached that day, where it is said, 21 days is required minimum to form a habit. Hope this habit stays with me. Learning continued on and , or , and not operator. It was an eye opener. and and or returns the value of one of the expression and not True or False and It returns the value of 2nd operand if the first operand is True else value of first. or It returns the value of 1st operand if it evaluates to True , else value of second. not Returns the inverse of the operator. Thoughts My Github Url Link to tweet Day 20 | Sunday June 30,2019 Days Progress Comparison operators in Python. < : Less than > : greater than <= : less than equal to >= : greater than equal to == : equal to != : Not equal to is : Identity, when both object points to same. Strings are compared based on their ASCII value. The capital letters are smaller than small letters. Thoughts My Github Url Link to tweet Day 19 | Friday June 28,2019 Days Progress What are the truthiness of various data types. integers: 0 is False any other number is True Collections: Empty list, tuple, Dictionary, sets are False Non Empty collections are True Strings: Empty String is False Non Empty String are True . None is False Thoughts My Github Url Link to tweet Day 18 | Wednesday June 26,2019 Days Progress Learned about the list slice. my_list[0:3 : Returns 0 - 2nd index my_list[:] : clone the entire list. my_list[-1] : Special way to get the last item. Thoughts My Github Url Link to tweet Day 17 | Tuesday June 25,2019 Days Progress Finally crossed the last attempts days. Learned about mutability. Basic data type are immutable. int , float , decimal , str , bool Containers data type are divided list , set , dict are mutable tuple is immutable. Thoughts My Github Url Link to tweet Day 16 | Monday June 24,2019 Days Progress Adding/Accessing dictionary elements. add new key/Value pair. nums[\"four\"] = 4 There are no duplicate key in Dictionaries. If new value is assigned to same key, it will override the old value. Existence of a key in Dictionaries. \"one\" in nums .update() : Combine two list. 3 important functions on Dictionaries .keys() : returns special list called dict keys .values() : returns a special list called dict values .item() : returns a list of tuple, called dict items Thoughts My Github Url Link to tweet Day 15 | Sunday June 23,2019 Days Progress Started learning about dictionaries. Dictionaries store key:value pair. Dictionaries are mutable but the keys are immutable. The search is very fast, just like sets . Retrieve the value with index as the key a[\"one\"] get() method can be used when we do not want an error while retrieving a value. Its returns a default value if the key is not present. Thoughts My Github Url Link to tweet Day 14 | Saturday June 22,2019 Days Progress Important set operation. union() or | : Returns the union of two sets. intersection() or & : Returns the intersection of two sets. difference() or - : present in 1 set but not in other. Thoughts My Github Url Link to tweet Day 13 | Thursday June 20,2019 Days Progress Updated my RAM with set operation for CRUD. add() - Adds item to the set. discard() - Removes item from the set, if not present, gives no error. remove() - Removes item from the set, if not present. gives KeyError . update() - Adds item from a sequence into a set. Thoughts My Github Url Link to tweet Day 12 | Wednesday June 19,2019 Days Progress Understood the basic premise around sets. Empty sets can only be created using set() function, empty {} creates a dict. Sets stores only immutable data type which can give a hash() value. a = {\"a\", (1, 2, 3), [1, 2, 3]} # TypeError: unhashable type: 'list' Sets are used to remove duplicates from List. Sets searching is very fast. Sets do not have a indexing order. Thoughts My Github Url Link to tweet Day 11 | Tuesday June 18,2019 Days Progress Explored the different ways to create a tuple Create a empty tuple. a = tuple() b = () Create a single item tuple. a = (1,) and not a = (1) Brackets are not mandatory for tuple. b = 1, 2, 3, 4, 5 indexing in tuple, same as list. b[0] Thoughts My Github Url Link to tweet Day 10 | Monday June 17,2019 Days Progress Established some basic understanding on list operations Operations to add item to the list. append() : add an item to end of the list. insert(2, \"bbbb\") : insert at an index. extend() : concatenates two list. Operation on list look-up, which is very slow, almost linear. index(value) : returns first index of value ValueError: if the value is not present in list. count(value) : returns the no of times a value is present. return's 0 is the value is not present. Operation to remove item from list. remove(value) : removes the value from the list, if not present does not throw error. pop() : remove and returns the last element of the list or the index. List are heterogeneous. Thoughts My Github Url Link to tweet Day 09 | Sunday June 16,2019 Days Progress Understood the list's sort and the built-in sorted function. sorted() returns a list list.sort() or list.reverse() returns None Thoughts My Github Url Link to tweet Day 08 | Saturday June 15,2019 Days Progress Started after a gap of 1 cheat day. List was the focus today. list can be created using [] or list() list is a ordered collection. list is a heterogeneous collection. list elements can be accessed using index start at 0 List has 1 efficient way of declaring. names = [ \"XXX\" , \"YYY\" , \"ZZZ\" , # unlike json, we can have comma at the last element, # it helps with git diff ] Thoughts My Github Url Link to tweet Day 07 | Thursday June 13,2019 Days Progress Function scope is little confusing without practice. There is a global scope and a local scope to a function. If same variable name is same, local scope gets preference. Global variable cannot be modified even thought it share the same name. This code will work. But this code will not work, gives UnboundLocalError: . The explanation is mentioned in the Python Documentation . Thoughts My Github Url Link to tweet Day 06 | Wednesday June 12,2019 Days Progress List or any other mutable data type should not be used as the default arguments. The list is initialized when the function is called the 1st time, and then it modifies the same list. Thoughts My Github Url Link to tweet Day 5 | Tuesday June 11,2019 Days Progress Started with the Function's Arguments . Positional arguments must be passed to functions. Default arguments are always provide at the end of function's argument list. We can give none , one , all arguments to a function with only default arguments list. Labeled arguments can be passed in any order to a function. Thoughts My Github Url Link to tweet Day 04 | Monday June 10,2019 Days Progress Started the functions section of the lecture. This lesson, teaches about different function type. Function with no arguments and no return type Function with no arguments and a return type Function with 2 arguments and a return type Function with multi-line function body. return is always optional in function, it returns <class 'NoneType'> Thoughts The function returning <class 'NoneType'> was a eye opener. My Github Url Link to tweet Day 03 | Sunday June 9,2019 Days Progress Completed the Data Type chapter in the FrontEnd Master's Python fundamentals. This chapter introduces some nice concept about data types 45j is a complex data type <class 'complex'> but not 45i , so j is the identifier for complex number. new_name f\"Hello, {name}\" is a format string, name in {} is the variable name which will be replaced. Same variable can be used to store number, or strings. String can be created by both 'string 1 ' or \" String 2 \" Integer division gives the result in floating point 3/2 = 1.5 Thoughts Python Data types have lot of power inside them with very less ambiguity. My Github Url Link to tweet Day 02 | Saturday June 8,2019 Days Progress Understood the VSCode basic settings, got help from a great tutorial by Corey Schafer The tutorials talks about these topics Change the way settings is displayed as JSON, in place of UI. Select virtual environment. Change color theme. Change file icons. Set the global python path - \"python.pythonPath\": \"<Path>\" Set a global python file formatter, we are using Black for this. \"python.formatting.provider\": \"black\", Also change the option to run the formatter on saving the file. \"editor.formatOnSave\": true, Enable Linting. Git Integration. Unit Testing. Thoughts VScode has lot of power, lets see how much I learn from it. My Github Url Link to tweet Day 01 | Friday June 7,2019 Days Progress Started the Python Fundamentals course by Nina Zakharenko. Today's main focus was setting my these things. Virtual environment. VScode setup. Faced few issues, which stackoverflow helped in solving Issues and Solutions There was no activate script when the virtual environment was created by using the command python -m venv .env On doing Google for the problem found that running the same command again solves the issue, so ran python -m venv env again and viola the activate script appeared. VScode was not recognizing the virtual environment created inside a sub folder in the project. Deleted the pre-existing environment and created a new virtual environment at the project root. Thoughts Programming is just efficient Google technique. My Github Url Link to tweet Reference QuoteFancy | Image Source 100DaysOfCode Official Website Learn Python Track from Team TreeHouse MIT 6.00SC Introduction to Computer Science and Programming Create a Tweet With image Preview for Free FrontEndMasters | Python Fundamentals | Nina Zakharenko Corey Schafer","tags":"#100DaysOfCode, python","url":"https://www.archerimagine.com/articles/100daysofcode-python/100Days-of-code-log-file_V_3_0.html","loc":"https://www.archerimagine.com/articles/100daysofcode-python/100Days-of-code-log-file_V_3_0.html"},{"title":"100 Days of Discrete Maths.","text":"Discrete Maths is a study of things which are discrete, which means things which can be counted. Discrete Maths forms the basis of a lot of concepts in algorithms and Computer science in general. I am starting this #100DaysOfX which Discrete Maths, to have a sufficient understanding of the concepts. I am using resources from NPTEL, MIT OCW, and ArsDigita University. The plan is to follow the 3 lectures mentioned in the references and the Rosen book on Discrete math. Day 16 | Thursday 21 March 2019 Days Progress Just a revision of implication, exclusive OR. Thoughts link to tweet Day 15 | Wednesday 20 March 2019 Days Progress Completed the logical inference lecture from NPTEL's Discrete Maths Thoughts Learned about:- logical inference for propositional calculus Fallacy logical inference for Quantifiers Normal Forms CNF DNF link to tweet Day 14 | Tuesday 19 March 2019 Days Progress Revision of Preposition calculus from Rosen Book. Thoughts Books are easier to understand once, the concept is understood. link to tweet Day 13 | Monday 18 March 2019 Days Progress Revision of lecture 4 from NPTEL's Discrete Maths Thoughts Learned that Implication and Equivalence are not the same. Understood about logical relationship involving quantifiers. link to tweet Day 12 | Wednesday 13 March 2019 Days Progress Revision of lecture 4 from NPTEL's Discrete Maths Thoughts Now learned properly about Predicates and Quantifiers. Scope of Quantifiers. Valid , Satisfiable and unsatisfiable predicates. link to tweet Day 11 | Tuesday 12 March 2019 Days Progress Again listened to the 4th lecture of NPTEL's Discrete Maths Thoughts Learned about Logical Inference. link to tweet Day 10 | Monday 11 March 2019 Days Progress Listened to the 4th lecture of NPTEL's Discrete Maths Learned a little about logical inference. Thoughts Still confusion over Predicate and Quantifiers. link to tweet Day 09 | Thursday 28 February 2019 Days Progress Reading and listening to explanation on how to negate a quantifiers. Learning about scope of a quantifiers. Thoughts Still have doubts on these topics. link to tweet Day 08 | Wednesday 27 February 2019 Days Progress In between the 4th Lecture of NPTEL's Discrete Maths Thoughts Great lecture on predicate logic Learned about:- Valid Expression Satisfiable Expression Unsatisfiable Expression These addition video's also helped. Universal and Existential Quantifiers [Discrete Math 1] Predicate Logic and Negating Quantifiers link to tweet Day 07 | Tuesday 26 February 2019 Days Progress Listened to the 2nd lecture of MIT 6.042J YouTube play-list Thoughts This lecture discusses about Proof by Contradiction Introduces to the concept of Induction proof. link to tweet Day 06 | Monday 25 February 2019 Days Progress Completed the 3rd Lecture of NPTEL's Discrete Maths . Thoughts Today I learned about:- Predicate and Quantifiers. Predicate Predicate Logic n-ary predicate Quantifiers Universal Existential Binding Variables Logical equivalence involving quantifiers. link to tweet Day 05 | Friday 22 February 2019 Today was a rest day for Discrete Maths. Day 04 | Thursday 21 February 2019 Days Progress Listened to the 2nd lecture of MIT 6.042J YouTube play-list This lecture discusses about Proof by Contradiction Introduces to the concept of Induction proof. Thoughts Today was focused on listening to the lecture, so have not taken detailed notes. link to tweet Day 03 | Wednesday 20 February 2019 Days Progress Completed the 2nd Lecture of NPTEL's Discrete Maths . Thoughts Today I learned about:- Proving implication without drawing all possible rows of truth table. Proved that implication is not associative. Learned about logical identities. Simplified complex compound propositions. Conversion between English to logic and vice versa. Rules of inference Modus Ponens Modus Tollens link to tweet Day 02 | Tuesday 19 February 2019 Days Progress Complete the First lecture of MIT 6.042J YouTube play list Thoughts I have not taken any notes, but the lecture was mostly focused on methods of proof, propositions and connectives. link to tweet Day 01 | Monday 18 February 2019 Days Progress Complete the First lecture of NPTEL's YouTube play list . Thoughts This lecture covers these topics:- Logic Propositions Logical Connectives ( \\(\\&\\) , \\(|\\) , \\(\\sim\\) ) and its truth tables Implication. ( \\(\\Rightarrow\\) ) Equivalence. ( \\(\\Leftrightarrow\\) ) Tautology, Contradiction & Contingency. Logical Identities Understanding Equivalence and Implication was little tough. These 2 video's provided the additional help. Rachel's Discrete Math Course - Implications (Lecture 5) Propositional logic | first order predicate logic| Propositional calculus | gate | net - part 5 Link to Tweet Reference NPTEL | Computer Sc - Discrete Mathematical Structures | Prof. Kamala Krithivasan ArsDigita | Discrete Mathematics and Its Applications | Rosen | Shai Simonson MIT 6.042J | Mathematics for Computer Science, Fall 2010 | Tom Leighton, Marten van Dijk Amazon | Discrete Mathematics and Its Applications (SIE) | Kenneth Rosen Latex | Math Symbols Pelican and Math Equations","tags":"#100DaysOfDiscreteMath","url":"https://www.archerimagine.com/articles/100daysofdiscretemath/100Days-of-DiscreteMath-log-file.html","loc":"https://www.archerimagine.com/articles/100daysofdiscretemath/100Days-of-DiscreteMath-log-file.html"},{"title":"100Days of Vim Log File","text":"I am starting this new journey into the world of VIM, with a hope that this time the mistakes of my past attempts will be rectified. This journey's goal is pre-decided so that there is no deviation from the plan of learning VIM. In the last attempt of learning VIM, I was doing the classic mistakes of learning vim as described by Mr. Bram Moolenaar in this video, that is, Learn every feature the editor offers and use the most efficient command all the time. The approach this time will be learning a little bit of commands in VIM and apply it daily for a few days, as it becomes part of the muscle memory move to the next set. In addition, I have also set my goals for this 100 Days to have a razor sharp focus and not deviating. The goal of this 100DaysofVim are:- Edit text effectively. Scroll and move in a file quickly. Navigate source code with ctags and key board shortcuts. Edit multiple files using buffers. No use of any vim plugins. Read and understand the vim help system. Integrate debugging with source code navigation in VIM. Day 16 | Thursday 21 March 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help about Editing a file Saw 1 screen cast from VimCast | Episodes Thoughts Nothing significant progress today. link to tweet Day 15 | Wednesday 20 March 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help about Starting Vim Saw 1 screen cast from VimCast | Episodes Thoughts Understood about the default spell checking mechanism of VIM. Will post the key bindings soon. link to tweet Day 14 | Tuesday 19 March 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help about Special Ex characters Saw 1 screen cast from VimCast | Episodes Thoughts Most of the Special EX char, can be used with the :edit command The Vimcast from wrapping and this completely went over the head, will revisit again. link to tweet Day 13 | Monday 18 March 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help about Ranges Saw 1 screen cast from VimCast | Episodes Thoughts Understanding Wrapping is little difficult in VIM. link to tweet Day 12 | Wednesday 13 March 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help about Command-line editing Saw 1 screen cast from VimCast | Episodes Thoughts link to tweet Day 11 | Tuesday 12 March 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help about Various commands Saw 1 screen cast from VimCast | Episodes Thoughts Configured the netrw to behave like a File Explorer. Took help from these links Vim: you don't need NERDtree or (maybe) netrw Magic of netrw in Vim link to tweet Day 10 | Monday 11 March 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help about Quickfix commands Saw 1 screen cast from VimCast | Episodes Thoughts Quickfix commands We need a arguments called makeprg to be configured. C program's by default have make as the makeprg . Python program can configure makeprg as pyflakes We can even configure this pased on the ftplugin VimCast How to change directory while editing a file link to tweet Day 09 | Thursday 28 February 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Saw 1 screen cast from VimCast | Episodes Thoughts Learned about s : = xi : delete a char and insert mode S := &#94;C : delete line and insert mode. Read the Options help, did not understand and word. link to tweet Day 08 | Wednesday 27 February 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Saw 1 screen cast from VimCast | Episodes Thoughts Learned about g; and g, : Navigate the change list in forward and backward direction. CTRL + 0 , CTRL + I : Navigate the jump list in forward and backward direction. link to tweet Day 07 | Tuesday 26 February 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Saw 1 screen cast from VimCast | Episodes Thoughts Learned about session when reading the Key Mapping section of vim help. mksession hello.vim vim -S hello.vim link to tweet Day 06 | Monday 25 February 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Saw 1 screen cast from VimCast | Episodes Thoughts link to tweet Day 05 | Friday 22 February 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Saw 1 screen cast from VimCast | Episodes Completed the course Udemy | Vim MasterClass | Jason Cannon Received the course completion certificate. Thoughts Learned about the gVIM clipboard buffers \"+ and \"* Tab's command :tabe : open a tab with file name CTRL-W T : move current split into a tab. tabc : close current tab tabo[nly] : one 1 tab open. gT and gt : to switch between tabs. tabmove : move tabs Completed these help topics Visual Mode Text Object link to tweet Day 04 | Thursday 21 February 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Lecture on buffers from Udemy | Vim MasterClass | Jason Cannon Saw 1 screen cast from VimCast | Episodes Thoughts Learned about the various windows commands. CTRL +w s or :sp `: horizontal split CTRL +w v :vsp : vertical split :only : closes all window except the active one Navigation is done by CTRL + w w , CTRL + w h , CTRL + w j , CTRL + w k , CTRL + w l Resize windows CTRL + w + , CTRL + w - , increase or decrease the size by 1 line CTRL + W _ , CTRL + w | , increase size of current window in height and width Moving window is done by CTRL + w R , CTRL + w H , CTRL + w J , CTRL + w K , CTRL + w L like bufdo we have a command windo which works only on opened window. Studied the Complex Changes from vim helps, did not understood much from this. link to tweet Day 03 | Wednesday 20 February 2019 Today's Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Lecture on buffers from Udemy | Vim MasterClass | Jason Cannon Thoughts Learned about these buffers commands. :buffers or :ls :bn or :bnext :bp or :bprevious :bf or :bfirst :bl or :blast CTRL + &#94; : last open buffers :set hidden :qall! :wall! :badd :bd :bufdo :Explore Studied the Changing Text from Vim help. cc , S , C , s all work on line. CTRL + A and CTRL + X has a very nice implementation. :ce , :le & :ri changes the alignment of line, center, left and right. link to tweet Day 2 | Tuesday 19 February 2019 Today's Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Saw 1 screen cast from VimCast | Episodes Thoughts Studied the Copying and Moving text. section of vim help. _ behaves the same as &#94; without a count preceding it. When count is preceding it, this behaves as j . What does the underscore motion do in vim? While learning about vim help file, I found that all the commands which are similar are generally kept together. Link To Tweet Day 1 | Monday 18 February 2019 Today's Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Thoughts Few commands which was very good. Using C to change the text from the cursor till end of line. Using D to delete the text from the cursor till end of line. Few Insert Mode commands. CTRL-T : insert one shiftwidth of indent in front of line. CTRL-D : deletes one shiftwidth of indent in front of line. Link to Tweet References How To Learn Vim: A Four Week Plan 100daysOfX YouTube | 7 Habits For Effective Text Editing 2.0 VimCast | Episodes","tags":"#100DaysOfVim","url":"https://www.archerimagine.com/articles/100daysofvim/100Days-of-vim-log-file-V-2-0.html","loc":"https://www.archerimagine.com/articles/100daysofvim/100Days-of-vim-log-file-V-2-0.html"},{"title":"100Days of Code Log File 2nd Attempt","text":"Hello World!, You are about the witness the beginning of an epic second coming of the 100-Day coding journey, A story that great sages will pass down from generation to generation. This quest will feature a potpourri of unfiltered joy, unrivaled pain, and unexpected epiphanies. Some moments, I will be the smartest man alive. Others moments, I will be a stupid idiot. But each day, I will be a valiant warrior, fighting to develop and perfect the skills necessary to evolve into a true beast with these keys. I have failed in my previous attempt for the challenge, which you can find here . There are learning from the previous failure, here are the modification which was done to the challenge according to my handicap. Selected the resource in advance, Learn Python Track from Team TreeHouse MIT 6.00SC Introduction to Computer Science and Programming Practice the 100Daysofcode for a month before committing fully. Create a time table and sticking to it. Studying at the same time everyday, brains craves for learning python in that hour of the day. Missing few (~5) days is acceptable. Ladies and gentleman, I present to you, #100DaysofCode with @ animeshkbhadra Day 16 | Thursday 21 March 2019 Days Progress Completed few regular expression tutorials from TreeHouse Thoughts Learned about:- Negation [&#94;abc] - A set that will not match any char of these characters a , b and c re.IGNORECASE , re.I - Flag to ignore case while searching re.VERBOSE , re.X - Flag that allows regular expression to span multiple lines. Groups ([abc]) - create a group, that contains a set of letters a , b and c (P<name>[abc]) - creates a named group, member can be accessed using group('name') re.MULTILINE , re.M - flag to make a pattern having lines. &#94; - Beginning of line. $ - End of line. link to tweet Day 15 | Wednesday 20 March 2019 Days Progress Today did not get much work done, solved few exercises in solo learn. Thoughts link to tweet Day 14 | Tuesday 19 March 2019 Days Progress Completed few regular expression tutorials from TreeHouse Thoughts Learned about counts in regular expressions. We can create expressions like these \\w{3} - Match any 3 word char \\w{,3} - Match 0,1,2 or 3 word char \\w{3,} - Match 3 or more char, no upper limit \\w{3,5} - Match 3,4 or 5 word char \\w? - Match 0 or 1 char \\w* - Match 0 or more \\w+ - Match 1 or more We can also pass a variable in regular expression string. \"\\w{%s}\" %count %s - for string %d - for decimal %f - for float Sets basic was also checked. [abc] - set of char a,b,c [a-z][A-Z][a-zA-Z] - Char ranges [0-9] - digit [&#94;2] - Not 2 link to tweet Day 13 | Monday 18 March 2019 Days Progress Started the Chapter 01 from Head First Python. Thoughts Learned about 2 types of import s. Learned about these module, datetime , random , time link to tweet Day 12 | Wednesday 13 March 2019 Days Progress Practiced some of the learning from Regular Expression from previous Day. Thoughts link to tweet Day 11 | Tuesday 12 March 2019 Days Progress Started the Module of Regular Expression from TreeHouse Thoughts Learned about File read and write operations like open() close() read() In addition to these explored re.match() - Matches against the beginning of text re.search() - returns the first match location anywhere in the text. Few escape sequence \\w - Matches Unicode word char including numbers but excluding special character. \\W - Matches anything which is NOT Unicode word or numbers \\s - Matches all white spaces \\S - Matches anything which is NOT white spaces \\d - Matches numbers \\D - Matches NOT numbers \\b - Matches word boundary. \\B - Matches NOT word boundary. link to tweet Day 10 | Monday 11 March 2019 Days Progress Read the first chapter of Head first Python. Wrote few code from the book. Thoughts Learned about the datetime module. link to tweet Day 09 | Tuesday 05 March 2019 Days Progress Completed the module of Quiz Game in the course. TreeHouse | Date and Times in Python Earned the Dates and Times badge Thoughts Learned about timezone. It is a very difficult to handle without pytz link to tweet Day 08 | Friday 01 March 2019 Days Progress Completed the module of Quiz Game in the course. TreeHouse | Date and Times in Python Earned the build timed quiz App badge. Thoughts Some concepts of games were very good. link to tweet Day 07 | Thursday 28 February 2019 Days Progress Started the course. TreeHouse | Date and Times in Python Received the badge Date and time Badge Thoughts Learned about strftime() and strptime() Made a script to create a link for wikipedia. link to tweet Day 06 | Wednesday 27 February 2019 Days Progress Today I just watched the second lecture from MIT OCW's MIT 6.00SC Introduction to Computer Science and Programming Thoughts In this lecture we discuss about:- Type of objects. Expression link to tweet Day 05 | Tuesday 26 February 2019 Days Progress There was a gap of 3 days. Started the Date and Time module. Thoughts This course teaches about the date and time module of Python. Major modules in datetime are date , datetime , time , timedelta , timezone , tzinfo - which is rarely used directly. Learned about .today() , .combine() , .timestamp() - which returns the epoch time. We can format the time with help from strftime() We can create time with string format strptime() link to tweet Day 04 | Friday 22 February 2019 Days Progress Completed the course TreeHouse | Write Better python Received the badge Clean Code Badge In addition completed the Write Better python Course of the Learn Python track. Thoughts This course taught about:- PEP-8 Coding style guide. PEP-20 which is the Zen of python, can be accessed using import this PEP-257 for docstrings DocString which can fit one line should. DocString that cannot, put the closing triple quote on their own line. Logging Module of python 6 Logs levels, CRITICAL , ERROR , WARNING , INFO , DEBUG , NOTSET Python Debugger pdb , can be invoked by calling import pdb; pdb.set_trace() link to tweet Day 03 | Thursday 21 February 2019 Days Progress I completed the Object Oriented Python course . With this I have also received the badge. Thoughts Today completed the project Dice Roller. I am still not confident in some part of Object Oriented Python, will soon polish it. link to tweet Day 02 | Wednesday 20 February 2019 Days Progress Today I just watched the first lecture from MIT OCW's MIT 6.00SC Introduction to Computer Science and Programming . This is a great lecture by John Guttag I am alternating between Learn Python Track from Team TreeHouse and MIT 6.00SC Introduction to Computer Science and Programming Thoughts Lecture 1 of MIT 6.00SC Introduction to Computer Science and Programming taught me about:- Declarative and Imperative Knowledge. Algorithms Fixed program and stored program computers Programming Language Syntax Static Semantics Semantics. Types of errors Compiled Vs Interpreted Language Link To Tweet Day 01 | Tuesday 19 February 2019 Days Progress I am learning Python from the learn python track of TreeHouse . I started this course, some days back, from now, will post regularly on behalf of #100DaysOfCode. Today I have completed the Advanced Object Badge. Thoughts Today I learned about:- @property : This decorator is used to convert a class method into a class property. @property.setter : This decorator is used to make the method set a class property. @classmethod : This is a decorator which takes a function as input works on it and returns another function as output. Link to Tweet Reference QuoteFancy | Image Source 100DaysOfCode Official Website Learn Python Track from Team TreeHouse MIT 6.00SC Introduction to Computer Science and Programming Create a Tweet With image Preview for Free","tags":"#100DaysOfCode, python","url":"https://www.archerimagine.com/articles/100daysofcode-python/100Days-of-code-log-file_V_2_0.html","loc":"https://www.archerimagine.com/articles/100daysofcode-python/100Days-of-code-log-file_V_2_0.html"},{"title":"Render Maths Equation with Pelican Blog","text":"De Morgan's Law De Morgan's Law (negation of conjunction) $$ \\neg(P \\wedge Q) \\equiv \\neg P \\vee \\neg Q $$ De Morgan's Law (negation of alternative) $$ \\neg(P \\vee Q) \\equiv \\neg P \\wedge \\neg Q $$ First Header Second Header Content Cell Content Cell Content Cell Content Cell","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/Render-Maths-pelican-blog.html","loc":"https://www.archerimagine.com/articles/pelican/Render-Maths-pelican-blog.html"},{"title":"Study Notes for Programming in C","text":"Introduction C is not only a \"System programming language\" but has a wide variety of use in other domain. Fundamental Data Types Characters char Integers int , short , long Floating point double , float Derived Data Types Pointers Pointers provide for a machine-independent address arithmetic. Arrays Structure Unions. Expressions Expressions are formed from Operator and Operands . Any expression including an assignment or a function call can be a statement . Functions Functions performs a single set of operation within a block of code. Functions may return values of Fundamental data types. Derived Data types. except arrays Pointers Structure Unions Variables Variables can be internal to a function. External but known only within a single source file. Visible to the entire program. This is frowned upon and we should rarely use. Preprocessing Preprocessing step performs macro substitution on program text, inclusion of other source file or conditional compilation. UseCase not provided in C Operations directly dealing with Composite Objects, like Array, Structure, Unions No Storage other than static and auto matic. No Input/Output facilities. No built in file Access. The above facilities are included by help of standard library defined by the ANSI C Standard . ANSI C Standard New syntax for declaring and defining functions. Definition of a standard library. C is not a strongly typed language. C frowns on but permits the interchange of pointer and integers which has been eliminated by ANSI. No Automatic conversion of incompatible data types. Reference Image Source","tags":"C Programming","url":"https://www.archerimagine.com/articles/c-programming/StudyNotes-Programming-in-C.html","loc":"https://www.archerimagine.com/articles/c-programming/StudyNotes-Programming-in-C.html"},{"title":"100Days of Vim Log File","text":"What is the definition of a Real Programmer is open for interpretation. I have been using VIM for last couple of years to do the basic editing and was happy with whatever I learn about using vim in the process. Deep within I always wanted to become a better user of vim, tried different experimentation which includes installing mindless plugins, copying from different vimrc files sometimes understanding and sometime not understanding the concepts. Moral of the story is nothing sticked to my brain or what we call muscle memory to use it efficiently. Fortunately enough when I was struggling with the idea to become better at vim, this article How To Learn Vim: A Four Week Plan came along and I was immediately interested, clubbed this with the 100daysofx and I had a plan at becoming better at VIM. Through this log file, I will update what ever resource I will use and what ever practice I am doing everyday. God permits if I become good at vim in these 100 days it will be an achievement for me. Hoping for the best I hereby jump into the world of Real Programmers. Day -4: Saturday January 6,2018 Today's Progress Completed the vimtutor exercise in ~11 minutes. Learned about these keystrokes. i : insert before the cursor. x : delete the character under the cursor. h : Left Movement. (Left most Key) j : Down Movement. (Anchor like j ) k : Up Movement. l : Right Movement. (Right most key) Thoughts In my first attempt while learning vimtutor some years ago, I was not able to understand that vimtutor is a separate command in shell. From that beginning to today where I am comfortable in using vimtutor , surely have come a long way. Link to Tweet Day -5: Friday January 5,2018 Today's Progress Completed the vimtutor exercise in ~11 minutes. Learned about these keystrokes. u : undo just one change. U : Undo changes in one line. Ctrl + r : Redo the last change. :s/searchforstring/replacewithSting/g : Search and replace in one line. Thoughts vimtutor is helping me gain the confidence on using VIM. Now I do not feel threatened by VIM. Link to Tweet #100DaysOfVim Day-04 Tweet Day -6: Thursday January 4,2018 Today's Progress Completed the vimtutor exercise in ~14 minutes. Learned about these keystrokes. w : jump a word, not special char e : jump to end of word. $ : jump to end of line. 0 : jump to first column of the line. Thoughts Just vimtutor teaches so many basic key strokes to become a better beginner, everyone thinking of learning vim should try this. Link to Tweet #100DaysOfVim Day-03 Tweet Day -7: Wednesday January 3,2018 Today's Progress Completed the vimtutor exercise in ~14 minutes. Learned about these keystrokes. a : Append at the next cursor point. A : Append at the end of line. u : undo a single instruction. U : undo a complete line. Thoughts Looking at the key combination of VIM, I feel like each characters have a similar story. Link to Tweet #100DaysOfVim Day-02 Tweet Day -8: Wednesday January 2,2018 Today's Progress Completed the vimtutor exercise in ~20 minutes. The idea is to, do this for next 7 days as suggested in one blog link. Thoughts I have been using vim for a long time, but never took the pain to master it. I am starting this challenge with an idea that I will be able to move to intermediate level vim. Link To Tweet #100DaysOfVim Day-01 Tweet References How To Learn Vim: A Four Week Plan 100daysofx Real Programmers xkcd","tags":"#100DaysOfVim","url":"https://www.archerimagine.com/articles/100daysofvim/100Days-of-vim-log-file.html","loc":"https://www.archerimagine.com/articles/100daysofvim/100Days-of-vim-log-file.html"},{"title":"100Days of Code Log File","text":"Hello World! , You are about the witness the beginning of an epic 100-Day coding journey, A story that great sages will pass down from generation to generation. This quest will feature a potpourri of unfettered joy, unrivaled pain, and unexpected epiphanies. Some moments, I will be the smartest man alive. Others moments, I will be a stupid idiot. But each day, I will be a valiant warrior, fighting to develop and perfect the skills necessary to evolve into a true beast with these keys. Ladies and gentleman, I present to you, #100DaysofCode with @ animeshkbhadra Day -8: Wednesday January 2,2018 Today's Progress Completed the vimtutor exercise in ~20 mins. The idea is to do this for next 7 days as suggested in one blog link. Thoughts Reference","tags":"#100DaysOfCode","url":"https://www.archerimagine.com/articles/100daysofcode/100Days-of-code-log-file.html","loc":"https://www.archerimagine.com/articles/100daysofcode/100Days-of-code-log-file.html"},{"title":"Tips to improve work flow in pelican blog.","text":"We all know a lot of meta-data is required for writing a pelican blog, we can use Sublime Text to improve this meta-data collection, and also see some useful commands to improve the output. FileHeader to Write Meta-Data in Sublime Text Sublime Text is a very nice Text editor for writing Markdown. There are already a lot of articles on how to configure Sublime text for Markdown. We will discuss about one specific package in Sublime Text called FileHeader , this package helps in writing custom File Header, so we can use this package to provide some Meta-Data to the pelican blog by default. FileHeader comes with predefined header template, if we want to change the content of these templates we can use a custom fileHeader template. Since we are writing our content in markdown, I have extended the default Markdown template. The modified Markdown Template have to saved in this path /home/username/.config/sublime-text-2/Packages/User/fileHeaderTemplatesUser named as Markdown.tmpl . FileHeader uses Jinja2 template, the Markdown template looks like this . Title : Date : {{ create_time }} Modified : {{ last_modified_time }} Category : Pelican Tags : pelican Slug : {{ file_name_without_extension }} Author : {{ author }} subtitle : Summary : keywords : [ TOC ] We have to give some configuration for it to work. Kindly add this in the FileHeader.sublime-settings . { \"custom_template_header_path\" : \"/home/username/.config/sublime-text-2/Packages/User/fileHeaderTemplatesUser\" , \"Default\" : { \"author\" : \"ABC\" } } After this every time you create a new Markdown file, the above template will be automatically applied. Clean the Output Directory We are working with two git repositories when writing a pelican blog. root This is the folder which has the content and all the setting file. output This is the actual HTML page generated by Pelican. Most often than not we might want to generate the complete blog with a clean build with commands like this. pelican content -ds publishconf.py # While Publishing pelican content -d # Local host. The problem with these above commands is they might delete the .git or even the CNAME directory inside output losing the link with version control. We can prevent this by adding this configuration into our pelicanconf.py OUTPUT_RETENTION = [ \".hg\" , \".git\" , \"CNAME\" ] This change will make sure that the above mentioned file are not deleted. Kindly keep in mind this works for only the above 2 version of the command, if you use make clean then this configuration is of no use. Reference clean should not remove .git metadata #574","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/tips-for-improving-workflow-in-pelican.html","loc":"https://www.archerimagine.com/articles/pelican/tips-for-improving-workflow-in-pelican.html"},{"title":"Integrating 3rd party services with Pelican Blog.","text":"We have already discussed two types of settings file in pelican , and we found that publishconf.py is the file which is picked along with pelicanconf.py when we are generating the blog for publishing. These 3rd party service integration happen over this publishconf.py file, because we do not want these services to be activated when running on localhost . Google Analytics. Google Analytics provide us with very valuable insights into how a user interacts with the website, like the links which users click the most etc. This is a free service which really helps when we are starting out. When we create an account with Google Analytics , we receive a tracking code which we need to provide to Pelican blog. This tracking code is generally of the form UA-********-* , which you can find in this menu flow Administration ---> Property Settings ---> Tracking Id , copy this tracking id and provide it to publishconf.py with this settings GOOGLE_ANALYTICS = \"UA-********-*\" This is it, we will get all the analytics data from our website on Google Analytics. DISQUS DISQUS is a commenting system which is used extensively in the pelican blog world as we have to engage with our visitors for providing feedback and anything which is deemed important to the visitors. Integrating Disqus with any blog requires a ShortName , which is a unique identifier for our site, which can be found in this path https://<username>.disqus.com/admin/settings/general/ . We need to add this setting into the publishconf.py . DISQUS_SITENAME = \"shortName\" The Elegant Theme provides a nice feature called Collapsible Comments in which we will not show all the comments when the page loads. This comments will only be shown when a user presses the comment link. We can use a COMMENTS_INTRO settings to draw the user for engaging with the site. MailChimp MailChimp is a way to provide newsletters to your active subscribers, in this way they can be informed for any new post also immediately. Mailchimp can also be used as an email marketing platform. This is also free for basic subscription. We will need a MAILCHIMP_FORM_ACTION , URL which we can get by creating one list in MailChimp. To create a mailing newsletter we can use these links MailChimp Create a New List Add a Signup Form to Your Website Reference What's a shortname? Host Your Own Signup Forms","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/integrating-3rd-party-services-with-pelican.html","loc":"https://www.archerimagine.com/articles/pelican/integrating-3rd-party-services-with-pelican.html"},{"title":"Home Page feature for Elegant Theme","text":"Elegant has a nice feature called the Home Page, which provides a About Me combined with information from GitHub on the project someone is working. These feature are again controlled by a configuration. In place of adding this configuration into pelicanconf.py we will create a new file called elegantconfig.py , and import this file into pelicanconf.py by calling from elegantConfig import * . This will make sure all the configuration from elegantconfig.py is available while generating the blog. There are two configuration for the Home Page. LANDING_PAGE_ABOUT LANDING_PAGE_ABOUT This is a Dictionary with two field \"title\" and \"details\" \"title\" : This gives a heading Home Page. \"details\" : It is the content in HTML which mostly contains the About Me description which one wants to provide. One clever way of writing this HTML is, write a about page in markdown, generate the pelican blog with this about page and then right click on the page and select view source, copy the relevant text with the HTML code in it. PROJECTS PROJECTS This is also a list of Dictionary with these field in each dictionary, and the value of each of these field is a string and not HTML in case of LANDING_PAGE_ABOUT 's \"details\" \"name\" : The project name which you want to be displayed. \"url\" : The link to the project. \"description\" : The description you want to provide for the project.","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/home-page-features-elegant-theme.html","loc":"https://www.archerimagine.com/articles/pelican/home-page-features-elegant-theme.html"},{"title":"Problem Faced when integrating with Elegant Themes.","text":"Elegant was my theme choice, and while integrating this theme with my blog I found that a lot of things does not work out of the box, and some changes are required in the themes itself. I am listing these changes here, if this are still unresolved in the future. Elegant , requires a lot of different plugins to make it work the way it was designed. If you see the documentation on the above link, it does not mention the important plugins which are required to make it work, it just provides an explanation on the feature. In this blog post I am listing down the issues which I faced while integrating the Elegant themes. favicon.ico not displayed with Elegant If you read the documentation on how to enable favicon.ico for Elegant themes, the process is pretty straight forward, just place the icons into this directory content/theme/images , and define STATIC_PATHS with STATIC_PATHS = ['theme/images', 'images'] . If you follow the above process then most probably you will still not be getting the favicon.ico , the reason being there is one more configuration which needs to enabled in pelicanconf.py in addition to above two. STATIC_PATHS = [ 'theme/images' , 'images' ] USE_SHORTCUT_ICONS = True The reason for this is, the links for including favicon are generated in this template themes/elegant/templates/_includes/favicon_links.html , and the link generation is under a configuration named USE_SHORTCUT_ICONS , so until we make it True the links will not be generated. Issues while integrating Tipue Search Prerequisite Tipue Search plugin integration is little different from other plugin. This plugin has an external dependency on BeautifulSoup , we have to install this python package first in our environment pelican1 pip install beautifulsoup4 We have to add this plugin name into pelicanconf.py PLUGINS = [ 'sitemap' , 'tipue_search' ] When we build our blog after this we should see our search functionality working, but we see 2 issues. Search Result are not displayed. Tipue search return undefined URL. Search Result are not displayed Elegant uses some predefined HTML pages to render few of its content, search functionality is based on one such file, so we have to provide that in the pelicanconf.py DIRECT_TEMPLATES = (( 'index' , 'tags' , 'categories' , 'archives' , 'search' , '404' )) When we do this changes we will see the search result getting listed, but on clicking those link it will redirect to an undefined URL. Tipue search return undefined URL. When we click on the Search result, we get the URL as undefined, this issues is still not solved, there is already a pull request pending on the Github. Kindly visit these two links for more details. * Tipue search return undefined url . * To solve the issue, we have to modify the plugin manually as mentioned in the pull request - Change the file in plugins/tipue_search/tipue_search.py , line no 61 add this code 'loc': page_url TOC Integration with Elegant Elegant theme has a side bar with the Table of Content of the blog post displayed. This is also achieved based on a plugin named extract_toc . We have to add this into the pelicanconf.py PLUGINS = [ 'sitemap' , 'tipue_search' , 'extract_toc' ] There is also a Markdown settings which we have to update in the same file like this. MARKDOWN = { 'extension_configs' : { 'markdown.extensions.toc' :{ 'permalink' : 'true' } } } In addition to the above changes, every blog post after the file meta-data section should have an entry named [TOC] Syntax Highlighting When writing a technical blog we might be interested in syntax highlighting of the code we write, we can achieve this with following configuration in pelicanconf.py MARKDOWN = { 'extension_configs' : { 'markdown.extensions.codehilite' : { 'css_class' : 'highlight' }, 'markdown.extensions.extra' : {}, 'markdown.extensions.meta' : {}, 'markdown.extensions.toc' :{ 'permalink' : 'true' }, }, 'output_format' : 'html5' , } Next and Previous Articles. When we read the documentation of Elegant Next and Previous Articles , it clearly states that we do not require any additional plugins for this feature, but it does not work out of the box. We have to integrate the neighbors plugins and then it works. Now we will have these in our pelicanconf.py PLUGINS = [ 'sitemap' , 'tipue_search' , 'extract_toc' , 'neighbors' ] Missing icons for social links Pelican supports a way to integrate most of the social Website like Twitter , Facebook , Github etc. This is done by the help of this settings in pelicanconf.py SOCIAL = (( 'github' , 'URL to your profile' ), ( 'linkedin-square' , 'URL to your profile' ), ( 'facebook' , 'URL to your profile' ), ( 'quora' , 'URL to your profile' ), ( 'reddit' , 'URL to your profile' ), ( 'twitter' , 'URL to your profile' ) ) This explains that we have a tuple of tuple in the settings name SOCIAL , if you see each website name is in a particular format, as explained in this link , this format is taken from Font Awesome icon for social links . When naming the website name in SOCIAL configuration, keep in mind the way the website in named in Font Awesome and remove the fa- part of the name. References Elegant Kindly read this documentation for other configuration which gives more flexibility in terms of the themes like Article Subtitle, Add License to your Site etc. pelicanconf of oncrashreboot blog Visit this configuration file for any doubts on the setting of elegant theme. Favicon documentation Tipue Search plugin Tipue search return undefined url . Tipue search return undefined url pull request Missing icons for social links Font Awesome icon for social links","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/integration-problem-with-elegant-theme.html","loc":"https://www.archerimagine.com/articles/pelican/integration-problem-with-elegant-theme.html"},{"title":"Expressing the content with Pelican Themes.","text":"We have already discussed about Pelican Themes and Plugins in this blog . We have also seen the comparison between the various popular themes. Elegant is our choice of Themes because of search functionality which it provides along with the minimalist concepts. We will first focus on how we can integrate one particular themes and also one particular plugins which will give us a fair amount of idea on how to integrate different plugins. Integrating the Elegant Theme We have already created folder named plugins and themes , which are clone of the Pelican Plugins and Themes repository. If we see the directory listing inside themes folder we fill find different folders with distinguished names, these are the themes name. To use any of the themes in this folder we have to add this pelican settings in the pelicanconf.py . THEME = 'themes/elegant' Follow the commands to generate the site and launch the site, and you have your new themes applied. pelican content cd output/ python -m pelican.server Integrating A Plugin into Pelican The process to integrate any plugin is also similar to integrating Themes. If we see inside the plugins directory which we had cloned, we will find a lot of different folder name just like in themes directory. Each of these name is a plugin name. To integrate a plugin into Pelican we have to add these 2 configuration into the pelicanconf.py file. PLUGIN_PATHS = [ 'plugin' ] # Name of the directory where plugin are kept. PLUGINS = [ 'sitemap' ] # Name of the particular plugin inside the directory. In the above code sample, we can see we have integrated the sitemap plugin, and as per the documentation , this generates a Sitemap which we generally submit it to some Webmaster tools. Likewise, if we want to integrate any other plugins , we have just add it to the list variable PLUGINS along with the settings required for that plugins defined in its documentation. References Elegant Pelican Plugins Pelican Themes Sitemap Documentation ,","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/expressing-with-pelican-themes.html","loc":"https://www.archerimagine.com/articles/pelican/expressing-with-pelican-themes.html"},{"title":"Customizing Pelican blog with the help of Plugin and themes","text":"Pelican Blogs gives its user the full power to customize to the want of the user. We have been using the default themes and default setting provided by Pelican with not much customization and the results are also quite good. Enhancing the present blog further will require us to use certain Plugins and Themes which will extended the functionality. These are the 2 repository which we should clone into for getting these enhancements. pelican-plugins This is the repository for the plugins. git clone --recursive https://github.com/getpelican/pelican-plugins.git plugins pelican-themes This is the themes repository. git clone --recursive https://github.com/getpelican/pelican-themes.git themes Plugins Vs Themes The first things which we have to decide is to choose the list of plugins or the Themes we want to use. I would say first decide on Themes and the see what all plugins are required to support these themes. All plugins are not plug able with all the themes. We have to first start by choosing the themes, and the corresponding plugin required for it. This path is again the easiest as we are already treading the known, when we are comfortable with this integration we can always go ahead and enhance the existing themes and plugins to suit our needs. Till we reach the Jedi stage this precooked solution is the best approach. Flex Vs Elegant vs BootStrap3 When we are deciding on which themes to choose for the blog, broadly the working choice would be Flex: Responsive Pelican theme Elegant Why it is the best Pelican theme BootStrap3 There are many others, which you are free to explore, but for my blog I had considered these 3 options. Each one has its own pros and cons, the choice was very difficult between these 3, and in future I might even consider jumping ships to the other themes. Let's discuss some outline of the above 3 themes Flex: Responsive Pelican theme This has out of the box integration with a lot of plugins including AddThis , which is not available for any other themes. This also has support for Google AdSense, which again is missing in most of the plugin. Actively maintained, the last check-in on its Github repository was on Apr 24, 2017. Tipue_Search is also on the way, there is an open issue on the repository, Search #49 This facility of search is the only reason I dropped Flex . BootStrap3 This is a full hack able implementation of BootStrap3, and will try to use these Themes, once I get my hand dirty enough with the modern CSS and Web Technologies. Elegant Why it is the best Pelican theme I chose this theme just for its minimalist design and search feature. It has an integration with MailChimp. This is not an actively managed project, the last commit on this repository was on Sep 8, 2014, which is good 3 years ago. We can use and modify this theme for our satisfaction if a bug is hampering our development. We have decided on our Themes, the Plugins required for these themes are as below. sitemap tipue_search extract_toc neighbors The standard features and enhancements already available Disqus Google Analytics MailChimp Integration. Custom 404 Page. Collapsible Comments Page and Article Subtitle. We will not try to see the integration of all the above feature with the Elegant themes. Reference Elegant Why it is the best Pelican theme BootStrap3 Flex: Responsive Pelican theme pelican-themes pelican-plugins","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/customizing-pelican-blog-with-plugin-and-themes.html","loc":"https://www.archerimagine.com/articles/pelican/customizing-pelican-blog-with-plugin-and-themes.html"},{"title":"Configuring Github pages with Custom Domain","text":"In the first series of blog post we have seen how to create a pelican blog, customize a little and also host it from Github pages. We can also use Github pages as a hosting service and link any of the domain name providers with this hosting. In this post I have taken the example of GoDaddy, but the process should not much different for any other domain name provider. The process to link a Godaddy Domain to Github Pages can be divided into 2 Steps. Configure your Github repository Configure the DNS at GoDaddy Configure Github Repository We have already made a Github Pages website in our previous post, kindly follow the steps mentioned here . Once we have a Github Pages URL, we have to configure a CNAME in this repository, this can be done in 2 ways. Local Repository Directly on Github. Local Repository Create a local file in the repository with the name CNAME. Just have one line in the file. example.com , where example.com is your domain you have bought from GoDaddy . Push the changes to Github Directly on Github Pages. On the repository in Github , you will see something like this. In the above Click on the Settings , Scroll down you will see something like this. Enter the domain you have purchased from GoDaddy . These are the only changes required to be done in GitHub. Configure the DNS at GoDaddy The easiest of the all the below references is Configuring a Godaddy domain name with github pages . The real issue in all the links is that it shows the old UI of GoDaddy , so some things get confusing. Go to the account setting page, which mostly will like in this link . The link will look like this. Click on the + Symbols in Front of Domain, and Click on the Manage DNS Link The link will show a lot of Records , go to the end of the Records , and click on the link ADD From the above option we have to add 3 entries. This is how all the 3 would look like after adding. Now you can launch and check your desired domain. Kindly wait 48 hrs for these changes to reflect, do not try to configure multiple times, if it does not work even after 48 hours kindly search for help, till then take a coffee break and have a nice time out of this screen. The domain XYX is no longer parked by Godaddy When we are doing the above process, even after 24 hours, when you launch your website, we find one of these errors. The domain XYX is no longer parked by Godaddy It is detected as a Malware in the office network. The website might launch for some times and sometimes you might get any one of the above 2 errors. Kindly check this in the Manage DNS page. We had added two A Names pointing to the GitHub URL as shown below. Check if you have any other A Names in addition to the above two, if you have, kindly delete that. The detailed issue can be read GoDaddy domain (randomly) not resolving to GitHub Pages References Setting Up a GoDaddy Domain Name With GitHub Configuring a Godaddy domain name with github pages Using a custom domain with GitHub Pages Using GitHub Pages To Host Your Website GoDaddy domain (randomly) not resolving to GitHub Pages [Help]: How to correctly connect my github pages blog to a custom domain? Redirecting GitHub Page to a custom domain Kindly read the above site, to understand what is the use of CNAME and A Record. Great introduction.","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/linking-domain-with-github-pages.html","loc":"https://www.archerimagine.com/articles/pelican/linking-domain-with-github-pages.html"},{"title":"Publishing your blog to github pages.","text":"We have set the basic blog, though it may not look great because of the theme which we are using, but we will change those. Presently we will focus on taking this blog for the world to see. Github Pages is the way to publish your blog on to the WWW. We will have 2 repositories for doing this. One repository is for the source, which is our blog content. Second repository is for the generated files in the output folder. Prerequisite We all should have a GitHub account, if you have then continue, else go to GitHub and create an account. The process is very straight forward, just follow the instruction on the website. The git should be locally configured. Kindly follow the steps mention in this page Setting your email in Git . The first Repository We should now create our first Repository onto github. Click on the + icon which shows this drop down. And click on the option New Repository as shown above. This will open up a page with looks like this Fill in the repository name which you want to name, and write a little Description also do not click on the check-box which read Initialize this repository with a README Since we already have a folder in our local system, to check-in we can leave this box unchecked. In future if need we can create a README file manually. Once you click on the Create Repository link, you will be taken to this page, with these as a content. This is the symbol of empty repository which is created and now we can push our local code to this folder. Push code from local to Github repository We can follow the instruction mentioned above to push our local code to github, but we have to make sure that we do not push the output directory in the same repository as the code, because the handling of output directory is different. Go to the root of the folder, where your blog is kept. cd ~/mySampleBlog/ Go to the root of the directory where the blog post are kept git init This command initializes an empty repository in the same folder. Create a file named .gitignore in the same folder. Copy the below content to the .gitignore file. *~ ._* *.lock *.DS_Store *.swp *.out *.py [ cod ] output The above code basically prevents all the local temporary files and the output directory to be checked in. git status This command will make sure that the output folder is not tracked by git, check the output as shown below. On branch master Initial commit Untracked files: ( use \"git add <file>...\" to include in what will be committed ) .gitignore Makefile content/ fabfile.py pelicanconf.py publishconf.py nothing added to commit but untracked files present ( use \"git add\" to track ) Only files listed above will be tracked, and output directory is not one of them. git add . Adds all the above files to be ready for commit. git commit -m \"first commit\" This will make a commit locally with the commit message as first commit git remote add origin https://github.com/pelicanBlog/mySampleBlog.git This commands connects the remote repository of github with our local code. The URL might be different for you. git push -u origin master This will push the local changes to the GitHub repository we created. It will ask for your username and password, kindly provide those and we have pushed our code to github. Pushing the output folder The output folder is out actual blog, so this is a little different from the previous way of pushing code to github. Please follow along to create this special repository. Select the same New Repository from the + icon shown in previous repository. In the New Repository form, the magic happens with the Repository Name field, fill in the name as username.github.io , where username is your github username. Fill In the description. Now there are two ways to push the output folder to this new repository, you can follow any one of them. Push the output folder as a repository which we already know. git init git add . git commit -m \"first commit\" git remote add origin https://github.com/pelicanBlog/pelicanBlog.github.io.git git push -u origin master After the above command, just launch this URL in your browser, https://username.github.io/ , just change username with your username. Since the original source folder we create with output directory mentioned in .gitignore , we can now add this repository as a submodule in that repository, to provide a link between both the source and the output Go to the root of the blog content, in this case cd ~/mySampleBlog git submodule add -f https://github.com/pelicanBlog/pelicanBlog.github.io.git output This adds the output folder as a submodule. git commit -m \"added submodule\" git push -u origin master The above will make the output as a submodule With this we have completed our part of the blog series. With this series of blog post we are able to achieve this. Understand what is pelican blog and how to use it with github pages. How to set up the Anaconda environment for pelican development. Understood what pelican-quickstart command does. Understood the basic commands to generate the pelican blog Understood the basic folder structure of the pelican blog Understood the pelican settings files and its uses. Created a github pages and pushed both our content and the blog post to it. The next series will focus into How to configure a domain with github pages. Use of a themes and plugin to enhance the website Integration with google analytics, mail chimp etc. Modifying the blogs to get the most out of the themes. Reference Github Help Setting your email in Git git-submodule How to publish a pelican site on Github Github Pages","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/publishing-blog-github-pages.html","loc":"https://www.archerimagine.com/articles/pelican/publishing-blog-github-pages.html"},{"title":"Understanding the Pelican Settings files.","text":"The basic work flow in pelican blogging is to first generate content, verify it locally using a localhost:8000 , and when everything is fine will publish it. Pelican comes with two settings files to separate these 2 process. These two files are. pelicanconf.py publishconf.py Let's check what is the use of these 2 files, and how to manipulate these files to get the most out of pelican. These setting files are mostly passed to the templates associated with the themes to generate the site, all these settings are some parameters to these templates. pelicanconf.py We had used the pelican-quickstart to generate this blog, when we use this command, we get a pre-configured pelicanconf.py and a publishconf.py files. This have the bare basic configuration to be used based on the questions we answered on the options. These files basically contains the setting's identifier for the pelican blog, all the setting identifiers are in all-caps, and the values numbers (5, 20, etc.), Boolean (True, False, None, etc.), dictionaries, or tuples should not be enclosed in quotation marks. pelicanconf.py is used to generate the site locally and tested over localhost:8000 . The basic pelicanconf.py would look like this. #!/usr/bin/env python # -*- coding: utf-8 -*- # from __future__ import unicode_literals AUTHOR = u 'Animesh' SITENAME = u 'Hello World' SITEURL = '' PATH = 'content' TIMEZONE = 'Asia/Kolkata' DEFAULT_LANG = u 'en' # Feed generation is usually not desired when developing FEED_ALL_ATOM = None CATEGORY_FEED_ATOM = None TRANSLATION_FEED_ATOM = None AUTHOR_FEED_ATOM = None AUTHOR_FEED_RSS = None ARTICLE_PATHS = [ 'articles' ,] ARTICLE_URL = 'articles/ {slug} .html' ARTICLE_SAVE_AS = 'articles/ {slug} .html' # Blogroll LINKS = (( 'Pelican' , 'http://getpelican.com/' ), ( 'Python.org' , 'http://python.org/' ), ( 'Jinja2' , 'http://jinja.pocoo.org/' ), ( 'You can modify those links in your config file' , '#' ),) # Social widget SOCIAL = (( 'You can add links in your config file' , '#' ), ( 'Another social link' , '#' ),) DEFAULT_PAGINATION = 10 # Uncomment following line if you want document-relative URLs when developing #RELATIVE_URLS = True If you remember correctly some of the settings value we had provided during the pelican-quickstart commands, like AUTHOR This is the name of the site author which we had entered during the questions asked. SITENAME This is the Name of the site which we provided. SITEURL Since we still do not have domain name registered, we had kept this empty, and also it makes sense to keep this empty for localhost testing. PATH content where we write our blog is the default path set. TIMEZONE This we had entered during the initial process, and in future if we want to change this we can change these settings. DEFAULT_LANG This was also entered during the pelican-quickstart process. FEEDS_* All the FEEDS_* related settings are empty because we have still not configured the RSS feeds settings, this we will change in the future. ARTICLE_* This is the settings which we modified for keeping the path of the post into one folder. LINKS This is a tuple of tuple, with each entry showing a link which you want provide in your blog. SOCIAL This is also a tuple of tuple, where each entry is meant to point to a name of a social network say Facebook and the link to your profile. DEFAULT_PAGINATION This is a number showing how many blogs should be listed on the front page. Some Themes use this setting for some other purposes. Now these settings are not fully exhaustive, Pelican has a huge list of setting, which we will revisit once we have the need of them. publishconf.py Unlike pelicanconf.py , this setting file is only used when we are supposed to publish our blog to the domain hosting. This configuration is accessed when we generate the site using the following commands. pelican content -s publishconf.py The basic content of these files look like this. #!/usr/bin/env python # -*- coding: utf-8 -*- # from __future__ import unicode_literals # This file is only used if you use `make publish` or # explicitly specify it as your config file. import os import sys sys . path . append ( os . curdir ) from pelicanconf import * SITEURL = '' RELATIVE_URLS = False FEED_ALL_ATOM = 'feeds/all.atom.xml' CATEGORY_FEED_ATOM = 'feeds/ %s .atom.xml' DELETE_OUTPUT_DIRECTORY = True # Following items are often useful when publishing #DISQUS_SITENAME = \"\" #GOOGLE_ANALYTICS = \"\" This file, basically build on top of the pelicanconf.py , as we can see from this line, from pelicanconf import * . What this means is all the configuration from pelicancongf.py is taken into consideration along with some specific configuration which is required for just publishing. If you see this setting most of them are empty which we will fill one by one as we make progress in our blog, but from the structure you might get an idea that this file pertains to its integration with the DISQUS comment system, and the GOOGLE_ANALYTICS code. We will see the use of this code once we integrate these feature into our blog. Reference Pelican Settings","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/pelican-settings-files.html","loc":"https://www.archerimagine.com/articles/pelican/pelican-settings-files.html"},{"title":"Pelican commands to generate the first blog.","text":"We have our boilerplate pelican blog available, now we have still not seen the magic of pelican. In this post we will see our blog coming to life. First Pelican Blog We have seen the folder structure here Now we will first execute some commands and see what happens with this boilerplate. Remember to be in the pelican1 environment. We can do this by. source activate pelican1 Once we are in pelican1 environment, we have all the pelican commands at our disposal. Kindly execute this command. pelican content The above command we are passing content as a parameter, which is nothing but one of the directory of the folder structure. The output will be WARNING: No valid files found in content. Done: Processed 0 articles, 0 drafts, 0 pages and 0 hidden pages in 0 .12 seconds. It clearly warns us about no valid files found in content , as we have not added any post to the directory. If you see inside the output folder, we will see some content in that namely these files archives.html authors.html categories.html index.html tags.html theme #directory which contains some predefined images and css. Now let us see what the blog looks like. Just execute these commands. cd output/ python -m pelican.server Once you execute the above commands we can see the output on browser on this path localhost:8000 and it will look something like this Nothing fancy here, but we will some content with some link and a default theme. It is petty good for being a boilerplate. The first post. Now we are ready for our first post, we will do the sample hello world which is the de-facto standard in programming languages first program. We will write the first post in Markdown . Create a file named HelloWorld.md in this directory content . Title: Hello World Date: 2017-04-29 11:01 Category: Pelican Hello World to Pelican Once create and saved this file, just run this command. pelican content This will have the following output. Done: Processed 1 article, 0 drafts, 0 pages and 0 hidden pages in 0 .27 seconds. This time it clearly states that we have 1 article. Then follow the other commands as discussed. cd output/ python -m pelican.server Again check the output on the browser at localhost:8000 . Now this time the output is different and it look like this. The area surrounded in ellipse are new. This shows us the power of pelican, we do not have to bother how the content is presented on screen, we have to only concentrate on writing content. Reference Pelican Doc Quickstart","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/pelican-commands-to-generate-first-pelican-blog-post.html","loc":"https://www.archerimagine.com/articles/pelican/pelican-commands-to-generate-first-pelican-blog-post.html"},{"title":"Pelican Folder Structure","text":"We will be able to extract the full juice of pelican once we understand the building blocks of the pelican blog. Folder structure and some files forms the basic of this. We will understand some of these files and folder use in this post. After the first blog post if we give make clean command we will see a folder structure just like this. This will have an empty output folder. We will understand the use of these folder and a proper way of managing your content. content folder This is the folder where all the magic happens. This is the root folder for our all content, we can use this folder for these purposes. Writing content Saving the images references in the content Static Pages (ex: about, contact etc) folder for storing our favicon.ico and apple icon Writing Content In traditional wordpress or most of the blogging platform the content is stored in this format /2015/05/24/my-content/ now this may be what some people might be happy with and you can also do the same, but this impacts the SEO ranking as the path becomes long and the date when the content was created has no significance to the actual content but is occupying space on the URL. In my opinion we can better organize our self with some meaning full structure. What we can do is we can create a folder in side content named articles and the sub folders inside articles for each category which you want to write or if you want to have a flat system you can place all your content inside articles . I prefer the sub folder approach as we can derive the category name just from the folder name. So go ahead for beginning create a folder inside content name articles cd content/ mkdir articles Now create your second post inside this and publish the content based on the command we learned on the previous post. Now if you launch the localhost:8000 , and click on the article title, you will see no difference but if you see on the URL bar, we still see that our post is without the article folder structure. We did not want this. Have a look In order to get the proper URL in the address bar we have to change somethings in the pelicanconf.py , we will explain what is the purpose of this file in a short file for not just add these 3 line into that config. ARTICLE_PATHS = [ 'articles' ,] ARTICLE_URL = 'articles/ {slug} .html' ARTICLE_SAVE_AS = 'articles/ {slug} .html' With is configuration in the config folder, just generate the blog with the commands already learned and then check the localhost:8000 . It should look like this. Now we have a proper folder structure. Static Pages Most often than not we will want some pages which will rarely change, like an About and a Contact pages. These type of static pages is also supported in the pelican blog. Just create a folder named pages inside content folder like this. cd content/ mkdir pages Now we are ready to make our sample About.md and Contact.md inside pages directory. Title: About Date: 2017-04-14 22:30 Modified: 2017-04-14 22:30 Slug: About Author: username Summary: This is a sample blog. The About Page for the blog. and Contact.md Title: Contact Date: 2017-04-14 22:30 Modified: 2017-04-14 22:30 Slug: Contact Author: username Summary: This is a sample blog contact page. The Contact Page for the blog. We again generate the blog and check it on localhost:8000 . When we see the output of pelican content command we will see this. Done: Processed 1 article, 0 drafts, 2 pages and 0 hidden pages in 0 .20 seconds. Which means that, the pages are generated. When we launch the localhost:8000 . we will see that the About and Contact menu like this We can modify these pages with the information which you want to furnish. Static Images Most often than not we will use images to link in our blogs, we can store all these images inside the content folder, having a directory called images . Do this by following these commands. cd content/ mkdir images Now we can copy any image inside this folder and try to link it into one of our blog. Just copy any image in this folder and copy the file name. Now create a link to this file inside the already existing blog post by the help of link of markdown, here is a sample. ![ Hello World 1 ]( {static}../../images/helloWorldPelicanPost.png \"Hello World 2\" ) We should understand some details about the above piece of code. Hello World 1 The alt text is good if there is a browser which block image, this alt text is displayed, showing an information about what this images were supposed to do. \"Hello World 2\" This is the title of the image, which is shown as a tool tip once we hover over the image. {static} This is a special syntax which is used by pelican to generate links, so be it URL or images, when using relative URLs kindly use this format. We have covered most of the folder structure inside article we will see the configuration files in the next blog post. Reference Pages Linking to internal content","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/pelican-folder-structure.html","loc":"https://www.archerimagine.com/articles/pelican/pelican-folder-structure.html"},{"title":"Pelican HelloWorld using pelican-quickstart","text":"Pelican makes it very easy to make the ground rolling ASAP. Pelican provides a great command pelican-quickstart , which asks a few questions to you and makes a boilerplate blog ready in a few seconds. We will go through the entire process explaining each option is details. Activate Python environment We had set up a separate python anaconda environment in our previous post now is the time to active the environment, we can do that by using a simple command. source activate pelican1 It will activate the pelican1 environment, and we can identify it by check the terminal prompt which will change to (pelican1) . Once the work is done, we can deactivate the same with a simple command. source deactivate pelican1 All the above command will work for Linux and Mac, kindly check the windows equivalent of the same. pelican-quickstart Now we are ready to divulge in the world of pelican. Pelican has a ready to bake command to set up the basic boilerplate for the blog. The command is called Before executing the below commands, just create a directory where you want your blog files to be stored. mkdir ~/myBlogDirectory cd ~/myBlogDirectory pelican-quickstart Just execute this commands and it asks you these series of questions, which we will talk in details. pelican-quickstart options The options shown after executing the pelican-quickstart are as below. We will discuss each and every option and their usage, we can always choose the default shown in capital letter, {Y|n} . Where do you want to create your new Website ? [.] Most probably we will keep the default as we are already in that directory, if not you can specify the path /home/pathtomyblog What will be the title of the blog? Provide a suitable title to your blog, do not worry even if you want to change it latter we can change it. Who will be the author of this Website ? Just provide any name you want whose name should be present as an author on the blog post, it can be your name as well. What will be the default language of this Website ? [en] The default choice is English , else you can give any language format mentioned in ISO 639-1 Language Codes, the list can be found ISO Language codes Do you want to specify a URL prefix? e.g., http://example.com (Y/n) If you already have purchased a domain give the domain name as shown in the example, else continue with n , we can later fill the domain name. Do you want to enable article pagination? (Y/n) We can go with the default of having pagination, which means how many posts of the blogs will be displayed in one page, the choice of this is in the next question. How many articles per page do you want? [10] The default choice is 10 , for the time being keep it that way. What is your time zone? [Europe/Paris] To change the time zone, we should be aware that these are tz database time zone, to exactly get the time zone codes for your country visit List of tz database time zones , give the code without the [] . Do you want to generate a Fabfile/Makefile to automate generation and publishing? (Y/n) There are multiple ways to automate the blog publishing process, makefile and fabic comes to our help, just chose the default and we will decide on this later. This creates two files in the directory, fabfile.py and Makefile Do you want an auto-reload & simpleHTTP script to assist with theme and site development? (Y/n) We have the help of auto-reload commands to automatically generates the preview as soon as we change anything in the themes, we might not require it initially, so keep it false. Do you want to upload your website using FTP? (y/N) If we had an FTP site where we could upload, just choose the default and say n Do you want to upload your website using SSH? (y/N) If you are hosting uses SSH, for our use case we will choose N , which is the default. Do you want to upload your website using Dropbox? (y/N) We can also use Dropbox to upload our static files, but for this, we will try some other time, for now choose the default which is N . Do you want to upload your website using S3? (y/N) We also have the facility of choosing amazon S3 for our site hosting, for now not needed chose N Do you want to upload your website using Rackspace Cloud Files? (y/N) Again the default N , we are not using Rackspace. Do you want to upload your website using GitHub Pages? (y/N) We can choose y here, but we will try another mechanism, for now choose the default N If you had chosen y in this option you will get this sub option. Is this your personal page (username.github.io)? (y/N) Choose y , After this we will get this message in either case. Done. Your new project is available at /home/username/myWork/mySampleBlog Our blogs boilerplate is available. Here is the folder structure. We have still not seen how the blog will look like, we will divulge into this in the next post, with all the pelican commands which is extremely important to get the full juice out of pelican. References How I setup Pelican List of tz database time zones List of ISO 639-1 codes pelican-quickstart.py","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/helloworld-pelican-quickstart.html","loc":"https://www.archerimagine.com/articles/pelican/helloworld-pelican-quickstart.html"},{"title":"How to setup Anaconda Python environment for Pelican Blog.","text":"Installing Pelican Pelican is a python package, so we can have multiple option to install pelican. There can be 3 option which I can think of. Direct Installation If we have only one python installation on the system, and we do not have any issue if we screw up this installation just use pip to install pelican. Installation using VirtualEnv. This approach is already mentioned in official documentation of pelican Installation using Conda. By now you could have understood that we will use Conda to install pelican. This is because Anaconda is already a prepackaged installation of very well know python package in both version 2 and 3. In future, I will update this page if I wrote about Anaconda installation, for now refer any documents on the google search for installation. Configure Anaconda for Pelican Blog The first thing we have to do is to create an environment using the python version 2. This can be done by this command. conda create -n pelican1 python=2 Once we have executed the above command we will have pelican1 as an environment. We can see the list of environment in the system by using. conda info --envs Which will provide, an output like this. pelican1 /home/username/anaconda3/envs/pelican1 py27 /home/username/anaconda3/envs/py27 py35 /home/username/anaconda3/envs/py35 root * /home/username/anaconda3 This shows all the available environment. We can activate the pelican1 by using this command. source activate pelican1 Now we have an environment which we can use for pelican development. The reason for this environment creation is to have a separate environment for experimentation with pelican, if anything goes wrong we do not disturb already existing programs. Configure Pelican Environment for Blog Once we have the environment, we have to install few packages in this environment. The first is to install PiP to install other package. Install Pip by using. conda install pip With the completion of installing pip , the first and foremost package to install is pelican, with this command. pip install pelican Since we will be using Markdown to write our blogs we need the markdown package, which we can install using. pip install Markdown There are some plugin and themes in pelican which might need some additional packages, we will install these 2. pip install Fabric pip install beautifulsoup4 Freeze the Requirement When we have all our installation complete, we should save our package history into a requirement.txt . We can use this requirement.txt to install all the above mentioned packages with same version in one go. We can freeze the details by using. pip freeze > requirements.txt Dependencies If we check the requirement.txt generated in the above step, we will see a lot of packages already installed apart from pelican , markdown , Fabric and beautifulsoup4 . These extra packages are dependencies for running pelican. feedgenerator: to generate the Atom feeds jinja2: for templating support pygments: for syntax highlighting docutils: for supporting reStructuredText as an input format pytz: for timezone definitions blinker: an object-to-object and broadcast signaling system unidecode: for ASCII transliterations of Unicode text six: for Python 2 and 3 compatibility utilities MarkupSafe: for a markup safe string implementation python-dateutil: to read the date metadata We have completed 2 important steps of our own blog publishing. Reference Installing Pelican Managing Python","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/python-setup-for-pelican-blog.html","loc":"https://www.archerimagine.com/articles/pelican/python-setup-for-pelican-blog.html"},{"title":"Cost effective blogging with Pelican and Github","text":"This is a series of blog post to help set up a static blog at minimal cost and integrating with all the popular tools such as Google Analytics, MailChimp, Disqus, Google Adsense. This will be the one stop place to find all the details for setting up a beginner level blog. I am a novice blogger and this blog would act as a journal, which will document my approach towards blogging, software development. The idea is to generate some revenues out of this blog in the long run. We all might have read about so many blogs which are able to generate good amount of traffic but in none one of those we have never read how do they achieve it. I may fail in my attempt, which might work as a guidance for someone to not follow this path and try another path for the same goal. The background theme to support this is to minimize my cost to the bare minimum so that the failure does not hurt me financially. Why Static Blog? There are already a lot of literature present behind this, just wanted to summarize those:- Cost:- This was the deciding factor for me, because of using a static website, this complete website can be hosted and deployed by just registering a domain name with a provider. I do not have to go for any hosting services etc. Easy of Writing Content:- I wanted to write my content using just Markdown , as i have grown comfortable writing in markdown. With using a static blog this was possible. Hosting:- We can serve these static HTML pages practically from any place, be it Github , Amazon S3 , Dropbox or any other place which can serve static HTML pages. I have chosen Github just to save the cost. Easy workflow:- The work flow is very simple when deploying with static blogs, just right you content in markdown, generate HTML, push your changes to github and that's it. Your content is not available online. You can even go crazy you cam automate the whole thing other than writing content. The above three are the main reason for choosing static blogs, but there could be many more valid reason for choosing static blogs. Most of the reason for me was personal in nature so you can also choose accordingly. Why Pelican? Once the approach to make this blog as static was finalized, the next big question came was which technology to choose, Pelican or Jekyll . There as already many comparison already available among these, but the only reason for me to choose pelican was because it uses python and jinja. In some near future I want to fully customize my blog with the knowledge of these two. Why Github? The final decision to be taken before starting this blog was to finalize the hosting provider. We have already mentioned some popular choice are Github Amazon S3 Dropbox I chose Github, for its near zero cost, it may cost you if you want to keep your repository private else it is completely free were as Amazon S3 would have required to shell out some money though less, with some extra benefits, but for the time being when I am just measuring the water it made sense to keep my cost down. Once all the above 3 moral questions were answered, setting up the blogs was easy and which will be documented in the future. Collated Blog post. How to set up Anaconda Python environment for Pelican Blog. Pelican HelloWorld using pelican-quickstart Pelican Folder Structure Pelican commands to generate the first blog. Understanding the Pelican Settings files Publishing your blog to github pages Configuring Github pages with Custom Domain Customizing Pelican blog with the help of Plugin and themes Expressing the content with Pelican Themes Problem Faced when integrating with Elegant Themes Home Page feature for Elegant Theme Integrating 3rd party services with Pelican Blog. Tips to improve work flow in pelican blog. Reference Making a Static Blog with Pelican This above blog explains why a static blog generator is good. Moving Blogs to Pelican This blog has a terrific explanation for pelican vs jekyll. Amazon S3 Vs Github Pages This explains the benefits of Amazon S3 over Github Pages, kindly check if you are affected because of this.","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/cost-effective-blogging-with-Pelican-and-Github.html","loc":"https://www.archerimagine.com/articles/pelican/cost-effective-blogging-with-Pelican-and-Github.html"}]};