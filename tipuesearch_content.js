var tipuesearch = {"pages":[{"title":"Who really uses AWS IAM API keys?","text":"Introduction Developer community uses the cloud technologies the most. In your conquest to learn about AWS, you have been focusing on configuring thing using the AWS console. As a developer, you may not find the use of AWS console efficient to do the tasks. Sounds like Neo's trapped in the matrix. It's you, but you didn't find Morphaeus. You have no knowledge of the existence of the RED pill to show you the truth. Soon you can access AWS Cloud with tools like CLIs, SDKs or HTTP APIs. These are the tools you completely understand. Even if you don't, sit tight we will make it easy to flow along. You will first create a user with programmatic access, then progress to configure the developer's machine to connect to the cloud. After this, you will get information from the cloud using programmatic access. Stay seated and enjoy this journey of programmatic access to the promised Earth of the Cloud. IAM API Keys You have already found out that there are three ways to connect to AWS . The AWS console is the most basic way to access. AWS CLIs and SDKs provide a much better way to access, as this can be controlled programmatically. To revise we will list does the various ways to access AWS. AWS Console AWS SDKs AWS CLIs Windows PowerShell You can use any of the above methods to access AWS, all of them use the AWS APIs in the back-end. The way of access to AWS does not change the features of AWS. You might think, AWS Console is easy to use, there is a web interface, you type in the username and password, which allows access to AWS. How will you use the AWS using these programmatic ways. IAM API Keys to the rescue You need to have access to IAM API keys to enable programmatic access to AWS. This API Keys is tied up with an IAM user, so you have to create an IAM user and enable the IAM API Keys. So let's dive into the world of creating and using the IAM API Keys. How to create IAM API Keys? The first step is to build an IAM user. You have already learned the creation of IAM User here . IAM User creation is a five step process. We have to follow all the steps with a minor change in Step 1 . You can see below, we have to just select the option, Programmatic Access . You can also enable this user to have an AWS Console access by enabling the option AWS Management console access . You have to follow the remaining steps from the IAM user creation article . In the step five, Success , you have to take some specific action. You might see in step five, your screen will look little different than the last time you created the user. We have two new field Access Key ID Secret access key You will get these two fields only when you enable the IAM user with programmatic access. Please download the security credentials in CSV format, and keep it safe. Previous time you may have seen this screen. In this the user did not have the programmatic access. Now you have a user with IAM API keys enabled. Properties of IAM API Keys You should keep in mind few important points about IAM API Keys. The above user creation step is the only time you will see both Access Key ID and Secret access key together. Access Key ID is only visible in IAM User's security credentials. It is advised to key the security credentials downloaded in CSV format. Once lost, you cannot recreate the Secret access key corresponding to the Access Key ID . You have to deactivate the old Access Key ID and create a new pair of Access Key ID & Secret access key . You might have already figured this out, since the IAM API keys are tied to an IAM User, we cannot have it associated with IAM Roles . The above combination should never be stored in an AWS EC2 instance, you should use IAM Roles for this. How to use the IAM API Keys? You can use these IAM API Keys in two major ways. AWS SDKs AWS CLIs We will go through both these steps. Using IAM API Keys with AWS CLI. We can install the Python based AWS CLI with a simple command if you have pip package manager. pip install awscli To use the AWS CLI you need to configure the AWS environment by running this command. aws configure If you get four questions as an output than it proves that the AWS CLI is installed. The above command will ask four questions which you can provide the details from the above Access Key ID & Secret access key you created for the new IAM user. AWS Access Key ID [ None ] : AKIAIOSFODNN7EXAMPLE AWS Secret Access Key [ None ] : wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY Default region name [ None ] : us-west-2 Default output format [ None ] : json Let me try to explain you the details about these 4 options. AWS Access Key ID [None] : fill the value you got for Access Key ID AWS Secret Access Key [None]: fill the value you got for the Secret access key . Default region name [None]: You should provide the region on which the AWS CLIs or SDKs should execute. Default output format [None]: JSON is generally the preferred option. The above execution creates 2 file in ~/.aws folder. -rw------- 1 user staff 116B Dec 24 23 :43 credentials -rw------- 1 user staff 44B Dec 24 23 :43 config The credentials file contains this, you gave the Access Key ID & the Secret access key above. [ default ] aws_access_key_id = AKIAIOSFODNN7EXAMPLE aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY The config file has this. [ default ] region = us-west-2 output = json With this you can execute AWS CLI specific commands. Here is a reference of such commands . Once you have the CLI installed and configured, using the SDK is much easier. Using IAM API Keys with AWS SDKs. AWS support SDKs in many languages. C++ Go Java JavaScript .NET Node.js PHP Python Ruby You will use the Python SDK for this example. The Python3 AWS SDK is called Boto3 You should complete the AWS CLI installation before proceeding. Boto3 installation | QuickStart Python has a very good package manager called pip , and we can install boto3 with a simple command pip install boto3 Post this you need to add a few details to some configuration files if you have still not installed the AWS CLI. Create the credentials and config file in the aws directory as shown above. Here is a sample code, which just list all the buckets on AWS S3. Even if you do not understand AWS S3 no worries. The below code will not give any errors if you do not have any S3 buckets. No error in the below code signifies that the AWS SDKs and CLIs are working in conjunction with each other. import boto3 s3 = boto3 . resource ( \"s3\" ) for bucket in s3 . buckets . all (): print ( bucket . name ) Conclusion AWS Console if a great way to use AWS, but it becomes difficult to use only AWS Console for all the tasks. There are times when you may need to do the job more than once. Manually performing these repeated tasks is very difficult and subject to errors. AWS offers two additional access channels, AWS CLIs and SDK. These accesses require a user enabled with programmatic access. The same user can also have the console access with programmatic access. After you have created the user, you should note that you receive an Access Key ID and a Secret Access Key. These pairs of key are only available during user creation. This is the reason we should download this information in the CSV format, as this information is required multiple time for access. You now have all the raw materials to connect to AWS programmatically. The next step is to download the Python AWS CLI package and configure it. Once configured, you will see a folder in the home directory ~/.aws . This folder now has all the sufficient information to connect to AWS. The CLI is a powerful tool to use, if you still want the SDK, you can download the BOTO3 Python SDKs and use the sample code to access all the bucket in the S3 if you have created. If the above code executes without any error you should assume that the SDKs is installed. You can now use any of the methods to access AWS. Let me know your experience of accessing the AWS using CLIs or SDKS, was it easy or hard. Info Graphics Reference Photo by Ales Nesetril on Unsplash AWS SDKs Browse by Programming Language Boto3 | Python SDK AWS CLI Command Reference","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-iam-api-keys.html","loc":"https://www.archerimagine.com/articles/aws/aws-iam-api-keys.html"},{"title":"Doing AWS STS the right way.","text":"Introduction You have seen in the previous topic on IAM Roles , some users and resource can assume a role, moreover an IAM Roles are like a hat which anyone can wear and gets its power. One important part of this should bother you, how does AWS authenticate such users, if the user is a genuine or not. AWS STS of Security token service plays an important part in enabling IAM Roles . When you are using a cross account resource or any federated users, you can also use AWS STS to provide temporary user credentials. AWS STS though can be used to support mobile application using AWS resources, but it is advised to use AWS cognitio, which will be discussed in the future. You will learn what is AWS STS, what are its benefits, when to use it. You will also learn to use a specific Action/API called assumerole to get access to an AWS resource for an AWS cross account. AWS STS AWS STS (Security token service) as the name suggest, provides a security token for accessing a AWS resources. You may think AWS STS as the provider of temporary access. AWS STS has these specific properties when assigning temporary access It can range from few minutes to a few hours. Once the AWS STS provided temporary token expires, it cannot be reused at any point. You can invoke AWS STS only through AWS SDKs or AWS CLIs. Benefits of AWS STS AWS STS solves a very specific problem for you, when you want someone to temporarily access your AWS resource without having concerns of revoking the permission. AWS STS provides a way to You should not embedded long term AWS security credentials into an application. You should not create extra IAM identities, using IAM roles with AWS STS is enough to satisfy the temporary access requirement. You do not have to worry about deactivating the AWS STS credentials, 36 hours is the maximum you can set the AWS STS expiry time depending on the API invoked. When to use AWS STS You have now understood what is AWS STS, also what are the benefits of AWS STS. You might also have guessed the use cases for using AWS STS. Here if a breakdown for this. In Hybrid Cloud setup, where you have to give access to the non AWS account holder. These methods are generally used for giving access to 3rd party SAML 2.0 Identity federation. Web Identity Federation. (Facebook, Github, etc.) Cross Account roles, when you have to give your developer account a temporary access to your production account. IAM roles for AWS services. AWS STS Actions You should learn about these five common AWS STS Actions. AssumeRole : This is used for getting cross account access. AssumeRoleWithWebIdentity : This is using any 3rd party web IDP like Google or Facebook. AssumeRoleWithSAML : This is for hybrid cloud, where you have an entity with SAML 2.0 GetFederationToken : This is used by the AWS root account or any IAM user. GetSessionToken : This is used by the AWS root account or any IAM user. Here is a comparison for you on the above APIs. AWS STS | AssumeRole Action You have the basic understanding of the different Action provided by AWS STS. Let's now try to use AssumeRole API to understand how this works. Here is what you are going to try, or what we call a problem definition. You will have a user with no permission on the AWS Account. Now create an IAM Role, with AmazonS3FullAccess permission. Once you have the Role, edit the trust relationship to give ARN of the user which does not have any permission. Now using AWS Boto3 SDK you will make the user connect to AWS. A pictorial representation of the step to help you understand. AWS STS | Create Role You have to follow all the steps mentioned in the article, IAM Roles . Once change would be this time we should select the Another AWS account option. You may need to give the 12 digit account number. The above steps are the same we will use for cross account access. The steps will not change. AWS STS | Change the trust relationship When you create the role like it is mentioned in the previous step, by default it will always point to the account root user, you have to change it to the ARN of the user you want to do a AssumeRole . Here is how you can do it. Select the role. Click on Trust Relationship options, and select the Edit trust relationship button. This will open a JSON Editor and edit the JSON for this particular user shown below, esp. the Principal, AWS option. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::123456789012:user/Test\" }, \"Action\" : \"sts:AssumeRole\" , \"Condition\" : {} } ] } You might be thinking, if we have to edit this option every time we have to assign to a new user, then how is this scalable? The answer is, most of the time we will add a particular group with the IAM role attached and the required user is added or removed from the group to control the access. AWS STS | BOTO3 code to AssumeRole Now you have to write using the BOTO3 SDK provide for Python, the sample code to AssumeRole . Here is the sample code. import boto3 import pprint from boto3.session import Session # Below is the ARN of the role. arn = \"arn:aws:iam::123456789012:role/account-s3-full-access\" session_name = \"example-role\" client = boto3 . client ( \"sts\" ) account_id = client . get_caller_identity ()[ \"Account\" ] print ( account_id ) # Assume role takes the roles ARN and a sample session name response = client . assume_role ( RoleArn = arn , RoleSessionName = session_name ) pprint . pprint ( response ) # Create an S3 resource that can access the account with the temporary credentials. temp_credentials = response [ \"Credentials\" ] # Access the S3 as a resource passing the temporary credentials received from STS. s3_resource = boto3 . resource ( \"s3\" , aws_access_key_id = temp_credentials [ \"AccessKeyId\" ], aws_secret_access_key = temp_credentials [ \"SecretAccessKey\" ], aws_session_token = temp_credentials [ \"SessionToken\" ], ) print ( f \"Listing buckets for the assumed role's account:\" ) for bucket in s3_resource . buckets . all (): print ( bucket . name ) Once you run the above code, you should be getting the list of S3 buckets in your account. Though the user did not have access initially. AWS STS | AssumeRole | Return Parameters You should be thinking what is returned by STS, here is the JSON response returned by calling assume_role BOTO3 API. { \"AssumedRoleUser\" :{ \"Arn\" : \"arn:aws:sts::123456789012:assumed-role/acc-s3-full-access/example-role\" , \"AssumedRoleId\" : \"AROAUXRIFYXT7BG3ENQGE:example-role\" }, \"Credentials\" :{ \"AccessKeyId\" : \"ASIAUXRIFYXTUVPIQWFL\" , \"Expiration\" : datetime.datetime( 2021 , 3,20,9,24,27, \"tzinfo=tzutc())\" , \"SecretAccessKey\" : \"BsXYZCGNuemA8wevm6CnYVfZtNgdGaoOCJ4VwXnf\" , \"SessionToken\" : \"FwoGZXIvYXdzEEoaDFAVerghnmasN971Z76yKwAfJgq3tccU72Gj6Xl28zJwJIUS/UEEMtwYmxUDsplTKg0if/keQ9z1BdoPFdLsmDtUiWDnfvIkICUbCeVk+DKI4c9LtdIAXmhpssg4IAMncYFsmh+ylOdbbcud134TOkDkCtuZMkfKuUbIMG3lTq10k93DsiUFAoH5pqyLAa9IyqHUbKUxwwde0UAcUU1lNFMO/sTZI8kAIQNM4cpGMxdyPsYZaX5M1IGWqr2gPNLqLtKLvi1oIGMi33r+lP9GWX5W+Ich1MHUAfUfhgqIjXHjpmDQY5S0e/WOTBwrPLoorgXQlHMak=\" }, \"ResponseMetadata\" :{ \"HTTPHeaders\" :{ \"content-length\" : \"1057\" , \"content-type\" : \"text/xml\" , \"date\" : \"Sat, 20 Mar 2021 08:24:27 GMT\" , \"x-amzn-requestid\" : \"72f38158-1d90-4619-a431-5a2fcf460a31\" }, \"HTTPStatusCode\" : 200 , \"RequestId\" : \"72f38158-1d90-4619-a431-5a2fcf460a31\" , \"RetryAttempts\" : 0 } } You should really be concerned about the Credentials parameters which is returned. It basically provides 4 information. AccessKeyId : The access key id, which is always required for programmatic access. Expiration : Generally it is 15 min, but can vary depending on the type of API being called. SecretAccessKey : The secret access key, which is also generated only once. SessionToken : As the name suggests, a unique way of identifying the session. Conclusion You might have now understood that IAM Roles and AWS STS have a symbiotic relationship. AWS STS is required when you need to provide these range of access. The cross account use case, ex. the developer account my need a temporary access to the production system. The Hybrid cloud use case, ex. On premise user, authenticated using SAML may need access to AWS resources. In Hybrid cloud use case and cross account use case, ex. Authenticating the user using the web identity providers. Sometimes IAM Services may need permission to another service, ex EC2 wants to write to a S3 bucket. The above access is provided to you using the AWS STS because. AWS STS provides short term credentials, which lives from a few minutes to some hours. We should not be bothered to revoke the access as you cannot reuse the expired access. AWS STS can be provided using the AWS SDKs or CLIs. The real benefits of AWS STS are, No need to embed long term credentials to the application. No need to create multiple identities for each access request. No need to revoke the access, as it expires automatically. The various actions provided by AWS STS are AssumeRole AssumeRoleWithWebIdentity AssumeRoleWithSAML GetFederationToken GetSessionToken and we discussed AssumeRole Action uses the cross account example, where you created an IAM role, edited the trust relationship and then the user assuming the role, by using the Boto3 SDKs. By doing this the IAM user from another account could access AWS resources for a short period of time. The AssumeRole returns these parameters when a call is made it AccessKeyId Expiration SecretAccessKey SessionToken You might recognize it returns all the parameters created for IAM user having programmatic access, i.e. AccessKeyId and SecretAccessKey. In addition, we get the SessionToken and an expiration time. Please provide you feedback if you have any other use case for AWS STS. Reference Photo by Ales Nesetril on Unsplash How to Use a Single IAM User to Easily Access All Your Accounts by Using the AWS CLI Introduction to AWS Security Token Service(STS) Switching to an IAM role (AWS API) Providing access to an IAM user in another AWS account that you own Requesting temporary security credentials Switching to a role (console) Boto3 Python Gist","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-sts.html","loc":"https://www.archerimagine.com/articles/aws/aws-sts.html"},{"title":"A foolproof guide to AWS IAM Roles","text":"Introduction The FaceLess Men in GoT (Games of Thrones), hope you remember this character, or Raven/Mystique in X-Men. They both share a common power, what would that be? The power to change identity as they wish to accomplish the task at hand. This is what IAM Roles do in a very broad sense. You can also think of IAM roles as the Invisibility Cloak in Harry Potter, who ever acquires it becomes invisible. IAM Roles are just like a hat, which anyone within AWS can wear, and get the powers presented by the hat, and loses the powers as soon as the hat is removed. The hat does not discriminate between real users or hardware like EC2, anyone can wear this hat. You will also see how IAM Roles can be used, how it is created and attached. You will also see how IAM user is a different concept than IAM Role. IAM Roles An IAM role is an identity that you can create having very specific permissions. The only difference being you are not associating this policy to any particular user or services at the time of creation. This IAM role can be assumed by anyone who needs it, be it a user or an AWS resource. You might be thinking, which problem does IAM role solve which a normal IAM user or a new IAM policy cannot solve. So lets understand why we need the use of IAM Role. IAM Roles for user When you are creating a user in your own account, you cannot give them all the permission as explained in IAM Introduction , the principle of least privileges. You cannot have one or multiple users will super user permissions, you cannot have Batman in every city, only Gotham. In addition, we can also have non AWS accounts, like cross account want to access your AWS services, or the federated user, who may also need access to the AWS services in case of Hybrid Cloud. You have to consider many other use-case along with the above two, for understanding we cannot keep on creating IAM Users to address such diverse range of permission request. IAM Roles for AWS Services You might have already read about IAM Policy , AWS resources cannot have IAM policy attach directly. There is another good policy, AWS credentials should not be stored on EC2 instances. If you consider the above two guidelines, it will not be possible for any AWS services to communicate with other AWS services / resources to complete the work. IAM Roles to the rescue When you consider both the limitation of IAM User and AWS services, you can think we need some type of magic hat, which gives the sufficient power to both User or Service to execute the task it is elevated for. This is where magic happens with IAM Roles. You can use IAM Roles for:- One AWS service have to access another AWS service. Example: Application running on EC2 may need to access S3. When you have a Hybrid Cloud implementation User from On Premise may want to access AWS Cloud infrastructure. EC2 can assume a role only at the time of creation, but once it has a role attached, you can modify it using APIs, CLIs or console at any time. You can attach only 1 role to EC2 at any given time. Benefits of IAM Role You may need people from Hybrid cloud implementation to access the AWS account, you may need IAM Roles to be attached once these users are authorized to use SAML or Active directory. You may have production and development account in AWS, and your development team may be required to be elevated to access a production account to fix a bug. This can be done using IAM Roles. You have an application running on an EC2 instance, It needs access to S3, this can be achieved using IAM Role. How to create an AWS IAM Role AWS IAM Role creation is a three step process. Step 1 | Select the trusted entity The first step guides you choose which services or identities can assume the role, your options are AWS Services The EC2 instance, lambda etc which needs access to other services. Another AWS account This is a cross account access, like a developer taking a production access etc. Web Identity Using OIDC to validate user, like the mobile phone apps. SAML 2.0 Federation Using office resources as authentication parameters, like using service Active Directory. You can select AWS Services for this experimentation, You can attach this to an EC2 instance, so that it can access the S3. Step 2 | Attach permission policy You can now select policy required to be attached, this step is same as the explained in IAM Policy . You have basically chosen from one of these 2 type Custom policy, created using the visual editor or JSON editor. AWS managed Policy. You can select the AmazonS3ReadOnlyAccess for attaching to EC2 instance as agreed in Step 1. Step 3 | Review You have to provide these details to complete the IAM Role Creation process. Role Name An explicit name, which can be used to attached to the EC2 instance. Role description A description about the role. Trusted Entities This is the service selected from Step 1 . Policies The policies attached, in this case AmazonS3ReadOnlyAccess How does the IAM roles looks after creation? You can see above, this is how an IAM Role looks like after creation. The policy document looks exactly same as an IAM Policy . This you can also assume as a baton in a relay race, the service or the user can run with the permission as soon as it gets the policy in the form of an IAM Role. IAM Roles Vs IAM Users You should see this video tutorial on YouTube. This video explains much better, also the flow chart shown above should also clear your doubts You can see the logic is elementary, If you have a non living things, like EC2 etc., it gets an IAM Roles. No question asked. If it is a living thing, like a person, we have to ask two questions. If the permission is temporary then he gets an IAM Roles, else he is an IAM User. Hope this clears your mind about the difference between two confusing terms. IAM Roles Vs IAM Policy IAM Roles and IAM Policy both have a JSON document identifying the rule. The only difference is the mention of Trusted Entities . Conclusion You might now be relieved by understanding the concept of IAM roles, as discussed we use IAM Roles for mainly three purposes. One AWS service using another service, like EC2 instance, wants to read from S3. In the hybrid environment, a non AWS user might need access to AWS resources temporarily. Cross account access, where a developer may need access to production account. You should also think of an IAM Roles as a hat, which a person or services wears and it magically gets the permission and when it drops the hat, it comes back to original state. You also should think about EC2 instance, it should not store the IAM User credentials, they should be using the IAM Roles instead. The IAM Roles can be attached to an EC2 instance, during creation, and can be changed afterwards, but not attached after creation. You can attach only one IAM role to an EC2 instance and not multiple. You learned that IAM Role creation is a 3 step process. Select the trusted entity Attach permission policy Review The IAM Role has been just like IAM Policy document, a JSON document having statements. You can be confused with IAM Role and IAM User, we can simplify it, saying, if it is a resource we use an IAM Roles. If a physical user needs temporary access, it uses IAM Roles, otherwise it is an IAM User. You can comment and let me know if the IAM user is different than IAM Role. Reference Photo by Ales Nesetril on Unsplash Photo by Laura Thonne on Unsplash Photo by Ali Kokab on Unsplash Photo by Almos Bechtold on Unsplash AWS IAM Overview - It's Surprisingly Simple - Users vs Roles","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-iam-roles.html","loc":"https://www.archerimagine.com/articles/aws/aws-iam-roles.html"},{"title":"Global Infrastructure comparison of AWS, GCP and Azure. Updated 2020","text":"Introduction AWS, Azure, GCP are the three largest cloud infrastructure providers. These three are already a big name within the technological space. These enterprises have a greater understanding of the technological space. You may be wondering what is the difference between these companies that offer cloud infrastructure?, what really sets these player apart? Can they not simply replicate in order to provide similar infrastructure and services? Let's us compare these Cloud Infrastructure provider with respect to just the infrastructure point of view, we will compare these on five broad terms. Region Availability Zone Countries Served. Connecting to On Premise CDNs The data used for these comparisons are collected from the individual cloud provider's documentation. AWS and GCP had their data presented in straightforward fashion, Azure was little indirect, but tried consolidating the data for you in the best possible manner. Region All the major Cloud Providers have their data centers, which host their core services across the planet to help you get the best reach. The region is a big differentiator for a lot of business to choose a Cloud Provider. The above picture depicts that Azure is a clear winner with almost double the number of regions from both its competitors. This decision to make Azure a clear winner is little in the Grey area. For example, if you have to run a dedicated cloud server out of China, you only have AWS and Azure as your option other than the local vendors. GCP won't be an option for you in China. Similarly for cloud operation in South Africa again, you are out of luck with Google. Azure has the highest density of the region among cloud providers, this helps in failover switching in case of a complete region going down, this is explained very beautifully for you in this blog . Availability zone Once you are through the region, the number of actual physical data centers available in a region, called as Availability Zone, is your next decision making point. The above graphs, clearly depicts that AWS and GCP are head to head and Azure is lagging behind. This will be the truth if you consider the above graph individually. When you see this data point along with the region data points, you can argue that all the cloud vendors are pretty tied up in these regards. Azure did announce its beta of AZs only in 2017, with that in mind it still has to catch-up, but till this point you will agree that the decision to opt for a particular cloud vendor based on geography is still not decisive. Countries Served Counties served individually by the cloud vendors may not be a such great differentiator for the service, but if you reside or your customer resides in any such location then you are out of luck. The above graph, clearly shows AWS as the winner, but even the others will be catching up. AWS will not have any more countries left to expand and these other vendors can easily catch-up if their intent to do so. Direct Connect Vs ExpressRoute Vs Dedicated Interconnect Direct Connect or Azure ExpressRoute or GCP's dedicated interconnect, is a way for you to connect your on-premise data center to AWS. This helps in reducing the cost to you and also avoids the open Internet for your private data. The above graphs shows Google as the winner, but as discussed there is not much difference with the other vendors. The above graphs also only considers the Direct Connect run by the vendors individually and not the partner data centers. When you consider the partners, then maybe the horizon may change. EdgeLocation Vs EdgeZone The CDNs, for making the content available to the user as close to his point of contact is a very important decision to make while choosing a cloud vendor. AWS is the winner again and that too with a big difference. This graph again considers only the vendor owned CDNs, the number may vary greatly if you include the partner's location. Conclusion The above depiction of five key infrastructure variable may not pursue you to a particular cloud vendor. Region and AZs can be a deciding factor, depending on where you want to run your data centers, but also you need to think that all regions are not created equals. As described in this blog , there is a hardware difference between region. When you are deciding between vendors, check these parameters before deciding Geographic limitation for operations. Hardware type as per the region. Countries Served is not a very important parameter to differentiate between the cloud vendors, as in the near future these may not be very important factor. Direct Connect and CDNs also cannot make an important factor if you consider the partner networks. This way you can be sure that just infrastructure cannot be the differentiator while choosing a Cloud vendors, until it is not because of geographic location of the server. There are other factors which come to play when you are deciding on a particular cloud provider, like the services provided, the pricing, ease of deployment, support provided and also expertise availability in your region. You have to take the holistic view while deciding on a particular cloud provider, even nowadays where the difference between cloud providers' services also matches, you can also opt for a multi cloud solution, choosing what is best from a particular cloud provider. Info graphics Reference Availability Regions and Zones for AWS, Azure & GCP Google Cloud Global Infrastructure [AZ-104] Region, Availability Zone, Availability Sets and Fault Domain,Update Domain In Microsoft Azure Azure Regions Interactive Map IaaS Resilience, which cloud platform is better? Azure or AWS? Global Infrastructure Google Cloud locations Introducing Azure Availability Zones for resiliency and high availability When we have discussion on the region, AZs, countries, direct connect and edge location above, what will you think is important factor when you are deciding on the cloud vendors, comment below and let me know?","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-global-Infrastructure.html","loc":"https://www.archerimagine.com/articles/aws/aws-global-Infrastructure.html"},{"title":"AWS Made Easy | IAM Users","text":"IAM Users IAM users are an important AWS resources. This is the identity given to a physical user or application when he logs into AWS. AWS identifies an IAM user with these identifiers. Friendly user name: We provide an identifiable user name. ARN: An amazon resource name (ARN), Access ID: A unique identifier which is returned when working with SDKs or CLIs. IAM Users | User Permission The default IAM User permission is non-explicit deny for all AWS services. To provide the user access we can be provided explicit permission in one of these ways. Adding a user to a group. Copying permission from another user. Attaching a policy directly. IAM Users | Groups In an organization, it makes sense to group each department into a group and the assign users to these groups. IAM Users | ARN ARN (Amazon resource name), is specified in this format. arn:aws:iam:<region>:<account_id>:user/admin the IAM user ARN is like this arn:aws:iam::<account_id>:user/admin The <region> is missing in a IAM user ARN, as IAM is a global service. The user/admin defines the actual resource. IAM Users | Create a new user. IAM user creation is a 5 step process. IAM Users | Creation | Step 1 | Add user The first step in the IAM User creation process is: Add user. We have to fill a few of the fields. Lets understand these fields. Set user details We should provide the user name for the IAM user, mostly a friendly name. We can add multiple user in this step. Select AWS access type: There are two ways for the user to access AWS resources Programmatic Access : This is to generate the access key ID and secret access key for AWS CLIs, SDKs and other development tools. AWS management console access : Creates a user who can log into the AWS console using a password. We can mix and match the programmatic access and also management console access. IAM Users | Creation | Step 2 | Set Permissions The next step in user creation is to set permissions. There are 3 ways to add permissions to a user. Add user to a group. Add the new user to a group based on the job function. Copy permission from existing users. If we have existing user, we can copy policy for that user. Attach existing policies directly. We can singularly attach a multiple policy to a user directly, both AWS managed and customer managed. The most efficient way to add permission to a group of user is to add the user to a group. IAM Users | Creation | Step 3 | Add Tags Tags are an important component in all service creation. We can assign a key/value pair to manage user efficiently. IAM Users | Creation | Step 4 | Review The penultimate steps of user creation is to review the configuration we have provided. As show in the screen shot above, it displays all the configuration selected till now based on which the user will be created. IAM Users | Creation | Step 5 | Success In the last step of user creation it has given these options. Download csv This CSV gives these details User Name Password Access Key ID Secret access key These two are provided when we have the programmatic access enabled. Console login link This is the only time we get the Secret access key along with the access key ID, there is no way to recreate the same secret access key again. We also got the some of the above information displayed accordingly the way the user was created. IAM User Properties Lets see a few of the properties displayed for the user in the AWS IAM dashboard. The above screen shot shows that the IAM user summary / property can be grouped into Permissions Groups Tags Security Credentials Access Advisor From the above group, the most important to consider is Security credentials. Permissions The above screen shot clearly depicts, this property displays the permission present to the user. In the above case, the user gets 2 IAM policy Attached directly. The IAMUserChangePassword is the policy applied directly to the user. Attached from group. The Administrator access is attached via the Group. Groups & Tags This property depicts the groups and the tags associated with the user. Access Advisor There is always a need as an administrator to review user behavior in AWS like the number of services used, the frequency in which it is used. In the Access Advisor tab, IAM provides this information about the user, and helps in identifying the unused permissions. Security Credentials The major section of security credentials are Sign-in credentials Provides with link to console sign-in link Console password: is it enabled or not Assigned MFA device: Is the device assigned or not Signing certificate: We can upload any specific X.509 certificate for use. Access Keys The keys to make programmatic calls to AWS by CLI, SDKs, API calls. We can even create an access key from here. SSH Keys for AWS code commit SSH public key to access AWS code commit. HTTPS Git credentials for AWS code commit User name and password to access AWS code commit Credentials for Amazon keyspaces. User name and password to access Amazon Keyspaces. Conclusion The AWS IAM user is anybody who will use the login credentials or the programmatic access to AWS. Each IAM user can easily be identified using a friendly name, or a unique ARN or an Access ID. The IAM user can get permission by getting itself attached to a group, or having the policy attached directly or copying from an existing user. Providing permission by adding them to a group is the best IAM policy. AWS IAM user creation is a 5 step process, but the important steps are only 2, the first one has given a user name to the user, along with the type of access it needs. This is followed by, adding permission to the user, which can be done by attaching it to a group or copying from an existing user or by attaching the policy directly. The AWS IAM user has specific properties like permission which it has and from where it got the permission, like attached directly or received it from a group. There is a specific property called Access Advisor, which an administrator can use to review if the user is regularly using the permissions or not and depending on that can review to keep the permission or remove it. AWS IAM users also have a property called security credentials, which tells if the user has a password or not and also if MFA is enabled or not, we can also see the access ID but not the key, and we can add the ssh keys for code commit. Each action done on AWS is done by some type of user. Reference Photo by Ales Nesetril on Unsplash AWS Docs | What is IAM?","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-iam-users.html","loc":"https://www.archerimagine.com/articles/aws/aws-iam-users.html"},{"title":"AWS Made Easy | IAM Policy","text":"IAM Policy A document which provides the details of the permission granted to access any AWS resources is called an IAM Policy. The default policy applied to all AWS users is non explicit deny . The IAM Policy takes effect as soon it is attached to a user or group, there is no delay in its application. The User & Groups can have more than 1 policy attached at any given time. Roles are utilized because we cannot attach policy directly to AWS resources. IAM Policy Document How does an IAM Policy look like. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"*\" , \"Resource\" : \"*\" } ] } The above is an example of an IAM Policy If we observe closely, these are the various important points to note about the IAM Policy. Statement Each IAM policy is composed of statements . Policy can have one or more statements. In the above example, there is only 1 statement, composed of Effect It tells if the impact is allowed or deny. It takes these 2 values. In the above example, it is Allow for all. Action What type of action is allowed or denied. We can drill it down to single API also, which we will see later. In the above example, it is allowing all actions. Resource Which resource are being accessed using the policy. Like in case of S3, it can mention the resource is a S3 bucket, not the full S3. In the above example, all the resources are being granted access. Explicit Allow & Explicit deny When granting access explicitly we have 2 categories of IAM Policy Explicit Allow Explicit Deny When a user has both explicit allow and explicit deny, then the explicit deny always take precedence. In all cases just remember deny is the best way. Example of explicit allow { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"*\" , \"Resource\" : \"*\" } ] } Example of explicit deny { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Deny\" , \"Action\" : \"*\" , \"Resource\" : \"*\" } ] } IAM Policy Templates We can have an IAM policy in two ways. Pre-Built Policy Custom Policy Pre-Built Policy These are the policies provided by AWS to all the users. We can just pick and use which ever we like. These policies are identified with an Amazon logo just next to them. Most commonly used pre-build polices are Administrator Access Power User Access Read-Only Access Administrator Access Administrator Access: Full Access to all AWS resources. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"*\" , \"Resource\" : \"*\" } ] } Power User Access Admin Access, except it does not allow user-group management. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"NotAction\" : [ \"iam:*\" , \"organizations:*\" , \"account:*\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:CreateServiceLinkedRole\" , \"iam:DeleteServiceLinkedRole\" , \"iam:ListRoles\" , \"organizations:DescribeOrganization\" , \"account:ListRegions\" ], \"Resource\" : \"*\" } ] } Read-Only Access Only View AWS resources. This policy is very big, please search for it in AWS. - The AWS managed policies cannot be edited, they are read only. Custom Policy Some time the requirements of a policy cannot be fulfilled from an AWS managed policy. We have to use a custom policy in those cases. We can import the pre-built policy into our own, and then modify them. Custom IAM Policy Generation In addition to AWS provided policy, user can also create custom policy. We have 2 methods to create custom policy. Visual Editor JSON Editor Visual Editor The are 4 selections, we have to do for creating a policy, as shown below. Service Choose a service like, IAM, S3, EC2 on which the policy is applicable. Actions Based on the service selected, we can choose the Actions, which can be associated with it. Primarily the actions are List Read Tagging Write Certain service have certain extra Actions associated with itself. Resources We can restrict the Actions allowed on Services selected to specific resources. We may not have to provide the full access. The access can be restricted to even a particular ARN. Request Conditions The policy can also specify if the MFA is required for the access Only a particular public IP should be We also restrict using the time or the day and various other parameters. All the conditions if selected is AND ed We can also take a easy way out, by \"Import Managed Policy\" and then modifying it to our need. After selecting the above information we are only left with providing Tags Review JSON Editor We can use JSON editor if we understand the JSON syntax, and the format in which AWS wants the KEY/Value pairs. We can also import an existing policy and then edit the JSON. Conclusion To summarize, IAM policy is a document representing the permission encapsulated inside for a USER or a Role. The default policy on IAM is non explicit deny, but though IAM we can give either an explicit deny or an explicit allow. The explicit deny always takes precedence over any explicit allow. IAM policy are categorized into either managed by AWS, or user created. There are many AWS managed policy and the most useful once are Administrator policy, Power user policy and Read Only Policy. The IAM policy is a JSON statement, and having these 3 statement at the minimum Effect Action Resource IAM policy can be created by using the custom visual editor or the IAM JSON editor. Generally for new users the visual editor is beneficial once the user becomes comfortable, we can change to JSON editor. Multiple policy can be applied to Groups and Users. Roles are used for AWS resources. Reference Photo by Ales Nesetril on Unsplash Creating IAM policies (console)","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-iam-policy.html","loc":"https://www.archerimagine.com/articles/aws/aws-iam-policy.html"},{"title":"AWS Made Easy | IAM Introduction","text":"IAM Introduction IAM provides the interface to join the great world of AWS. Its major goal is to provide both human and computer, who are the consumers of AWS services a way to access them. IAM Components IAM service have five(5) components:- User - The user of AWS resource, can be a person or a machine Groups - The above user can be grouped together Policy - It defines the permission of the IAM identity. Roles - A roles are just a policy, but not associated with the Users directly. API Keys - The keys used to programmatically accessing the AWS console. IAM property AWS works on two very important principles. Principle of least privilege. Any user, group must be granted minimum permission to complete the activity. Non explicit deny rule. We are new user have no explicit allow rule for a resource, AWS assumes it to be access denied. Only an explicit allow can override a non explicit deny rule. Admin Vs root User root user This is the very first user which is created when we sign-up for AWS using our emailID. This user has god like access and is not under the control of IAM. This login must never be used for daily activities within AWS. These 5 steps should be done as soon as we login to AWS root account. Delete the root access keys. Enable MFA Create user Create groups and assign user to groups Apply an IAM password policy Admin user We can create a copy of the root user with complete admin access. This is the Admin user. This is user comes under the purview of IAM and should be used as a daily driver in AWS. Conclusion When we login to the AWS console for the first time we create the root user, this is the god like user which has all the access and have no control via the IAM policies. This user is not supposed to be used for day to day activity, in place we should be using another user called admin which has all the policies similar to root user but is controlled via IAM. There are a few important steps to perform as soon as we login to the AWS console for the first time, which is to delete the programmatic access key for the root user followed by creation of users and groups. The policy to be attached to groups and user can be part of groups. There also should be a password policy specified for the users. AWS works on 2 important principle, Principle of least privilege Non explicit deny rule This means that, all the user should be provided the bare minimum of the access required to get the job done, and if not specified, by default the user does not have access to any resources on AWS. Reference Photo by Ales Nesetril on Unsplash","tags":"aws","url":"https://www.archerimagine.com/articles/aws/aws-iam-introduction.html","loc":"https://www.archerimagine.com/articles/aws/aws-iam-introduction.html"},{"title":"AWS - Shared Responsibility & Global Infrastructure.","text":"AWS - Global Infrastructure AWS is a cloud infrastructure provider, it's a pioneer in this field. Cloud and AWS have become synonyms, it would be difficult for a developer today to be unaware of AWS or cloud in general. This blog is an effort of mine to present this vast and ever expanding concept into very easy and fun method. Learning new concepts in their 40's is very difficult, would try to make it as easy as possible for anyone want to embark on this journey. AWS/Cloud has become an indispensable part of the Software industry due to a few of these important properties. Global Deployment Millisecond Latency Commission and decommission at will Pay as you use. Even if core AWS services are not available in certain location, it still uses different non AWS services to enable developer to relax and focus on developing great software. User Type There are 2 types of AWS users. root user The user which is created when we first become an AWS consumer by signing in. This user has God like permission, and is not governed by IAM rules. It is not safe to use this user for day to day activity. Administrator user This is the user which is created though the IAM. This controls the management of other user. This is the user which should be used for daily activity. It has all the access just like root user. Best Practices Always enable MFA for all users. Have an explicit password policy. i.e. expiry of password. Connection Type There are 3 ways to connect to AWS AWS Console The default mode, which we all use. This is the website for AWS, which can do pretty much all the stuff possible. AWS SDKs These are developers to make use of all the AWS resources while developing the application. Major programming languages are supported. AWS CLIs This use command line interface to control the AWS infrastructure. Service Location AWS hosts its service in two types of location AWS Edge Location These does not host any specific AWS service. These hosts only CDN or CloudFront. AWS region These are the real AWS data centers, which host all AWS services. Each region is further divided into an Availability Zone (AZs). AZs are created to provide fault tolerance and high availability. Most services are region specific, which means all the AZs will have that service. AWS - Shared Responsibility Model AWS provides us with the bare metal infrastructure for running our application on their server. This does not imply that outside our application code, we can rely only on AWS. AWS and us as a user work in a shared responsibility model, wherein, we have a certain responsibility and AWS has certain, and AWS is willing to shoulder certain responsibility with increased cost. AWS's responsibility is broadly related to the physical server's like, physical security, personnel security, discarding the storage devices, DDoS protection, and hyper visor isolation. User's responsibility is mostly towards their application like, proper security groups, IAM, MFA, OS patches and VPCs. Compute Services The major compute services provided by AWS are. Elastic Compute Cloud (EC2) Elastic Container service (ECS) AWS Lambda Elastic BeanStalk In the above 4, the user has the maximum control of configuration on EC2 as its bare metal server, with some services running. Lambda and Beanstalk provide the minimum configuration user and most of them are managed by AWS. Storage Service Just like computer the major storage service provided by AWS are * RDS - SQL database. * DynamoDB - No SQL database * RedShift - Data Warehousing * ElastiCache - In memory Cache. Summary AWS is a global infrastructure, it has to serve both its developer and consumer with great efficiency and low latency. This is the reason the AWS has its core services spread across the globe called the AWS region. It also has its edge location spread across the globe to serve the consumer. AWS region is subdivided into availability zones (AZs), which helps to provide high reliability and are connected to each other via a very low latency fiber optic. When an AWS service is mentioned to be region specific, it implies that it is available in all the AZs. There are two types of an Administrator user of AWS service. The first is the root user which has infinite God like permission and is not controlled via the IAM. There is also a admin user which we create from IAM. The admin user have all the permission like the root user, its is just that it was created by IAM. The user admin or root both should have MFA enabled and also have a password policy associated with it. The AWS services can be accessed using 3 methods. AWS Console AWS SDKs AWS CLIs All of the above will use the AWS APIs to connect with the core services. AWS Console is what every start using first and once becoming comfortable they rely on only SDKs or the CLIs. AWS has a shared responsibility with the users, where AWS manages most of the physical security and the server management like DDoS protection and HyperVisor isolation, the user is entrusted with the responsibility of managing the software securities like IAM, MFA, OS Patches, Security groups etc. What is the difference between the AWS Region and AWS Edge Location? When explaining the AWS Global Infrastructure these 3 Keywords are often used. Region Availability Zone (AZs) Edge Location Each of the above are key pieces of the AWS Infrastructure, but to understand the fundamental difference we have seen these with a different perspective. The above are divided based on User of the content present on AWS (End User) User of the content provider to AWS (Developer) Region and AZs as already explained are Developer focused, as a result, they are very few and have to adhere to the highest standard of scrutiny. Edge Location on the other hand, are User focused, Like CDN, Global Accelerator etc., their job is to make the content reach to and fro from the user faster, reduce the latency. As a result are far too many and spread across the world where it can serve its user better. Reference Photo by Eberhard Grossgasteiger on Unsplash Global Infrastructure Shared Responsibility Model Info-graphics","tags":"aws","url":"https://www.archerimagine.com/articles/aws/general-aws.html","loc":"https://www.archerimagine.com/articles/aws/general-aws.html"},{"title":"What is the cost of len() function in python?","text":"","tags":"Python","url":"https://www.archerimagine.com/articles/python/cost-of-len-function-in-python.html","loc":"https://www.archerimagine.com/articles/python/cost-of-len-function-in-python.html"},{"title":"100Days of Code Log File 4th Attempt","text":"Hello World!, You are about the witness the beginning of an epic second coming of the 100-Day coding journey, A story that great sages will pass down from generation to generation. This quest will feature a potpourri of unfiltered joy, unrivaled pain, and unexpected epiphanies. Some moments, I will be the smartest man alive. Others moments, I will be a stupid idiot. But each day, I will be a valiant warrior, fighting to develop and perfect the skills necessary to evolve into a true beast with these keys. I have failed in my previous attempt for the challenge, which you can find here . There are learning from the previous failure, here are the modification which was done to the challenge according to my handicap. Ladies and gentleman, I present to you, #100DaysofCode with @ animeshkbhadra Day 02 | Thursday 07 May 2020 Days Progress Completed the CSS Trasform lecture from Udemy . It was pretty amazing to learn about various type of transforms Thoughts These are the transform's learned. rotate : rotates the object. skew : skews the object. scale : zoom by a factor. We can also combine transition with transforms to give a nice animating effect. translate3d takes a mandatory z-index as a paramter. Reference Day 01 | Wednesday 06 May 2020 Days Progress I am learning HTMl/CSS from the Udemy . I started this course, some days back, from now, will post regularly on behalf of #100DaysOfCode. Today I have completed the CSS Transition. Thoughts Today I learned about:- The CSS transition property. Link to Tweet Reference QuoteFancy | Image Source 100DaysOfCode Official Website Udemy","tags":"#100DaysOfCode, python","url":"https://www.archerimagine.com/articles/100daysofcode-python/100Days-of-code-log-file_V_4_0.html","loc":"https://www.archerimagine.com/articles/100daysofcode-python/100Days-of-code-log-file_V_4_0.html"},{"title":"How to use Sublime Text 2 as a MarkDown Editor.","text":"Sublime Text Sublime Text is a great editor for code and text. There is nothing I can add to the above fact. Markdown is a great format to write the ReadMe's in GitHub, and a lot of other social media also allows MarkDown Format. Sublime Text 2 have basic code snippets triggered for MarkDown Editing. This itself is a huge plus for using Sublime Text for MarkDown Editing though the support is limited to these 6 Snippets, as a user we can always enhance these. The basic assumption I am making that you have the Package Control Plug-in installed. Sublime Text 2 with the help of these 2 Plug-in makes Markdown Editing work like a charm. * MarkdownEditing * InsertDate Plug-ins MarkDown Editing The basic support from Sublime Text 2 is enough to use it as a MarkDown Editor but the MarkdownEditing provides great support in terms of. Paring of Asterisks, underscores, brackets Creation of Numbered list and Un-numbered list, with tab support of Indents. Great key bindings. Good theme with decent syntax highlighting, though I will propose read the blog in the Reference section. These basic settings in helps a lot in MarkDown editing. Save it in Markdown (Standard).sublime-settings file. { # Sets the Color Theme. \"color_scheme\": \"Packages/MarkdownEditing/MarkdownEditor-Dark.tmTheme\", # Sometime md is not recognized as a MarkDown Extensions. \"extensions\": [ \"md\" ], # Enable Spell Check. \"spell_check\": true, # If you want the text to start in the left hand corner, default is Centered. \"draw_centered\": false, \"wrap_width\": 180 } Kindly visit the documentation in the GitHub page for more configuration and settings. Insert Date When writing a blog post using MarkDown we need to enter Date in the present Blog. This plug-in InsertDate is a great way to insert localized time into any documents based of few KeyStrokes. This plug-in is vastly configurable, so we can customize completely to our needs. Change this configuration in the file insert_date.sublime-settings { # Mention your local Time zone \"tz_in\": \"Asia/Kolkata\", # Sunday July 28,2019 Prints in this format \"format\": \"%A %B%e,%Y\" } strftime reference and sandbox , use this to format the date according to your need. The default Key combination to trigger this is [Ctrl + f5] [Ctrl + f5] . This works on all platform except Mac, as it triggers the VoiceOver App. Kindly see this issue for resolution. Conclusion In Conclusion, I will only try to mention that both, * MarkdownEditing * InsertDate are great Plug-ins to write MarkDown files, Lets use these to make a better ReadMe in Github. Reference How to Set Up Sublime Text for a Vastly Better Markdown Writing Experience MarkdownEditing InsertDate Default keybindings don't work on OS X Yosemite #28","tags":"SublimeText","url":"https://www.archerimagine.com/articles/sublimetext/how-to-use-sublime-text-2-as-markdown-editor.html","loc":"https://www.archerimagine.com/articles/sublimetext/how-to-use-sublime-text-2-as-markdown-editor.html"},{"title":"100Days of Code Log File 3rd Attempt","text":"Hello World!, You are about the witness the beginning of an epic third coming of the 100-Day coding journey, A story that great sages will pass down from generation to generation. This quest will feature a potpourri of unfiltered joy, unrivaled pain, and unexpected epiphanies. Some moments, I will be the smartest man alive. Others moments, I will be a stupid idiot. But each day, I will be a valiant warrior, fighting to develop and perfect the skills necessary to evolve into a true beast with these keys. I have failed in my previous attempt for the challenge, which you can find here . There are learning from the previous failure, here are the modification which was done to the challenge according to my handicap. Selected the resource in advance, Learn Python Track from Team TreeHouse FrontEndMasters | Python Fundamentals | Nina Zakharenko Complete first 21 Days first. Ladies and gentleman, I present to you, #100DaysofCode with @ animeshkbhadra Day 45 | Thursday August 8,2019 Days Progress Always group the custom exceptions into a common module called exception in application. The custom exception class name should always end with error or exception, signifying the purpose. Thoughts My Github Url Link to tweet Day 44 | Wednesday August 7,2019 Days Progress Error Specificity The order of catching Exception should be from specific to general. The code should never be catching generic Exception . Custom Exceptions We can raise our custom exception by inheriting from base class Exception . The base Exception __init__() takes a custom message. raise is the keyword to raise the custom exception. Thoughts My Github Url Link to tweet Day 43 | Monday August 5,2019 Days Progress Exception is a great way to make the application more robust. try/except are the tools to catch exceptions. BaseException is the base class of all exception which should never be caught in our code. Exception is the exception class which needs to be caught. except takes a tuple of exception. Thoughts My Github Url Link to tweet Day 42 | Thursday August 1,2019 Days Progress Inheritance is the best way to share property and responsibility across the code. Car has 4 wheels, but Bike has only 2 wheels, but most other parts are similar. It creates code re-usability. Divides the code from more generic to specific. Multiple inheritance is possible, but mostly restricted to Mixins. Thoughts My Github Url Link to tweet Day 41 | Wednesday July 31,2019 Days Progress str() : On our raw object, is we use str() it gives a completely non-relevant information. We can change this by overriding the __str__() method in our class. repr() : Its informs a way of creation of the object. We can override __repr__ method in our class for this information. Thoughts My Github Url Link to tweet Day 40 | Monday July 29,2019 Days Progress isinstance() : returns True if an object is an instance of a class. issubclass() : returns True is an object is a subclass of a class. bool is a subclass and instance of int object is the parent class of all object. any() : returns True , if any value in the collection is True. all() : returns True , if all the value in the collection is True. bool : bool being a subclass of int , we have some weird combination, which we can try. True + True : 2 {0,1, True, False} : returns {0, 1} Thoughts My Github Url Link to tweet Day 39 | Sunday July 28,2019 Days Progress Class methods are unique methods, which operates on class variables. @classmethod : a special decorator to create a class method. Class methods take cls as argument and not self . The instance object can access the class methods, since it is aware of their existence. Thoughts My Github Url Link to tweet Day 38 | Saturday July 27,2019 Days Progress The journey into object oriented programming with python. Python has no protection of its class variables for modification unlike Java. __init__(self) It is special function which python calls under the hood when initializing a object. It takes self by default. Thoughts My Github Url Link to tweet Day 37 | Friday July 26,2019 Days Progress OOPs concept. Everything is an object in Python. Class is template, blue print of a object. Instance is a specific creation of a class. We can have both Class Variables Accessed by class. Instance Variable Accessed by instance of a class. self is a special name given to a instance in python. Thoughts My Github Url Link to tweet Day 36 | Thursday July 25,2019 Days Progress .items() on a dict() returns a list of tuples. A list of tuples can also be converted back to a dictionary. zip() function combines two lists into a list of tuples. for loop can be used to iterate over a zip function. Using zip() on asymmetrical list will create a list of tuples with the least no of elements common in the list. dict(zip(list1, list2)) : converts a list of tuple back to a dictionary. Thoughts My Github Url Link to tweet Day 35 | Wednesday July 24,2019 Days Progress Enhanced the learning of list slice. Simple index gives an element of the list. The slice has the following format. [1:3] : the index will start from 1, ends at 3 - 1 = 2 No of elements will be 3 - 1 = 2 Slice also supports negative index. -1 : give the last element -len(list) : gives the first element Slice also has 3rd index, which is step . [::2] : skips 2 elements. [::-1] : short cut to reverse a list. Thoughts My Github Url Link to tweet Day 34 | Tuesday July 23,2019 Days Progress Today is the day, when I am more confused than aware. There is no tuple comprehension in python, but there are Generator expressions. Generator expressions are created with this syntax. (num * num for num in range(11)) Ideally we expect it to be a tuple comprehension, but it is a generator object. Generator expressions are a memory efficient way of creating a big list. We cannot index on the generator object, gives a TypeError . next() and for/each can be used to iterate over a generator. Thoughts My Github Url References. Generator Unpacking Why is there no tuple comprehension in Python? Link to tweet Day 33 | Monday July 22,2019 Days Progress The set and dict comprehensions are very similar in syntax. set comprehensions. {num * num for num in range(11)} dict comprehensions. {num: num * num for num in range(11)} Both set and dict comprehensions are not ordered. Thoughts My Github Url Link to tweet Day 32 | Friday July 19,2019 Days Progress Built-in function which works on the list sum() : returns the sum of a list min() : returns the smallest element in the list. max() : returns the biggest element in the list. sorted() : returns a sorted list. reverse=True as a default argument will reverse the list. Thoughts My Github Url Link to tweet Day 31 | Thursday July 18,2019 Days Progress Continued with List comprehensions. We can use conditionals in list comprehension to filter values. [num**2 for num in numbers if num %2 == 0] The conditional comes at the end. This conditional works as a filter. We have 3 ways to use List comprehensions. Map - All values are used Filter - Some values are used Map/Filter - Some values filtered and then modified. Thoughts My Github Url Link to tweet Day 30 | Tuesday July16,2019 Days Progress There are various type conversion which we can use. split() : convert string to a list. join() : convert a list to string. \",\".join(my_list) tuple unpacking can also be used when using .split() Learned about list comprehensions. [num**2 for num in numbers] : list of square of numbers. We right the square brackets first Followed by the for loop. The variable used in the for loop is available to the left of loop, num Apply mapping on the variable left of for loop. We can combine with tuple and f strings to get more valuable information. Thoughts My Github Url Link to tweet Day 29 | Saturday July 13,2019 Days Progress Worked on the Github Api Project using these concepts. requests exceptions APIs Query Parameters. Thoughts My Github Url Link to tweet Day 28 | Friday July 12,2019 Days Progress learned mostly about requests library. 4 HTTP methods GET POST PUT DELETE HTTP code 1XX : Information 2XX : Success 3XX : Redirection 4XX : Client error 5XX : Server error Thoughts My Github Url Link to tweet Day 27 | Thursday July 11,2019 Days Progress __name__ is a nice way to put code when we want to execute as a script. The above code is not invoked when called as a library. try/except is a great way to catch error's in code, and give alternate execution path. Thoughts My Github Url Link to tweet Day 26 | Wednesday July 10,2019 Days Progress Mostly learned about python file's and debugging techniques. Thoughts My Github Url Link to tweet Day 25 | Tuesday July 9,2019 Days Progress while loop is used to iterate over numbers in place of sequence. Also when we are not sure about the no of iteration break : It stops the execution of loop, and jumps to end of loop. If present in nested loop, it breaks the loop it is present in. continue : It stops the loop execution and jump to start of the loop. return : It also return the loop control to outside of the loop. Thoughts My Github Url Link to tweet Day 24 | Friday July 5,2019 Days Progress Today's learning was mostly on the Control flow. We have different variant of if condition. if if-else if-elif-else We can also check truthiness. Thoughts My Github Url Link to tweet Day 23 | Thursday July 4,2019 Days Progress Continuing the process of learning loop, understood about looping with dict There are mainly 3 looping with dict .key() - this is default, so do not have to specify in for loop. .items() - returns a tuple of key, value pair. .values() - returns only the values in dict . .enumerate(.items()) - gives, index and a tuple of key value. Thoughts My Github Url Link to tweet Day 22 | Wednesday July 3,2019 Days Progress Mostly improved the understanding of loops with list and range function. for loops create a temporary variable which is in scope outside of for loop. range() is a good function for looping, it has 3 variant. range(5) : default, creates the number from 0 to 4 range(1,5) : start and end index, creates the number from 1 to 4, 5 non inclusive. range(1,5,2) : the 3rd argument is steps , will step those many numbers. It does not take any Keyword arguments. Thoughts My Github Url Link to tweet Day 21 | Monday July 1,2019 Days Progress I have reached that day, where it is said, 21 days is required minimum to form a habit. Hope this habit stays with me. Learning continued on and , or , and not operator. It was an eye opener. and and or returns the value of one of the expression and not True or False and It returns the value of 2nd operand if the first operand is True else value of first. or It returns the value of 1st operand if it evaluates to True , else value of second. not Returns the inverse of the operator. Thoughts My Github Url Link to tweet Day 20 | Sunday June 30,2019 Days Progress Comparison operators in Python. < : Less than > : greater than <= : less than equal to >= : greater than equal to == : equal to != : Not equal to is : Identity, when both object points to same. Strings are compared based on their ASCII value. The capital letters are smaller than small letters. Thoughts My Github Url Link to tweet Day 19 | Friday June 28,2019 Days Progress What are the truthiness of various data types. integers: 0 is False any other number is True Collections: Empty list, tuple, Dictionary, sets are False Non Empty collections are True Strings: Empty String is False Non Empty String are True . None is False Thoughts My Github Url Link to tweet Day 18 | Wednesday June 26,2019 Days Progress Learned about the list slice. my_list[0:3 : Returns 0 - 2nd index my_list[:] : clone the entire list. my_list[-1] : Special way to get the last item. Thoughts My Github Url Link to tweet Day 17 | Tuesday June 25,2019 Days Progress Finally crossed the last attempts days. Learned about mutability. Basic data type are immutable. int , float , decimal , str , bool Containers data type are divided list , set , dict are mutable tuple is immutable. Thoughts My Github Url Link to tweet Day 16 | Monday June 24,2019 Days Progress Adding/Accessing dictionary elements. add new key/Value pair. nums[\"four\"] = 4 There are no duplicate key in Dictionaries. If new value is assigned to same key, it will override the old value. Existence of a key in Dictionaries. \"one\" in nums .update() : Combine two list. 3 important functions on Dictionaries .keys() : returns special list called dict keys .values() : returns a special list called dict values .item() : returns a list of tuple, called dict items Thoughts My Github Url Link to tweet Day 15 | Sunday June 23,2019 Days Progress Started learning about dictionaries. Dictionaries store key:value pair. Dictionaries are mutable but the keys are immutable. The search is very fast, just like sets . Retrieve the value with index as the key a[\"one\"] get() method can be used when we do not want an error while retrieving a value. Its returns a default value if the key is not present. Thoughts My Github Url Link to tweet Day 14 | Saturday June 22,2019 Days Progress Important set operation. union() or | : Returns the union of two sets. intersection() or & : Returns the intersection of two sets. difference() or - : present in 1 set but not in other. Thoughts My Github Url Link to tweet Day 13 | Thursday June 20,2019 Days Progress Updated my RAM with set operation for CRUD. add() - Adds item to the set. discard() - Removes item from the set, if not present, gives no error. remove() - Removes item from the set, if not present. gives KeyError . update() - Adds item from a sequence into a set. Thoughts My Github Url Link to tweet Day 12 | Wednesday June 19,2019 Days Progress Understood the basic premise around sets. Empty sets can only be created using set() function, empty {} creates a dict. Sets stores only immutable data type which can give a hash() value. a = {\"a\", (1, 2, 3), [1, 2, 3]} # TypeError: unhashable type: 'list' Sets are used to remove duplicates from List. Sets searching is very fast. Sets do not have a indexing order. Thoughts My Github Url Link to tweet Day 11 | Tuesday June 18,2019 Days Progress Explored the different ways to create a tuple Create a empty tuple. a = tuple() b = () Create a single item tuple. a = (1,) and not a = (1) Brackets are not mandatory for tuple. b = 1, 2, 3, 4, 5 indexing in tuple, same as list. b[0] Thoughts My Github Url Link to tweet Day 10 | Monday June 17,2019 Days Progress Established some basic understanding on list operations Operations to add item to the list. append() : add an item to end of the list. insert(2, \"bbbb\") : insert at an index. extend() : concatenates two list. Operation on list look-up, which is very slow, almost linear. index(value) : returns first index of value ValueError: if the value is not present in list. count(value) : returns the no of times a value is present. return's 0 is the value is not present. Operation to remove item from list. remove(value) : removes the value from the list, if not present does not throw error. pop() : remove and returns the last element of the list or the index. List are heterogeneous. Thoughts My Github Url Link to tweet Day 09 | Sunday June 16,2019 Days Progress Understood the list's sort and the built-in sorted function. sorted() returns a list list.sort() or list.reverse() returns None Thoughts My Github Url Link to tweet Day 08 | Saturday June 15,2019 Days Progress Started after a gap of 1 cheat day. List was the focus today. list can be created using [] or list() list is a ordered collection. list is a heterogeneous collection. list elements can be accessed using index start at 0 List has 1 efficient way of declaring. names = [ \"XXX\" , \"YYY\" , \"ZZZ\" , # unlike json, we can have comma at the last element, # it helps with git diff ] Thoughts My Github Url Link to tweet Day 07 | Thursday June 13,2019 Days Progress Function scope is little confusing without practice. There is a global scope and a local scope to a function. If same variable name is same, local scope gets preference. Global variable cannot be modified even thought it share the same name. This code will work. But this code will not work, gives UnboundLocalError: . The explanation is mentioned in the Python Documentation . Thoughts My Github Url Link to tweet Day 06 | Wednesday June 12,2019 Days Progress List or any other mutable data type should not be used as the default arguments. The list is initialized when the function is called the 1st time, and then it modifies the same list. Thoughts My Github Url Link to tweet Day 5 | Tuesday June 11,2019 Days Progress Started with the Function's Arguments . Positional arguments must be passed to functions. Default arguments are always provide at the end of function's argument list. We can give none , one , all arguments to a function with only default arguments list. Labeled arguments can be passed in any order to a function. Thoughts My Github Url Link to tweet Day 04 | Monday June 10,2019 Days Progress Started the functions section of the lecture. This lesson, teaches about different function type. Function with no arguments and no return type Function with no arguments and a return type Function with 2 arguments and a return type Function with multi-line function body. return is always optional in function, it returns <class 'NoneType'> Thoughts The function returning <class 'NoneType'> was a eye opener. My Github Url Link to tweet Day 03 | Sunday June 9,2019 Days Progress Completed the Data Type chapter in the FrontEnd Master's Python fundamentals. This chapter introduces some nice concept about data types 45j is a complex data type <class 'complex'> but not 45i , so j is the identifier for complex number. new_name f\"Hello, {name}\" is a format string, name in {} is the variable name which will be replaced. Same variable can be used to store number, or strings. String can be created by both 'string 1 ' or \" String 2 \" Integer division gives the result in floating point 3/2 = 1.5 Thoughts Python Data types have lot of power inside them with very less ambiguity. My Github Url Link to tweet Day 02 | Saturday June 8,2019 Days Progress Understood the VSCode basic settings, got help from a great tutorial by Corey Schafer The tutorials talks about these topics Change the way settings is displayed as JSON, in place of UI. Select virtual environment. Change color theme. Change file icons. Set the global python path - \"python.pythonPath\": \"<Path>\" Set a global python file formatter, we are using Black for this. \"python.formatting.provider\": \"black\", Also change the option to run the formatter on saving the file. \"editor.formatOnSave\": true, Enable Linting. Git Integration. Unit Testing. Thoughts VScode has lot of power, lets see how much I learn from it. My Github Url Link to tweet Day 01 | Friday June 7,2019 Days Progress Started the Python Fundamentals course by Nina Zakharenko. Today's main focus was setting my these things. Virtual environment. VScode setup. Faced few issues, which stackoverflow helped in solving Issues and Solutions There was no activate script when the virtual environment was created by using the command python -m venv .env On doing Google for the problem found that running the same command again solves the issue, so ran python -m venv env again and viola the activate script appeared. VScode was not recognizing the virtual environment created inside a sub folder in the project. Deleted the pre-existing environment and created a new virtual environment at the project root. Thoughts Programming is just efficient Google technique. My Github Url Link to tweet Reference QuoteFancy | Image Source 100DaysOfCode Official Website Learn Python Track from Team TreeHouse MIT 6.00SC Introduction to Computer Science and Programming Create a Tweet With image Preview for Free FrontEndMasters | Python Fundamentals | Nina Zakharenko Corey Schafer","tags":"#100DaysOfCode, python","url":"https://www.archerimagine.com/articles/100daysofcode-python/100Days-of-code-log-file_V_3_0.html","loc":"https://www.archerimagine.com/articles/100daysofcode-python/100Days-of-code-log-file_V_3_0.html"},{"title":"100 Days of Discrete Maths.","text":"Discrete Maths is a study of things which are discrete, which means things which can be counted. Discrete Maths forms the basis of a lot of concepts in algorithms and Computer science in general. I am starting this #100DaysOfX which Discrete Maths, to have a sufficient understanding of the concepts. I am using resources from NPTEL, MIT OCW, and ArsDigita University. The plan is to follow the 3 lectures mentioned in the references and the Rosen book on Discrete math. Day 16 | Thursday 21 March 2019 Days Progress Just a revision of implication, exclusive OR. Thoughts link to tweet Day 15 | Wednesday 20 March 2019 Days Progress Completed the logical inference lecture from NPTEL's Discrete Maths Thoughts Learned about:- logical inference for propositional calculus Fallacy logical inference for Quantifiers Normal Forms CNF DNF link to tweet Day 14 | Tuesday 19 March 2019 Days Progress Revision of Preposition calculus from Rosen Book. Thoughts Books are easier to understand once, the concept is understood. link to tweet Day 13 | Monday 18 March 2019 Days Progress Revision of lecture 4 from NPTEL's Discrete Maths Thoughts Learned that Implication and Equivalence are not the same. Understood about logical relationship involving quantifiers. link to tweet Day 12 | Wednesday 13 March 2019 Days Progress Revision of lecture 4 from NPTEL's Discrete Maths Thoughts Now learned properly about Predicates and Quantifiers. Scope of Quantifiers. Valid , Satisfiable and unsatisfiable predicates. link to tweet Day 11 | Tuesday 12 March 2019 Days Progress Again listened to the 4th lecture of NPTEL's Discrete Maths Thoughts Learned about Logical Inference. link to tweet Day 10 | Monday 11 March 2019 Days Progress Listened to the 4th lecture of NPTEL's Discrete Maths Learned a little about logical inference. Thoughts Still confusion over Predicate and Quantifiers. link to tweet Day 09 | Thursday 28 February 2019 Days Progress Reading and listening to explanation on how to negate a quantifiers. Learning about scope of a quantifiers. Thoughts Still have doubts on these topics. link to tweet Day 08 | Wednesday 27 February 2019 Days Progress In between the 4th Lecture of NPTEL's Discrete Maths Thoughts Great lecture on predicate logic Learned about:- Valid Expression Satisfiable Expression Unsatisfiable Expression These addition video's also helped. Universal and Existential Quantifiers [Discrete Math 1] Predicate Logic and Negating Quantifiers link to tweet Day 07 | Tuesday 26 February 2019 Days Progress Listened to the 2nd lecture of MIT 6.042J YouTube play-list Thoughts This lecture discusses about Proof by Contradiction Introduces to the concept of Induction proof. link to tweet Day 06 | Monday 25 February 2019 Days Progress Completed the 3rd Lecture of NPTEL's Discrete Maths . Thoughts Today I learned about:- Predicate and Quantifiers. Predicate Predicate Logic n-ary predicate Quantifiers Universal Existential Binding Variables Logical equivalence involving quantifiers. link to tweet Day 05 | Friday 22 February 2019 Today was a rest day for Discrete Maths. Day 04 | Thursday 21 February 2019 Days Progress Listened to the 2nd lecture of MIT 6.042J YouTube play-list This lecture discusses about Proof by Contradiction Introduces to the concept of Induction proof. Thoughts Today was focused on listening to the lecture, so have not taken detailed notes. link to tweet Day 03 | Wednesday 20 February 2019 Days Progress Completed the 2nd Lecture of NPTEL's Discrete Maths . Thoughts Today I learned about:- Proving implication without drawing all possible rows of truth table. Proved that implication is not associative. Learned about logical identities. Simplified complex compound propositions. Conversion between English to logic and vice versa. Rules of inference Modus Ponens Modus Tollens link to tweet Day 02 | Tuesday 19 February 2019 Days Progress Complete the First lecture of MIT 6.042J YouTube play list Thoughts I have not taken any notes, but the lecture was mostly focused on methods of proof, propositions and connectives. link to tweet Day 01 | Monday 18 February 2019 Days Progress Complete the First lecture of NPTEL's YouTube play list . Thoughts This lecture covers these topics:- Logic Propositions Logical Connectives ( \\(\\&\\) , \\(|\\) , \\(\\sim\\) ) and its truth tables Implication. ( \\(\\Rightarrow\\) ) Equivalence. ( \\(\\Leftrightarrow\\) ) Tautology, Contradiction & Contingency. Logical Identities Understanding Equivalence and Implication was little tough. These 2 video's provided the additional help. Rachel's Discrete Math Course - Implications (Lecture 5) Propositional logic | first order predicate logic| Propositional calculus | gate | net - part 5 Link to Tweet Reference NPTEL | Computer Sc - Discrete Mathematical Structures | Prof. Kamala Krithivasan ArsDigita | Discrete Mathematics and Its Applications | Rosen | Shai Simonson MIT 6.042J | Mathematics for Computer Science, Fall 2010 | Tom Leighton, Marten van Dijk Amazon | Discrete Mathematics and Its Applications (SIE) | Kenneth Rosen Latex | Math Symbols Pelican and Math Equations","tags":"#100DaysOfDiscreteMath","url":"https://www.archerimagine.com/articles/100daysofdiscretemath/100Days-of-DiscreteMath-log-file.html","loc":"https://www.archerimagine.com/articles/100daysofdiscretemath/100Days-of-DiscreteMath-log-file.html"},{"title":"100Days of Vim Log File","text":"I am starting this new journey into the world of VIM, with a hope that this time the mistakes of my past attempts will be rectified. This journey's goal is pre-decided so that there is no deviation from the plan of learning VIM. In the last attempt of learning VIM, I was doing the classic mistakes of learning vim as described by Mr. Bram Moolenaar in this video, that is, Learn every feature the editor offers and use the most efficient command all the time. The approach this time will be learning a little bit of commands in VIM and apply it daily for a few days, as it becomes part of the muscle memory move to the next set. In addition, I have also set my goals for this 100 Days to have a razor sharp focus and not deviating. The goal of this 100DaysofVim are:- Edit text effectively. Scroll and move in a file quickly. Navigate source code with ctags and key board shortcuts. Edit multiple files using buffers. No use of any vim plugins. Read and understand the vim help system. Integrate debugging with source code navigation in VIM. Day 16 | Thursday 21 March 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help about Editing a file Saw 1 screen cast from VimCast | Episodes Thoughts Nothing significant progress today. link to tweet Day 15 | Wednesday 20 March 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help about Starting Vim Saw 1 screen cast from VimCast | Episodes Thoughts Understood about the default spell checking mechanism of VIM. Will post the key bindings soon. link to tweet Day 14 | Tuesday 19 March 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help about Special Ex characters Saw 1 screen cast from VimCast | Episodes Thoughts Most of the Special EX char, can be used with the :edit command The Vimcast from wrapping and this completely went over the head, will revisit again. link to tweet Day 13 | Monday 18 March 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help about Ranges Saw 1 screen cast from VimCast | Episodes Thoughts Understanding Wrapping is little difficult in VIM. link to tweet Day 12 | Wednesday 13 March 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help about Command-line editing Saw 1 screen cast from VimCast | Episodes Thoughts link to tweet Day 11 | Tuesday 12 March 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help about Various commands Saw 1 screen cast from VimCast | Episodes Thoughts Configured the netrw to behave like a File Explorer. Took help from these links Vim: you don't need NERDtree or (maybe) netrw Magic of netrw in Vim link to tweet Day 10 | Monday 11 March 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help about Quickfix commands Saw 1 screen cast from VimCast | Episodes Thoughts Quickfix commands We need a arguments called makeprg to be configured. C program's by default have make as the makeprg . Python program can configure makeprg as pyflakes We can even configure this pased on the ftplugin VimCast How to change directory while editing a file link to tweet Day 09 | Thursday 28 February 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Saw 1 screen cast from VimCast | Episodes Thoughts Learned about s : = xi : delete a char and insert mode S := &#94;C : delete line and insert mode. Read the Options help, did not understand and word. link to tweet Day 08 | Wednesday 27 February 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Saw 1 screen cast from VimCast | Episodes Thoughts Learned about g; and g, : Navigate the change list in forward and backward direction. CTRL + 0 , CTRL + I : Navigate the jump list in forward and backward direction. link to tweet Day 07 | Tuesday 26 February 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Saw 1 screen cast from VimCast | Episodes Thoughts Learned about session when reading the Key Mapping section of vim help. mksession hello.vim vim -S hello.vim link to tweet Day 06 | Monday 25 February 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Saw 1 screen cast from VimCast | Episodes Thoughts link to tweet Day 05 | Friday 22 February 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Saw 1 screen cast from VimCast | Episodes Completed the course Udemy | Vim MasterClass | Jason Cannon Received the course completion certificate. Thoughts Learned about the gVIM clipboard buffers \"+ and \"* Tab's command :tabe : open a tab with file name CTRL-W T : move current split into a tab. tabc : close current tab tabo[nly] : one 1 tab open. gT and gt : to switch between tabs. tabmove : move tabs Completed these help topics Visual Mode Text Object link to tweet Day 04 | Thursday 21 February 2019 Days Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Lecture on buffers from Udemy | Vim MasterClass | Jason Cannon Saw 1 screen cast from VimCast | Episodes Thoughts Learned about the various windows commands. CTRL +w s or :sp `: horizontal split CTRL +w v :vsp : vertical split :only : closes all window except the active one Navigation is done by CTRL + w w , CTRL + w h , CTRL + w j , CTRL + w k , CTRL + w l Resize windows CTRL + w + , CTRL + w - , increase or decrease the size by 1 line CTRL + W _ , CTRL + w | , increase size of current window in height and width Moving window is done by CTRL + w R , CTRL + w H , CTRL + w J , CTRL + w K , CTRL + w L like bufdo we have a command windo which works only on opened window. Studied the Complex Changes from vim helps, did not understood much from this. link to tweet Day 03 | Wednesday 20 February 2019 Today's Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Lecture on buffers from Udemy | Vim MasterClass | Jason Cannon Thoughts Learned about these buffers commands. :buffers or :ls :bn or :bnext :bp or :bprevious :bf or :bfirst :bl or :blast CTRL + &#94; : last open buffers :set hidden :qall! :wall! :badd :bd :bufdo :Explore Studied the Changing Text from Vim help. cc , S , C , s all work on line. CTRL + A and CTRL + X has a very nice implementation. :ce , :le & :ri changes the alignment of line, center, left and right. link to tweet Day 2 | Tuesday 19 February 2019 Today's Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Saw 1 screen cast from VimCast | Episodes Thoughts Studied the Copying and Moving text. section of vim help. _ behaves the same as &#94; without a count preceding it. When count is preceding it, this behaves as j . What does the underscore motion do in vim? While learning about vim help file, I found that all the commands which are similar are generally kept together. Link To Tweet Day 1 | Monday 18 February 2019 Today's Progress Complete the vimtutor exercise. Studied the quick reference in vim help. Thoughts Few commands which was very good. Using C to change the text from the cursor till end of line. Using D to delete the text from the cursor till end of line. Few Insert Mode commands. CTRL-T : insert one shiftwidth of indent in front of line. CTRL-D : deletes one shiftwidth of indent in front of line. Link to Tweet References How To Learn Vim: A Four Week Plan 100daysOfX YouTube | 7 Habits For Effective Text Editing 2.0 VimCast | Episodes","tags":"#100DaysOfVim","url":"https://www.archerimagine.com/articles/100daysofvim/100Days-of-vim-log-file-V-2-0.html","loc":"https://www.archerimagine.com/articles/100daysofvim/100Days-of-vim-log-file-V-2-0.html"},{"title":"100Days of Code Log File 2nd Attempt","text":"Hello World!, You are about the witness the beginning of an epic second coming of the 100-Day coding journey, A story that great sages will pass down from generation to generation. This quest will feature a potpourri of unfiltered joy, unrivaled pain, and unexpected epiphanies. Some moments, I will be the smartest man alive. Others moments, I will be a stupid idiot. But each day, I will be a valiant warrior, fighting to develop and perfect the skills necessary to evolve into a true beast with these keys. I have failed in my previous attempt for the challenge, which you can find here . There are learning from the previous failure, here are the modification which was done to the challenge according to my handicap. Selected the resource in advance, Learn Python Track from Team TreeHouse MIT 6.00SC Introduction to Computer Science and Programming Practice the 100Daysofcode for a month before committing fully. Create a time table and sticking to it. Studying at the same time everyday, brains craves for learning python in that hour of the day. Missing few (~5) days is acceptable. Ladies and gentleman, I present to you, #100DaysofCode with @ animeshkbhadra Day 16 | Thursday 21 March 2019 Days Progress Completed few regular expression tutorials from TreeHouse Thoughts Learned about:- Negation [&#94;abc] - A set that will not match any char of these characters a , b and c re.IGNORECASE , re.I - Flag to ignore case while searching re.VERBOSE , re.X - Flag that allows regular expression to span multiple lines. Groups ([abc]) - create a group, that contains a set of letters a , b and c (P<name>[abc]) - creates a named group, member can be accessed using group('name') re.MULTILINE , re.M - flag to make a pattern having lines. &#94; - Beginning of line. $ - End of line. link to tweet Day 15 | Wednesday 20 March 2019 Days Progress Today did not get much work done, solved few exercises in solo learn. Thoughts link to tweet Day 14 | Tuesday 19 March 2019 Days Progress Completed few regular expression tutorials from TreeHouse Thoughts Learned about counts in regular expressions. We can create expressions like these \\w{3} - Match any 3 word char \\w{,3} - Match 0,1,2 or 3 word char \\w{3,} - Match 3 or more char, no upper limit \\w{3,5} - Match 3,4 or 5 word char \\w? - Match 0 or 1 char \\w* - Match 0 or more \\w+ - Match 1 or more We can also pass a variable in regular expression string. \"\\w{%s}\" %count %s - for string %d - for decimal %f - for float Sets basic was also checked. [abc] - set of char a,b,c [a-z][A-Z][a-zA-Z] - Char ranges [0-9] - digit [&#94;2] - Not 2 link to tweet Day 13 | Monday 18 March 2019 Days Progress Started the Chapter 01 from Head First Python. Thoughts Learned about 2 types of import s. Learned about these module, datetime , random , time link to tweet Day 12 | Wednesday 13 March 2019 Days Progress Practiced some of the learning from Regular Expression from previous Day. Thoughts link to tweet Day 11 | Tuesday 12 March 2019 Days Progress Started the Module of Regular Expression from TreeHouse Thoughts Learned about File read and write operations like open() close() read() In addition to these explored re.match() - Matches against the beginning of text re.search() - returns the first match location anywhere in the text. Few escape sequence \\w - Matches Unicode word char including numbers but excluding special character. \\W - Matches anything which is NOT Unicode word or numbers \\s - Matches all white spaces \\S - Matches anything which is NOT white spaces \\d - Matches numbers \\D - Matches NOT numbers \\b - Matches word boundary. \\B - Matches NOT word boundary. link to tweet Day 10 | Monday 11 March 2019 Days Progress Read the first chapter of Head first Python. Wrote few code from the book. Thoughts Learned about the datetime module. link to tweet Day 09 | Tuesday 05 March 2019 Days Progress Completed the module of Quiz Game in the course. TreeHouse | Date and Times in Python Earned the Dates and Times badge Thoughts Learned about timezone. It is a very difficult to handle without pytz link to tweet Day 08 | Friday 01 March 2019 Days Progress Completed the module of Quiz Game in the course. TreeHouse | Date and Times in Python Earned the build timed quiz App badge. Thoughts Some concepts of games were very good. link to tweet Day 07 | Thursday 28 February 2019 Days Progress Started the course. TreeHouse | Date and Times in Python Received the badge Date and time Badge Thoughts Learned about strftime() and strptime() Made a script to create a link for wikipedia. link to tweet Day 06 | Wednesday 27 February 2019 Days Progress Today I just watched the second lecture from MIT OCW's MIT 6.00SC Introduction to Computer Science and Programming Thoughts In this lecture we discuss about:- Type of objects. Expression link to tweet Day 05 | Tuesday 26 February 2019 Days Progress There was a gap of 3 days. Started the Date and Time module. Thoughts This course teaches about the date and time module of Python. Major modules in datetime are date , datetime , time , timedelta , timezone , tzinfo - which is rarely used directly. Learned about .today() , .combine() , .timestamp() - which returns the epoch time. We can format the time with help from strftime() We can create time with string format strptime() link to tweet Day 04 | Friday 22 February 2019 Days Progress Completed the course TreeHouse | Write Better python Received the badge Clean Code Badge In addition completed the Write Better python Course of the Learn Python track. Thoughts This course taught about:- PEP-8 Coding style guide. PEP-20 which is the Zen of python, can be accessed using import this PEP-257 for docstrings DocString which can fit one line should. DocString that cannot, put the closing triple quote on their own line. Logging Module of python 6 Logs levels, CRITICAL , ERROR , WARNING , INFO , DEBUG , NOTSET Python Debugger pdb , can be invoked by calling import pdb; pdb.set_trace() link to tweet Day 03 | Thursday 21 February 2019 Days Progress I completed the Object Oriented Python course . With this I have also received the badge. Thoughts Today completed the project Dice Roller. I am still not confident in some part of Object Oriented Python, will soon polish it. link to tweet Day 02 | Wednesday 20 February 2019 Days Progress Today I just watched the first lecture from MIT OCW's MIT 6.00SC Introduction to Computer Science and Programming . This is a great lecture by John Guttag I am alternating between Learn Python Track from Team TreeHouse and MIT 6.00SC Introduction to Computer Science and Programming Thoughts Lecture 1 of MIT 6.00SC Introduction to Computer Science and Programming taught me about:- Declarative and Imperative Knowledge. Algorithms Fixed program and stored program computers Programming Language Syntax Static Semantics Semantics. Types of errors Compiled Vs Interpreted Language Link To Tweet Day 01 | Tuesday 19 February 2019 Days Progress I am learning Python from the learn python track of TreeHouse . I started this course, some days back, from now, will post regularly on behalf of #100DaysOfCode. Today I have completed the Advanced Object Badge. Thoughts Today I learned about:- @property : This decorator is used to convert a class method into a class property. @property.setter : This decorator is used to make the method set a class property. @classmethod : This is a decorator which takes a function as input works on it and returns another function as output. Link to Tweet Reference QuoteFancy | Image Source 100DaysOfCode Official Website Learn Python Track from Team TreeHouse MIT 6.00SC Introduction to Computer Science and Programming Create a Tweet With image Preview for Free","tags":"#100DaysOfCode, python","url":"https://www.archerimagine.com/articles/100daysofcode-python/100Days-of-code-log-file_V_2_0.html","loc":"https://www.archerimagine.com/articles/100daysofcode-python/100Days-of-code-log-file_V_2_0.html"},{"title":"Render Maths Equation with Pelican Blog","text":"De Morgan's Law De Morgan's Law (negation of conjunction) $$ \\neg(P \\wedge Q) \\equiv \\neg P \\vee \\neg Q $$ De Morgan's Law (negation of alternative) $$ \\neg(P \\vee Q) \\equiv \\neg P \\wedge \\neg Q $$ First Header Second Header Content Cell Content Cell Content Cell Content Cell","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/Render-Maths-pelican-blog.html","loc":"https://www.archerimagine.com/articles/pelican/Render-Maths-pelican-blog.html"},{"title":"Study Notes for Programming in C","text":"Introduction C is not only a \"System programming language\" but has a wide variety of use in other domain. Fundamental Data Types Characters char Integers int , short , long Floating point double , float Derived Data Types Pointers Pointers provide for a machine-independent address arithmetic. Arrays Structure Unions. Expressions Expressions are formed from Operator and Operands . Any expression including an assignment or a function call can be a statement . Functions Functions performs a single set of operation within a block of code. Functions may return values of Fundamental data types. Derived Data types. except arrays Pointers Structure Unions Variables Variables can be internal to a function. External but known only within a single source file. Visible to the entire program. This is frowned upon and we should rarely use. Preprocessing Preprocessing step performs macro substitution on program text, inclusion of other source file or conditional compilation. UseCase not provided in C Operations directly dealing with Composite Objects, like Array, Structure, Unions No Storage other than static and auto matic. No Input/Output facilities. No built in file Access. The above facilities are included by help of standard library defined by the ANSI C Standard . ANSI C Standard New syntax for declaring and defining functions. Definition of a standard library. C is not a strongly typed language. C frowns on but permits the interchange of pointer and integers which has been eliminated by ANSI. No Automatic conversion of incompatible data types. Reference Image Source","tags":"C Programming","url":"https://www.archerimagine.com/articles/c-programming/StudyNotes-Programming-in-C.html","loc":"https://www.archerimagine.com/articles/c-programming/StudyNotes-Programming-in-C.html"},{"title":"100Days of Vim Log File","text":"What is the definition of a Real Programmer is open for interpretation. I have been using VIM for last couple of years to do the basic editing and was happy with whatever I learn about using vim in the process. Deep within I always wanted to become a better user of vim, tried different experimentation which includes installing mindless plugins, copying from different vimrc files sometimes understanding and sometime not understanding the concepts. Moral of the story is nothing sticked to my brain or what we call muscle memory to use it efficiently. Fortunately enough when I was struggling with the idea to become better at vim, this article How To Learn Vim: A Four Week Plan came along and I was immediately interested, clubbed this with the 100daysofx and I had a plan at becoming better at VIM. Through this log file, I will update what ever resource I will use and what ever practice I am doing everyday. God permits if I become good at vim in these 100 days it will be an achievement for me. Hoping for the best I hereby jump into the world of Real Programmers. Day -4: Saturday January 6,2018 Today's Progress Completed the vimtutor exercise in ~11 minutes. Learned about these keystrokes. i : insert before the cursor. x : delete the character under the cursor. h : Left Movement. (Left most Key) j : Down Movement. (Anchor like j ) k : Up Movement. l : Right Movement. (Right most key) Thoughts In my first attempt while learning vimtutor some years ago, I was not able to understand that vimtutor is a separate command in shell. From that beginning to today where I am comfortable in using vimtutor , surely have come a long way. Link to Tweet Day -5: Friday January 5,2018 Today's Progress Completed the vimtutor exercise in ~11 minutes. Learned about these keystrokes. u : undo just one change. U : Undo changes in one line. Ctrl + r : Redo the last change. :s/searchforstring/replacewithSting/g : Search and replace in one line. Thoughts vimtutor is helping me gain the confidence on using VIM. Now I do not feel threatened by VIM. Link to Tweet #100DaysOfVim Day-04 Tweet Day -6: Thursday January 4,2018 Today's Progress Completed the vimtutor exercise in ~14 minutes. Learned about these keystrokes. w : jump a word, not special char e : jump to end of word. $ : jump to end of line. 0 : jump to first column of the line. Thoughts Just vimtutor teaches so many basic key strokes to become a better beginner, everyone thinking of learning vim should try this. Link to Tweet #100DaysOfVim Day-03 Tweet Day -7: Wednesday January 3,2018 Today's Progress Completed the vimtutor exercise in ~14 minutes. Learned about these keystrokes. a : Append at the next cursor point. A : Append at the end of line. u : undo a single instruction. U : undo a complete line. Thoughts Looking at the key combination of VIM, I feel like each characters have a similar story. Link to Tweet #100DaysOfVim Day-02 Tweet Day -8: Wednesday January 2,2018 Today's Progress Completed the vimtutor exercise in ~20 minutes. The idea is to, do this for next 7 days as suggested in one blog link. Thoughts I have been using vim for a long time, but never took the pain to master it. I am starting this challenge with an idea that I will be able to move to intermediate level vim. Link To Tweet #100DaysOfVim Day-01 Tweet References How To Learn Vim: A Four Week Plan 100daysofx Real Programmers xkcd","tags":"#100DaysOfVim","url":"https://www.archerimagine.com/articles/100daysofvim/100Days-of-vim-log-file.html","loc":"https://www.archerimagine.com/articles/100daysofvim/100Days-of-vim-log-file.html"},{"title":"100Days of Code Log File","text":"Hello World! , You are about the witness the beginning of an epic 100-Day coding journey, A story that great sages will pass down from generation to generation. This quest will feature a potpourri of unfettered joy, unrivaled pain, and unexpected epiphanies. Some moments, I will be the smartest man alive. Others moments, I will be a stupid idiot. But each day, I will be a valiant warrior, fighting to develop and perfect the skills necessary to evolve into a true beast with these keys. Ladies and gentleman, I present to you, #100DaysofCode with @ animeshkbhadra Day -8: Wednesday January 2,2018 Today's Progress Completed the vimtutor exercise in ~20 mins. The idea is to do this for next 7 days as suggested in one blog link. Thoughts Reference","tags":"#100DaysOfCode","url":"https://www.archerimagine.com/articles/100daysofcode/100Days-of-code-log-file.html","loc":"https://www.archerimagine.com/articles/100daysofcode/100Days-of-code-log-file.html"},{"title":"Tips to improve work flow in pelican blog.","text":"We all know a lot of meta-data is required for writing a pelican blog, we can use Sublime Text to improve this meta-data collection, and also see some useful commands to improve the output. FileHeader to Write Meta-Data in Sublime Text Sublime Text is a very nice Text editor for writing Markdown. There are already a lot of articles on how to configure Sublime text for Markdown. We will discuss about one specific package in Sublime Text called FileHeader , this package helps in writing custom File Header, so we can use this package to provide some Meta-Data to the pelican blog by default. FileHeader comes with predefined header template, if we want to change the content of these templates we can use a custom fileHeader template. Since we are writing our content in markdown, I have extended the default Markdown template. The modified Markdown Template have to saved in this path /home/username/.config/sublime-text-2/Packages/User/fileHeaderTemplatesUser named as Markdown.tmpl . FileHeader uses Jinja2 template, the Markdown template looks like this . Title : Date : {{ create_time }} Modified : {{ last_modified_time }} Category : Pelican Tags : pelican Slug : {{ file_name_without_extension }} Author : {{ author }} subtitle : Summary : keywords : [ TOC ] We have to give some configuration for it to work. Kindly add this in the FileHeader.sublime-settings . { \"custom_template_header_path\" : \"/home/username/.config/sublime-text-2/Packages/User/fileHeaderTemplatesUser\" , \"Default\" : { \"author\" : \"ABC\" } } After this every time you create a new Markdown file, the above template will be automatically applied. Clean the Output Directory We are working with two git repositories when writing a pelican blog. root This is the folder which has the content and all the setting file. output This is the actual HTML page generated by Pelican. Most often than not we might want to generate the complete blog with a clean build with commands like this. pelican content -ds publishconf.py # While Publishing pelican content -d # Local host. The problem with these above commands is they might delete the .git or even the CNAME directory inside output losing the link with version control. We can prevent this by adding this configuration into our pelicanconf.py OUTPUT_RETENTION = [ \".hg\" , \".git\" , \"CNAME\" ] This change will make sure that the above mentioned file are not deleted. Kindly keep in mind this works for only the above 2 version of the command, if you use make clean then this configuration is of no use. Reference clean should not remove .git metadata #574","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/tips-for-improving-workflow-in-pelican.html","loc":"https://www.archerimagine.com/articles/pelican/tips-for-improving-workflow-in-pelican.html"},{"title":"Integrating 3rd party services with Pelican Blog.","text":"We have already discussed two types of settings file in pelican , and we found that publishconf.py is the file which is picked along with pelicanconf.py when we are generating the blog for publishing. These 3rd party service integration happen over this publishconf.py file, because we do not want these services to be activated when running on localhost . Google Analytics. Google Analytics provide us with very valuable insights into how a user interacts with the website, like the links which users click the most etc. This is a free service which really helps when we are starting out. When we create an account with Google Analytics , we receive a tracking code which we need to provide to Pelican blog. This tracking code is generally of the form UA-********-* , which you can find in this menu flow Administration ---> Property Settings ---> Tracking Id , copy this tracking id and provide it to publishconf.py with this settings GOOGLE_ANALYTICS = \"UA-********-*\" This is it, we will get all the analytics data from our website on Google Analytics. DISQUS DISQUS is a commenting system which is used extensively in the pelican blog world as we have to engage with our visitors for providing feedback and anything which is deemed important to the visitors. Integrating Disqus with any blog requires a ShortName , which is a unique identifier for our site, which can be found in this path https://<username>.disqus.com/admin/settings/general/ . We need to add this setting into the publishconf.py . DISQUS_SITENAME = \"shortName\" The Elegant Theme provides a nice feature called Collapsible Comments in which we will not show all the comments when the page loads. This comments will only be shown when a user presses the comment link. We can use a COMMENTS_INTRO settings to draw the user for engaging with the site. MailChimp MailChimp is a way to provide newsletters to your active subscribers, in this way they can be informed for any new post also immediately. Mailchimp can also be used as an email marketing platform. This is also free for basic subscription. We will need a MAILCHIMP_FORM_ACTION , URL which we can get by creating one list in MailChimp. To create a mailing newsletter we can use these links MailChimp Create a New List Add a Signup Form to Your Website Reference What's a shortname? Host Your Own Signup Forms","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/integrating-3rd-party-services-with-pelican.html","loc":"https://www.archerimagine.com/articles/pelican/integrating-3rd-party-services-with-pelican.html"},{"title":"Home Page feature for Elegant Theme","text":"Elegant has a nice feature called the Home Page, which provides a About Me combined with information from GitHub on the project someone is working. These feature are again controlled by a configuration. In place of adding this configuration into pelicanconf.py we will create a new file called elegantconfig.py , and import this file into pelicanconf.py by calling from elegantConfig import * . This will make sure all the configuration from elegantconfig.py is available while generating the blog. There are two configuration for the Home Page. LANDING_PAGE_ABOUT LANDING_PAGE_ABOUT This is a Dictionary with two field \"title\" and \"details\" \"title\" : This gives a heading Home Page. \"details\" : It is the content in HTML which mostly contains the About Me description which one wants to provide. One clever way of writing this HTML is, write a about page in markdown, generate the pelican blog with this about page and then right click on the page and select view source, copy the relevant text with the HTML code in it. PROJECTS PROJECTS This is also a list of Dictionary with these field in each dictionary, and the value of each of these field is a string and not HTML in case of LANDING_PAGE_ABOUT 's \"details\" \"name\" : The project name which you want to be displayed. \"url\" : The link to the project. \"description\" : The description you want to provide for the project.","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/home-page-features-elegant-theme.html","loc":"https://www.archerimagine.com/articles/pelican/home-page-features-elegant-theme.html"},{"title":"Problem Faced when integrating with Elegant Themes.","text":"Elegant was my theme choice, and while integrating this theme with my blog I found that a lot of things does not work out of the box, and some changes are required in the themes itself. I am listing these changes here, if this are still unresolved in the future. Elegant , requires a lot of different plugins to make it work the way it was designed. If you see the documentation on the above link, it does not mention the important plugins which are required to make it work, it just provides an explanation on the feature. In this blog post I am listing down the issues which I faced while integrating the Elegant themes. favicon.ico not displayed with Elegant If you read the documentation on how to enable favicon.ico for Elegant themes, the process is pretty straight forward, just place the icons into this directory content/theme/images , and define STATIC_PATHS with STATIC_PATHS = ['theme/images', 'images'] . If you follow the above process then most probably you will still not be getting the favicon.ico , the reason being there is one more configuration which needs to enabled in pelicanconf.py in addition to above two. STATIC_PATHS = [ 'theme/images' , 'images' ] USE_SHORTCUT_ICONS = True The reason for this is, the links for including favicon are generated in this template themes/elegant/templates/_includes/favicon_links.html , and the link generation is under a configuration named USE_SHORTCUT_ICONS , so until we make it True the links will not be generated. Issues while integrating Tipue Search Prerequisite Tipue Search plugin integration is little different from other plugin. This plugin has an external dependency on BeautifulSoup , we have to install this python package first in our environment pelican1 pip install beautifulsoup4 We have to add this plugin name into pelicanconf.py PLUGINS = [ 'sitemap' , 'tipue_search' ] When we build our blog after this we should see our search functionality working, but we see 2 issues. Search Result are not displayed. Tipue search return undefined URL. Search Result are not displayed Elegant uses some predefined HTML pages to render few of its content, search functionality is based on one such file, so we have to provide that in the pelicanconf.py DIRECT_TEMPLATES = (( 'index' , 'tags' , 'categories' , 'archives' , 'search' , '404' )) When we do this changes we will see the search result getting listed, but on clicking those link it will redirect to an undefined URL. Tipue search return undefined URL. When we click on the Search result, we get the URL as undefined, this issues is still not solved, there is already a pull request pending on the Github. Kindly visit these two links for more details. * Tipue search return undefined url . * To solve the issue, we have to modify the plugin manually as mentioned in the pull request - Change the file in plugins/tipue_search/tipue_search.py , line no 61 add this code 'loc': page_url TOC Integration with Elegant Elegant theme has a side bar with the Table of Content of the blog post displayed. This is also achieved based on a plugin named extract_toc . We have to add this into the pelicanconf.py PLUGINS = [ 'sitemap' , 'tipue_search' , 'extract_toc' ] There is also a Markdown settings which we have to update in the same file like this. MARKDOWN = { 'extension_configs' : { 'markdown.extensions.toc' :{ 'permalink' : 'true' } } } In addition to the above changes, every blog post after the file meta-data section should have an entry named [TOC] Syntax Highlighting When writing a technical blog we might be interested in syntax highlighting of the code we write, we can achieve this with following configuration in pelicanconf.py MARKDOWN = { 'extension_configs' : { 'markdown.extensions.codehilite' : { 'css_class' : 'highlight' }, 'markdown.extensions.extra' : {}, 'markdown.extensions.meta' : {}, 'markdown.extensions.toc' :{ 'permalink' : 'true' }, }, 'output_format' : 'html5' , } Next and Previous Articles. When we read the documentation of Elegant Next and Previous Articles , it clearly states that we do not require any additional plugins for this feature, but it does not work out of the box. We have to integrate the neighbors plugins and then it works. Now we will have these in our pelicanconf.py PLUGINS = [ 'sitemap' , 'tipue_search' , 'extract_toc' , 'neighbors' ] Missing icons for social links Pelican supports a way to integrate most of the social Website like Twitter , Facebook , Github etc. This is done by the help of this settings in pelicanconf.py SOCIAL = (( 'github' , 'URL to your profile' ), ( 'linkedin-square' , 'URL to your profile' ), ( 'facebook' , 'URL to your profile' ), ( 'quora' , 'URL to your profile' ), ( 'reddit' , 'URL to your profile' ), ( 'twitter' , 'URL to your profile' ) ) This explains that we have a tuple of tuple in the settings name SOCIAL , if you see each website name is in a particular format, as explained in this link , this format is taken from Font Awesome icon for social links . When naming the website name in SOCIAL configuration, keep in mind the way the website in named in Font Awesome and remove the fa- part of the name. References Elegant Kindly read this documentation for other configuration which gives more flexibility in terms of the themes like Article Subtitle, Add License to your Site etc. pelicanconf of oncrashreboot blog Visit this configuration file for any doubts on the setting of elegant theme. Favicon documentation Tipue Search plugin Tipue search return undefined url . Tipue search return undefined url pull request Missing icons for social links Font Awesome icon for social links","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/integration-problem-with-elegant-theme.html","loc":"https://www.archerimagine.com/articles/pelican/integration-problem-with-elegant-theme.html"},{"title":"Expressing the content with Pelican Themes.","text":"We have already discussed about Pelican Themes and Plugins in this blog . We have also seen the comparison between the various popular themes. Elegant is our choice of Themes because of search functionality which it provides along with the minimalist concepts. We will first focus on how we can integrate one particular themes and also one particular plugins which will give us a fair amount of idea on how to integrate different plugins. Integrating the Elegant Theme We have already created folder named plugins and themes , which are clone of the Pelican Plugins and Themes repository. If we see the directory listing inside themes folder we fill find different folders with distinguished names, these are the themes name. To use any of the themes in this folder we have to add this pelican settings in the pelicanconf.py . THEME = 'themes/elegant' Follow the commands to generate the site and launch the site, and you have your new themes applied. pelican content cd output/ python -m pelican.server Integrating A Plugin into Pelican The process to integrate any plugin is also similar to integrating Themes. If we see inside the plugins directory which we had cloned, we will find a lot of different folder name just like in themes directory. Each of these name is a plugin name. To integrate a plugin into Pelican we have to add these 2 configuration into the pelicanconf.py file. PLUGIN_PATHS = [ 'plugin' ] # Name of the directory where plugin are kept. PLUGINS = [ 'sitemap' ] # Name of the particular plugin inside the directory. In the above code sample, we can see we have integrated the sitemap plugin, and as per the documentation , this generates a Sitemap which we generally submit it to some Webmaster tools. Likewise, if we want to integrate any other plugins , we have just add it to the list variable PLUGINS along with the settings required for that plugins defined in its documentation. References Elegant Pelican Plugins Pelican Themes Sitemap Documentation ,","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/expressing-with-pelican-themes.html","loc":"https://www.archerimagine.com/articles/pelican/expressing-with-pelican-themes.html"},{"title":"Customizing Pelican blog with the help of Plugin and themes","text":"Pelican Blogs gives its user the full power to customize to the want of the user. We have been using the default themes and default setting provided by Pelican with not much customization and the results are also quite good. Enhancing the present blog further will require us to use certain Plugins and Themes which will extended the functionality. These are the 2 repository which we should clone into for getting these enhancements. pelican-plugins This is the repository for the plugins. git clone --recursive https://github.com/getpelican/pelican-plugins.git plugins pelican-themes This is the themes repository. git clone --recursive https://github.com/getpelican/pelican-themes.git themes Plugins Vs Themes The first things which we have to decide is to choose the list of plugins or the Themes we want to use. I would say first decide on Themes and the see what all plugins are required to support these themes. All plugins are not plug able with all the themes. We have to first start by choosing the themes, and the corresponding plugin required for it. This path is again the easiest as we are already treading the known, when we are comfortable with this integration we can always go ahead and enhance the existing themes and plugins to suit our needs. Till we reach the Jedi stage this precooked solution is the best approach. Flex Vs Elegant vs BootStrap3 When we are deciding on which themes to choose for the blog, broadly the working choice would be Flex: Responsive Pelican theme Elegant Why it is the best Pelican theme BootStrap3 There are many others, which you are free to explore, but for my blog I had considered these 3 options. Each one has its own pros and cons, the choice was very difficult between these 3, and in future I might even consider jumping ships to the other themes. Let's discuss some outline of the above 3 themes Flex: Responsive Pelican theme This has out of the box integration with a lot of plugins including AddThis , which is not available for any other themes. This also has support for Google AdSense, which again is missing in most of the plugin. Actively maintained, the last check-in on its Github repository was on Apr 24, 2017. Tipue_Search is also on the way, there is an open issue on the repository, Search #49 This facility of search is the only reason I dropped Flex . BootStrap3 This is a full hack able implementation of BootStrap3, and will try to use these Themes, once I get my hand dirty enough with the modern CSS and Web Technologies. Elegant Why it is the best Pelican theme I chose this theme just for its minimalist design and search feature. It has an integration with MailChimp. This is not an actively managed project, the last commit on this repository was on Sep 8, 2014, which is good 3 years ago. We can use and modify this theme for our satisfaction if a bug is hampering our development. We have decided on our Themes, the Plugins required for these themes are as below. sitemap tipue_search extract_toc neighbors The standard features and enhancements already available Disqus Google Analytics MailChimp Integration. Custom 404 Page. Collapsible Comments Page and Article Subtitle. We will not try to see the integration of all the above feature with the Elegant themes. Reference Elegant Why it is the best Pelican theme BootStrap3 Flex: Responsive Pelican theme pelican-themes pelican-plugins","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/customizing-pelican-blog-with-plugin-and-themes.html","loc":"https://www.archerimagine.com/articles/pelican/customizing-pelican-blog-with-plugin-and-themes.html"},{"title":"Configuring Github pages with Custom Domain","text":"In the first series of blog post we have seen how to create a pelican blog, customize a little and also host it from Github pages. We can also use Github pages as a hosting service and link any of the domain name providers with this hosting. In this post I have taken the example of GoDaddy, but the process should not much different for any other domain name provider. The process to link a Godaddy Domain to Github Pages can be divided into 2 Steps. Configure your Github repository Configure the DNS at GoDaddy Configure Github Repository We have already made a Github Pages website in our previous post, kindly follow the steps mentioned here . Once we have a Github Pages URL, we have to configure a CNAME in this repository, this can be done in 2 ways. Local Repository Directly on Github. Local Repository Create a local file in the repository with the name CNAME. Just have one line in the file. example.com , where example.com is your domain you have bought from GoDaddy . Push the changes to Github Directly on Github Pages. On the repository in Github , you will see something like this. In the above Click on the Settings , Scroll down you will see something like this. Enter the domain you have purchased from GoDaddy . These are the only changes required to be done in GitHub. Configure the DNS at GoDaddy The easiest of the all the below references is Configuring a Godaddy domain name with github pages . The real issue in all the links is that it shows the old UI of GoDaddy , so some things get confusing. Go to the account setting page, which mostly will like in this link . The link will look like this. Click on the + Symbols in Front of Domain, and Click on the Manage DNS Link The link will show a lot of Records , go to the end of the Records , and click on the link ADD From the above option we have to add 3 entries. This is how all the 3 would look like after adding. Now you can launch and check your desired domain. Kindly wait 48 hrs for these changes to reflect, do not try to configure multiple times, if it does not work even after 48 hours kindly search for help, till then take a coffee break and have a nice time out of this screen. The domain XYX is no longer parked by Godaddy When we are doing the above process, even after 24 hours, when you launch your website, we find one of these errors. The domain XYX is no longer parked by Godaddy It is detected as a Malware in the office network. The website might launch for some times and sometimes you might get any one of the above 2 errors. Kindly check this in the Manage DNS page. We had added two A Names pointing to the GitHub URL as shown below. Check if you have any other A Names in addition to the above two, if you have, kindly delete that. The detailed issue can be read GoDaddy domain (randomly) not resolving to GitHub Pages References Setting Up a GoDaddy Domain Name With GitHub Configuring a Godaddy domain name with github pages Using a custom domain with GitHub Pages Using GitHub Pages To Host Your Website GoDaddy domain (randomly) not resolving to GitHub Pages [Help]: How to correctly connect my github pages blog to a custom domain? Redirecting GitHub Page to a custom domain Kindly read the above site, to understand what is the use of CNAME and A Record. Great introduction.","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/linking-domain-with-github-pages.html","loc":"https://www.archerimagine.com/articles/pelican/linking-domain-with-github-pages.html"},{"title":"Publishing your blog to github pages.","text":"We have set the basic blog, though it may not look great because of the theme which we are using, but we will change those. Presently we will focus on taking this blog for the world to see. Github Pages is the way to publish your blog on to the WWW. We will have 2 repositories for doing this. One repository is for the source, which is our blog content. Second repository is for the generated files in the output folder. Prerequisite We all should have a GitHub account, if you have then continue, else go to GitHub and create an account. The process is very straight forward, just follow the instruction on the website. The git should be locally configured. Kindly follow the steps mention in this page Setting your email in Git . The first Repository We should now create our first Repository onto github. Click on the + icon which shows this drop down. And click on the option New Repository as shown above. This will open up a page with looks like this Fill in the repository name which you want to name, and write a little Description also do not click on the check-box which read Initialize this repository with a README Since we already have a folder in our local system, to check-in we can leave this box unchecked. In future if need we can create a README file manually. Once you click on the Create Repository link, you will be taken to this page, with these as a content. This is the symbol of empty repository which is created and now we can push our local code to this folder. Push code from local to Github repository We can follow the instruction mentioned above to push our local code to github, but we have to make sure that we do not push the output directory in the same repository as the code, because the handling of output directory is different. Go to the root of the folder, where your blog is kept. cd ~/mySampleBlog/ Go to the root of the directory where the blog post are kept git init This command initializes an empty repository in the same folder. Create a file named .gitignore in the same folder. Copy the below content to the .gitignore file. *~ ._* *.lock *.DS_Store *.swp *.out *.py [ cod ] output The above code basically prevents all the local temporary files and the output directory to be checked in. git status This command will make sure that the output folder is not tracked by git, check the output as shown below. On branch master Initial commit Untracked files: ( use \"git add <file>...\" to include in what will be committed ) .gitignore Makefile content/ fabfile.py pelicanconf.py publishconf.py nothing added to commit but untracked files present ( use \"git add\" to track ) Only files listed above will be tracked, and output directory is not one of them. git add . Adds all the above files to be ready for commit. git commit -m \"first commit\" This will make a commit locally with the commit message as first commit git remote add origin https://github.com/pelicanBlog/mySampleBlog.git This commands connects the remote repository of github with our local code. The URL might be different for you. git push -u origin master This will push the local changes to the GitHub repository we created. It will ask for your username and password, kindly provide those and we have pushed our code to github. Pushing the output folder The output folder is out actual blog, so this is a little different from the previous way of pushing code to github. Please follow along to create this special repository. Select the same New Repository from the + icon shown in previous repository. In the New Repository form, the magic happens with the Repository Name field, fill in the name as username.github.io , where username is your github username. Fill In the description. Now there are two ways to push the output folder to this new repository, you can follow any one of them. Push the output folder as a repository which we already know. git init git add . git commit -m \"first commit\" git remote add origin https://github.com/pelicanBlog/pelicanBlog.github.io.git git push -u origin master After the above command, just launch this URL in your browser, https://username.github.io/ , just change username with your username. Since the original source folder we create with output directory mentioned in .gitignore , we can now add this repository as a submodule in that repository, to provide a link between both the source and the output Go to the root of the blog content, in this case cd ~/mySampleBlog git submodule add -f https://github.com/pelicanBlog/pelicanBlog.github.io.git output This adds the output folder as a submodule. git commit -m \"added submodule\" git push -u origin master The above will make the output as a submodule With this we have completed our part of the blog series. With this series of blog post we are able to achieve this. Understand what is pelican blog and how to use it with github pages. How to set up the Anaconda environment for pelican development. Understood what pelican-quickstart command does. Understood the basic commands to generate the pelican blog Understood the basic folder structure of the pelican blog Understood the pelican settings files and its uses. Created a github pages and pushed both our content and the blog post to it. The next series will focus into How to configure a domain with github pages. Use of a themes and plugin to enhance the website Integration with google analytics, mail chimp etc. Modifying the blogs to get the most out of the themes. Reference Github Help Setting your email in Git git-submodule How to publish a pelican site on Github Github Pages","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/publishing-blog-github-pages.html","loc":"https://www.archerimagine.com/articles/pelican/publishing-blog-github-pages.html"},{"title":"Understanding the Pelican Settings files.","text":"The basic work flow in pelican blogging is to first generate content, verify it locally using a localhost:8000 , and when everything is fine will publish it. Pelican comes with two settings files to separate these 2 process. These two files are. pelicanconf.py publishconf.py Let's check what is the use of these 2 files, and how to manipulate these files to get the most out of pelican. These setting files are mostly passed to the templates associated with the themes to generate the site, all these settings are some parameters to these templates. pelicanconf.py We had used the pelican-quickstart to generate this blog, when we use this command, we get a pre-configured pelicanconf.py and a publishconf.py files. This have the bare basic configuration to be used based on the questions we answered on the options. These files basically contains the setting's identifier for the pelican blog, all the setting identifiers are in all-caps, and the values numbers (5, 20, etc.), Boolean (True, False, None, etc.), dictionaries, or tuples should not be enclosed in quotation marks. pelicanconf.py is used to generate the site locally and tested over localhost:8000 . The basic pelicanconf.py would look like this. #!/usr/bin/env python # -*- coding: utf-8 -*- # from __future__ import unicode_literals AUTHOR = u 'Animesh' SITENAME = u 'Hello World' SITEURL = '' PATH = 'content' TIMEZONE = 'Asia/Kolkata' DEFAULT_LANG = u 'en' # Feed generation is usually not desired when developing FEED_ALL_ATOM = None CATEGORY_FEED_ATOM = None TRANSLATION_FEED_ATOM = None AUTHOR_FEED_ATOM = None AUTHOR_FEED_RSS = None ARTICLE_PATHS = [ 'articles' ,] ARTICLE_URL = 'articles/ {slug} .html' ARTICLE_SAVE_AS = 'articles/ {slug} .html' # Blogroll LINKS = (( 'Pelican' , 'http://getpelican.com/' ), ( 'Python.org' , 'http://python.org/' ), ( 'Jinja2' , 'http://jinja.pocoo.org/' ), ( 'You can modify those links in your config file' , '#' ),) # Social widget SOCIAL = (( 'You can add links in your config file' , '#' ), ( 'Another social link' , '#' ),) DEFAULT_PAGINATION = 10 # Uncomment following line if you want document-relative URLs when developing #RELATIVE_URLS = True If you remember correctly some of the settings value we had provided during the pelican-quickstart commands, like AUTHOR This is the name of the site author which we had entered during the questions asked. SITENAME This is the Name of the site which we provided. SITEURL Since we still do not have domain name registered, we had kept this empty, and also it makes sense to keep this empty for localhost testing. PATH content where we write our blog is the default path set. TIMEZONE This we had entered during the initial process, and in future if we want to change this we can change these settings. DEFAULT_LANG This was also entered during the pelican-quickstart process. FEEDS_* All the FEEDS_* related settings are empty because we have still not configured the RSS feeds settings, this we will change in the future. ARTICLE_* This is the settings which we modified for keeping the path of the post into one folder. LINKS This is a tuple of tuple, with each entry showing a link which you want provide in your blog. SOCIAL This is also a tuple of tuple, where each entry is meant to point to a name of a social network say Facebook and the link to your profile. DEFAULT_PAGINATION This is a number showing how many blogs should be listed on the front page. Some Themes use this setting for some other purposes. Now these settings are not fully exhaustive, Pelican has a huge list of setting, which we will revisit once we have the need of them. publishconf.py Unlike pelicanconf.py , this setting file is only used when we are supposed to publish our blog to the domain hosting. This configuration is accessed when we generate the site using the following commands. pelican content -s publishconf.py The basic content of these files look like this. #!/usr/bin/env python # -*- coding: utf-8 -*- # from __future__ import unicode_literals # This file is only used if you use `make publish` or # explicitly specify it as your config file. import os import sys sys . path . append ( os . curdir ) from pelicanconf import * SITEURL = '' RELATIVE_URLS = False FEED_ALL_ATOM = 'feeds/all.atom.xml' CATEGORY_FEED_ATOM = 'feeds/ %s .atom.xml' DELETE_OUTPUT_DIRECTORY = True # Following items are often useful when publishing #DISQUS_SITENAME = \"\" #GOOGLE_ANALYTICS = \"\" This file, basically build on top of the pelicanconf.py , as we can see from this line, from pelicanconf import * . What this means is all the configuration from pelicancongf.py is taken into consideration along with some specific configuration which is required for just publishing. If you see this setting most of them are empty which we will fill one by one as we make progress in our blog, but from the structure you might get an idea that this file pertains to its integration with the DISQUS comment system, and the GOOGLE_ANALYTICS code. We will see the use of this code once we integrate these feature into our blog. Reference Pelican Settings","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/pelican-settings-files.html","loc":"https://www.archerimagine.com/articles/pelican/pelican-settings-files.html"},{"title":"Pelican commands to generate the first blog.","text":"We have our boilerplate pelican blog available, now we have still not seen the magic of pelican. In this post we will see our blog coming to life. First Pelican Blog We have seen the folder structure here Now we will first execute some commands and see what happens with this boilerplate. Remember to be in the pelican1 environment. We can do this by. source activate pelican1 Once we are in pelican1 environment, we have all the pelican commands at our disposal. Kindly execute this command. pelican content The above command we are passing content as a parameter, which is nothing but one of the directory of the folder structure. The output will be WARNING: No valid files found in content. Done: Processed 0 articles, 0 drafts, 0 pages and 0 hidden pages in 0 .12 seconds. It clearly warns us about no valid files found in content , as we have not added any post to the directory. If you see inside the output folder, we will see some content in that namely these files archives.html authors.html categories.html index.html tags.html theme #directory which contains some predefined images and css. Now let us see what the blog looks like. Just execute these commands. cd output/ python -m pelican.server Once you execute the above commands we can see the output on browser on this path localhost:8000 and it will look something like this Nothing fancy here, but we will some content with some link and a default theme. It is petty good for being a boilerplate. The first post. Now we are ready for our first post, we will do the sample hello world which is the de-facto standard in programming languages first program. We will write the first post in Markdown . Create a file named HelloWorld.md in this directory content . Title: Hello World Date: 2017-04-29 11:01 Category: Pelican Hello World to Pelican Once create and saved this file, just run this command. pelican content This will have the following output. Done: Processed 1 article, 0 drafts, 0 pages and 0 hidden pages in 0 .27 seconds. This time it clearly states that we have 1 article. Then follow the other commands as discussed. cd output/ python -m pelican.server Again check the output on the browser at localhost:8000 . Now this time the output is different and it look like this. The area surrounded in ellipse are new. This shows us the power of pelican, we do not have to bother how the content is presented on screen, we have to only concentrate on writing content. Reference Pelican Doc Quickstart","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/pelican-commands-to-generate-first-pelican-blog-post.html","loc":"https://www.archerimagine.com/articles/pelican/pelican-commands-to-generate-first-pelican-blog-post.html"},{"title":"Pelican Folder Structure","text":"We will be able to extract the full juice of pelican once we understand the building blocks of the pelican blog. Folder structure and some files forms the basic of this. We will understand some of these files and folder use in this post. After the first blog post if we give make clean command we will see a folder structure just like this. This will have an empty output folder. We will understand the use of these folder and a proper way of managing your content. content folder This is the folder where all the magic happens. This is the root folder for our all content, we can use this folder for these purposes. Writing content Saving the images references in the content Static Pages (ex: about, contact etc) folder for storing our favicon.ico and apple icon Writing Content In traditional wordpress or most of the blogging platform the content is stored in this format /2015/05/24/my-content/ now this may be what some people might be happy with and you can also do the same, but this impacts the SEO ranking as the path becomes long and the date when the content was created has no significance to the actual content but is occupying space on the URL. In my opinion we can better organize our self with some meaning full structure. What we can do is we can create a folder in side content named articles and the sub folders inside articles for each category which you want to write or if you want to have a flat system you can place all your content inside articles . I prefer the sub folder approach as we can derive the category name just from the folder name. So go ahead for beginning create a folder inside content name articles cd content/ mkdir articles Now create your second post inside this and publish the content based on the command we learned on the previous post. Now if you launch the localhost:8000 , and click on the article title, you will see no difference but if you see on the URL bar, we still see that our post is without the article folder structure. We did not want this. Have a look In order to get the proper URL in the address bar we have to change somethings in the pelicanconf.py , we will explain what is the purpose of this file in a short file for not just add these 3 line into that config. ARTICLE_PATHS = [ 'articles' ,] ARTICLE_URL = 'articles/ {slug} .html' ARTICLE_SAVE_AS = 'articles/ {slug} .html' With is configuration in the config folder, just generate the blog with the commands already learned and then check the localhost:8000 . It should look like this. Now we have a proper folder structure. Static Pages Most often than not we will want some pages which will rarely change, like an About and a Contact pages. These type of static pages is also supported in the pelican blog. Just create a folder named pages inside content folder like this. cd content/ mkdir pages Now we are ready to make our sample About.md and Contact.md inside pages directory. Title: About Date: 2017-04-14 22:30 Modified: 2017-04-14 22:30 Slug: About Author: username Summary: This is a sample blog. The About Page for the blog. and Contact.md Title: Contact Date: 2017-04-14 22:30 Modified: 2017-04-14 22:30 Slug: Contact Author: username Summary: This is a sample blog contact page. The Contact Page for the blog. We again generate the blog and check it on localhost:8000 . When we see the output of pelican content command we will see this. Done: Processed 1 article, 0 drafts, 2 pages and 0 hidden pages in 0 .20 seconds. Which means that, the pages are generated. When we launch the localhost:8000 . we will see that the About and Contact menu like this We can modify these pages with the information which you want to furnish. Static Images Most often than not we will use images to link in our blogs, we can store all these images inside the content folder, having a directory called images . Do this by following these commands. cd content/ mkdir images Now we can copy any image inside this folder and try to link it into one of our blog. Just copy any image in this folder and copy the file name. Now create a link to this file inside the already existing blog post by the help of link of markdown, here is a sample. ![ Hello World 1 ]( {static}../../images/helloWorldPelicanPost.png \"Hello World 2\" ) We should understand some details about the above piece of code. Hello World 1 The alt text is good if there is a browser which block image, this alt text is displayed, showing an information about what this images were supposed to do. \"Hello World 2\" This is the title of the image, which is shown as a tool tip once we hover over the image. {static} This is a special syntax which is used by pelican to generate links, so be it URL or images, when using relative URLs kindly use this format. We have covered most of the folder structure inside article we will see the configuration files in the next blog post. Reference Pages Linking to internal content","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/pelican-folder-structure.html","loc":"https://www.archerimagine.com/articles/pelican/pelican-folder-structure.html"},{"title":"Pelican HelloWorld using pelican-quickstart","text":"Pelican makes it very easy to make the ground rolling ASAP. Pelican provides a great command pelican-quickstart , which asks a few questions to you and makes a boilerplate blog ready in a few seconds. We will go through the entire process explaining each option is details. Activate Python environment We had set up a separate python anaconda environment in our previous post now is the time to active the environment, we can do that by using a simple command. source activate pelican1 It will activate the pelican1 environment, and we can identify it by check the terminal prompt which will change to (pelican1) . Once the work is done, we can deactivate the same with a simple command. source deactivate pelican1 All the above command will work for Linux and Mac, kindly check the windows equivalent of the same. pelican-quickstart Now we are ready to divulge in the world of pelican. Pelican has a ready to bake command to set up the basic boilerplate for the blog. The command is called Before executing the below commands, just create a directory where you want your blog files to be stored. mkdir ~/myBlogDirectory cd ~/myBlogDirectory pelican-quickstart Just execute this commands and it asks you these series of questions, which we will talk in details. pelican-quickstart options The options shown after executing the pelican-quickstart are as below. We will discuss each and every option and their usage, we can always choose the default shown in capital letter, {Y|n} . Where do you want to create your new Website ? [.] Most probably we will keep the default as we are already in that directory, if not you can specify the path /home/pathtomyblog What will be the title of the blog? Provide a suitable title to your blog, do not worry even if you want to change it latter we can change it. Who will be the author of this Website ? Just provide any name you want whose name should be present as an author on the blog post, it can be your name as well. What will be the default language of this Website ? [en] The default choice is English , else you can give any language format mentioned in ISO 639-1 Language Codes, the list can be found ISO Language codes Do you want to specify a URL prefix? e.g., http://example.com (Y/n) If you already have purchased a domain give the domain name as shown in the example, else continue with n , we can later fill the domain name. Do you want to enable article pagination? (Y/n) We can go with the default of having pagination, which means how many posts of the blogs will be displayed in one page, the choice of this is in the next question. How many articles per page do you want? [10] The default choice is 10 , for the time being keep it that way. What is your time zone? [Europe/Paris] To change the time zone, we should be aware that these are tz database time zone, to exactly get the time zone codes for your country visit List of tz database time zones , give the code without the [] . Do you want to generate a Fabfile/Makefile to automate generation and publishing? (Y/n) There are multiple ways to automate the blog publishing process, makefile and fabic comes to our help, just chose the default and we will decide on this later. This creates two files in the directory, fabfile.py and Makefile Do you want an auto-reload & simpleHTTP script to assist with theme and site development? (Y/n) We have the help of auto-reload commands to automatically generates the preview as soon as we change anything in the themes, we might not require it initially, so keep it false. Do you want to upload your website using FTP? (y/N) If we had an FTP site where we could upload, just choose the default and say n Do you want to upload your website using SSH? (y/N) If you are hosting uses SSH, for our use case we will choose N , which is the default. Do you want to upload your website using Dropbox? (y/N) We can also use Dropbox to upload our static files, but for this, we will try some other time, for now choose the default which is N . Do you want to upload your website using S3? (y/N) We also have the facility of choosing amazon S3 for our site hosting, for now not needed chose N Do you want to upload your website using Rackspace Cloud Files? (y/N) Again the default N , we are not using Rackspace. Do you want to upload your website using GitHub Pages? (y/N) We can choose y here, but we will try another mechanism, for now choose the default N If you had chosen y in this option you will get this sub option. Is this your personal page (username.github.io)? (y/N) Choose y , After this we will get this message in either case. Done. Your new project is available at /home/username/myWork/mySampleBlog Our blogs boilerplate is available. Here is the folder structure. We have still not seen how the blog will look like, we will divulge into this in the next post, with all the pelican commands which is extremely important to get the full juice out of pelican. References How I setup Pelican List of tz database time zones List of ISO 639-1 codes pelican-quickstart.py","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/helloworld-pelican-quickstart.html","loc":"https://www.archerimagine.com/articles/pelican/helloworld-pelican-quickstart.html"},{"title":"How to setup Anaconda Python environment for Pelican Blog.","text":"Installing Pelican Pelican is a python package, so we can have multiple option to install pelican. There can be 3 option which I can think of. Direct Installation If we have only one python installation on the system, and we do not have any issue if we screw up this installation just use pip to install pelican. Installation using VirtualEnv. This approach is already mentioned in official documentation of pelican Installation using Conda. By now you could have understood that we will use Conda to install pelican. This is because Anaconda is already a prepackaged installation of very well know python package in both version 2 and 3. In future, I will update this page if I wrote about Anaconda installation, for now refer any documents on the google search for installation. Configure Anaconda for Pelican Blog The first thing we have to do is to create an environment using the python version 2. This can be done by this command. conda create -n pelican1 python=2 Once we have executed the above command we will have pelican1 as an environment. We can see the list of environment in the system by using. conda info --envs Which will provide, an output like this. pelican1 /home/username/anaconda3/envs/pelican1 py27 /home/username/anaconda3/envs/py27 py35 /home/username/anaconda3/envs/py35 root * /home/username/anaconda3 This shows all the available environment. We can activate the pelican1 by using this command. source activate pelican1 Now we have an environment which we can use for pelican development. The reason for this environment creation is to have a separate environment for experimentation with pelican, if anything goes wrong we do not disturb already existing programs. Configure Pelican Environment for Blog Once we have the environment, we have to install few packages in this environment. The first is to install PiP to install other package. Install Pip by using. conda install pip With the completion of installing pip , the first and foremost package to install is pelican, with this command. pip install pelican Since we will be using Markdown to write our blogs we need the markdown package, which we can install using. pip install Markdown There are some plugin and themes in pelican which might need some additional packages, we will install these 2. pip install Fabric pip install beautifulsoup4 Freeze the Requirement When we have all our installation complete, we should save our package history into a requirement.txt . We can use this requirement.txt to install all the above mentioned packages with same version in one go. We can freeze the details by using. pip freeze > requirements.txt Dependencies If we check the requirement.txt generated in the above step, we will see a lot of packages already installed apart from pelican , markdown , Fabric and beautifulsoup4 . These extra packages are dependencies for running pelican. feedgenerator: to generate the Atom feeds jinja2: for templating support pygments: for syntax highlighting docutils: for supporting reStructuredText as an input format pytz: for timezone definitions blinker: an object-to-object and broadcast signaling system unidecode: for ASCII transliterations of Unicode text six: for Python 2 and 3 compatibility utilities MarkupSafe: for a markup safe string implementation python-dateutil: to read the date metadata We have completed 2 important steps of our own blog publishing. Reference Installing Pelican Managing Python","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/python-setup-for-pelican-blog.html","loc":"https://www.archerimagine.com/articles/pelican/python-setup-for-pelican-blog.html"},{"title":"Cost effective blogging with Pelican and Github","text":"This is a series of blog post to help set up a static blog at minimal cost and integrating with all the popular tools such as Google Analytics, MailChimp, Disqus, Google Adsense. This will be the one stop place to find all the details for setting up a beginner level blog. I am a novice blogger and this blog would act as a journal, which will document my approach towards blogging, software development. The idea is to generate some revenues out of this blog in the long run. We all might have read about so many blogs which are able to generate good amount of traffic but in none one of those we have never read how do they achieve it. I may fail in my attempt, which might work as a guidance for someone to not follow this path and try another path for the same goal. The background theme to support this is to minimize my cost to the bare minimum so that the failure does not hurt me financially. Why Static Blog? There are already a lot of literature present behind this, just wanted to summarize those:- Cost:- This was the deciding factor for me, because of using a static website, this complete website can be hosted and deployed by just registering a domain name with a provider. I do not have to go for any hosting services etc. Easy of Writing Content:- I wanted to write my content using just Markdown , as i have grown comfortable writing in markdown. With using a static blog this was possible. Hosting:- We can serve these static HTML pages practically from any place, be it Github , Amazon S3 , Dropbox or any other place which can serve static HTML pages. I have chosen Github just to save the cost. Easy workflow:- The work flow is very simple when deploying with static blogs, just right you content in markdown, generate HTML, push your changes to github and that's it. Your content is not available online. You can even go crazy you cam automate the whole thing other than writing content. The above three are the main reason for choosing static blogs, but there could be many more valid reason for choosing static blogs. Most of the reason for me was personal in nature so you can also choose accordingly. Why Pelican? Once the approach to make this blog as static was finalized, the next big question came was which technology to choose, Pelican or Jekyll . There as already many comparison already available among these, but the only reason for me to choose pelican was because it uses python and jinja. In some near future I want to fully customize my blog with the knowledge of these two. Why Github? The final decision to be taken before starting this blog was to finalize the hosting provider. We have already mentioned some popular choice are Github Amazon S3 Dropbox I chose Github, for its near zero cost, it may cost you if you want to keep your repository private else it is completely free were as Amazon S3 would have required to shell out some money though less, with some extra benefits, but for the time being when I am just measuring the water it made sense to keep my cost down. Once all the above 3 moral questions were answered, setting up the blogs was easy and which will be documented in the future. Collated Blog post. How to set up Anaconda Python environment for Pelican Blog. Pelican HelloWorld using pelican-quickstart Pelican Folder Structure Pelican commands to generate the first blog. Understanding the Pelican Settings files Publishing your blog to github pages Configuring Github pages with Custom Domain Customizing Pelican blog with the help of Plugin and themes Expressing the content with Pelican Themes Problem Faced when integrating with Elegant Themes Home Page feature for Elegant Theme Integrating 3rd party services with Pelican Blog. Tips to improve work flow in pelican blog. Reference Making a Static Blog with Pelican This above blog explains why a static blog generator is good. Moving Blogs to Pelican This blog has a terrific explanation for pelican vs jekyll. Amazon S3 Vs Github Pages This explains the benefits of Amazon S3 over Github Pages, kindly check if you are affected because of this.","tags":"Pelican","url":"https://www.archerimagine.com/articles/pelican/cost-effective-blogging-with-Pelican-and-Github.html","loc":"https://www.archerimagine.com/articles/pelican/cost-effective-blogging-with-Pelican-and-Github.html"}]};